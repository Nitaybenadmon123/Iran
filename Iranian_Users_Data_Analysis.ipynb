{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w9_WRvuHxVE"
   },
   "source": [
    "Iran Twitter / X Data Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PofLTHdHfU_"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Iran Twitter / X Data Analysis Project\n",
    "# ====================================================\n",
    "# Course: Data Science Final Project (SCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q requests pandas urllib3 emoji transformers sentence-transformers scikit-learn matplotlib seaborn plotly numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOI74U-kVM5v"
   },
   "source": [
    "# **italicized text*POI Scraper (Wikipedia Categories â†’ CSV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "myZurgeOLVxN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â© Skipping government_ministers_of_iran: file already exists.\n",
      "â© Skipping presidents_of_iran: file already exists.\n",
      "â© Skipping vice_presidents_of_iran: file already exists.\n",
      "â© Skipping iranian_ayatollahs: file already exists.\n",
      "â© Skipping iranian_actors: file already exists.\n",
      "â© Skipping iranian_singers: file already exists.\n",
      "â© Skipping iranian_scientists: file already exists.\n",
      "â© Skipping iranian_economists: file already exists.\n",
      "â© Skipping iranian_writers: file already exists.\n",
      "â© Skipping iranian_football_managers: file already exists.\n",
      "â© Skipping hospitals_in_iran: file already exists.\n",
      "â© Skipping private_hospitals_in_iran: file already exists.\n",
      "â© Skipping teaching_hospitals_in_iran: file already exists.\n",
      "â© Skipping iranian_physicians: file already exists.\n",
      "â© Skipping iranian_cardiologists: file already exists.\n",
      "â© Skipping iranian_women_physicians: file already exists.\n",
      "â© Skipping 21st-century_iranian_physicians: file already exists.\n",
      "â© Skipping 19th-century_iranian_physicians: file already exists.\n",
      "â© Skipping medical_and_health_organisations_based_in_iran: file already exists.\n",
      "â© Skipping healthcare_in_iran: file already exists.\n",
      "â© Skipping medicine_in_iran: file already exists.\n",
      "â© Skipping iranian_journalists: file already exists.\n",
      "â© Skipping iranian_women_journalists: file already exists.\n",
      "ğŸ“‚ ×¡×•×¨×§: Category:Iranian_reporters\n",
      "âš ï¸ ××™×Ÿ × ×ª×•× ×™× ×¢×‘×•×¨ Category:Iranian_reporters\n",
      "ğŸ“‚ ×¡×•×¨×§: Category:Iranian_correspondents\n",
      "âš ï¸ ××™×Ÿ × ×ª×•× ×™× ×¢×‘×•×¨ Category:Iranian_correspondents\n",
      "â© Skipping iranian_editors: file already exists.\n",
      "ğŸ“‚ ×¡×•×¨×§: Category:Iranian_news_readers\n",
      "âš ï¸ ××™×Ÿ × ×ª×•× ×™× ×¢×‘×•×¨ Category:Iranian_news_readers\n",
      "â© Skipping iranian_activists: file already exists.\n",
      "ğŸ“‚ ×¡×•×¨×§: Category:Iranian_political_activists\n",
      "âš ï¸ ××™×Ÿ × ×ª×•× ×™× ×¢×‘×•×¨ Category:Iranian_political_activists\n",
      "â© Skipping iranian_human_rights_activists: file already exists.\n",
      "â© Skipping iranian_women_activists: file already exists.\n",
      "â© Skipping iranian_feminists: file already exists.\n",
      "â© Skipping iranian_dissidents: file already exists.\n",
      "â© Skipping iranian_bloggers: file already exists.\n",
      "â© Skipping iranian_footballers: file already exists.\n",
      "â© Skipping iranian_athletes: file already exists.\n",
      "â© Skipping iranian_wrestlers: file already exists.\n",
      "â© Skipping iranian_taekwondo_practitioners: file already exists.\n",
      "â© Skipping iranian_volleyball_players: file already exists.\n",
      "â© Skipping iranian_weightlifters: file already exists.\n",
      "â© Skipping iranian_television_actors: file already exists.\n",
      "â© Skipping iranian_film_actors: file already exists.\n",
      "â© Skipping iranian_pop_singers: file already exists.\n",
      "â© Skipping iranian_rappers: file already exists.\n",
      "â© Skipping iranian_comedians: file already exists.\n",
      "\n",
      "================= Summary =================\n",
      "â€¢ Category:Iranian_news_readers -> iranian_news_readers: 0 ×¨×©×•××•×ª  |  ×§×•×‘×¥: -\n",
      "â€¢ Category:Iranian_correspondents -> iranian_correspondents: 0 ×¨×©×•××•×ª  |  ×§×•×‘×¥: -\n",
      "â€¢ Category:Iranian_reporters -> iranian_reporters: 0 ×¨×©×•××•×ª  |  ×§×•×‘×¥: -\n",
      "â€¢ Category:Iranian_political_activists -> iranian_political_activists: 0 ×¨×©×•××•×ª  |  ×§×•×‘×¥: -\n",
      "â€¢ Category:Government_ministers_of_Iran -> government_ministers_of_iran: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: government_ministers_of_iran_wikipedia.csv\n",
      "â€¢ Category:Iranian_singers -> iranian_singers: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_singers_wikipedia.csv\n",
      "â€¢ Category:Iranian_scientists -> iranian_scientists: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_scientists_wikipedia.csv\n",
      "â€¢ Category:Iranian_economists -> iranian_economists: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_economists_wikipedia.csv\n",
      "â€¢ Category:Iranian_writers -> iranian_writers: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_writers_wikipedia.csv\n",
      "â€¢ Category:Presidents_of_Iran -> presidents_of_iran: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: presidents_of_iran_wikipedia.csv\n",
      "â€¢ Category:Vice_presidents_of_Iran -> vice_presidents_of_iran: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: vice_presidents_of_iran_wikipedia.csv\n",
      "â€¢ Category:Iranian_ayatollahs -> iranian_ayatollahs: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_ayatollahs_wikipedia.csv\n",
      "â€¢ Category:Iranian_actors -> iranian_actors: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_actors_wikipedia.csv\n",
      "â€¢ Category:Teaching_hospitals_in_Iran -> teaching_hospitals_in_iran: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: teaching_hospitals_in_iran_wikipedia.csv\n",
      "â€¢ Category:Private_hospitals_in_Iran -> private_hospitals_in_iran: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: private_hospitals_in_iran_wikipedia.csv\n",
      "â€¢ Category:Hospitals_in_Iran -> hospitals_in_iran: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: hospitals_in_iran_wikipedia.csv\n",
      "â€¢ Category:Iranian_football_managers -> iranian_football_managers: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_football_managers_wikipedia.csv\n",
      "â€¢ Category:Iranian_physicians -> iranian_physicians: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_physicians_wikipedia.csv\n",
      "â€¢ Category:Iranian_cardiologists -> iranian_cardiologists: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_cardiologists_wikipedia.csv\n",
      "â€¢ Category:Iranian_women_physicians -> iranian_women_physicians: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_women_physicians_wikipedia.csv\n",
      "â€¢ Category:21st-century_Iranian_physicians -> 21st-century_iranian_physicians: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: 21st-century_iranian_physicians_wikipedia.csv\n",
      "â€¢ Category:Medicine_in_Iran -> medicine_in_iran: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: medicine_in_iran_wikipedia.csv\n",
      "â€¢ Category:Healthcare_in_Iran -> healthcare_in_iran: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: healthcare_in_iran_wikipedia.csv\n",
      "â€¢ Category:Medical_and_health_organisations_based_in_Iran -> medical_and_health_organisations_based_in_iran: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: medical_and_health_organisations_based_in_iran_wikipedia.csv\n",
      "â€¢ Category:19th-century_Iranian_physicians -> 19th-century_iranian_physicians: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: 19th-century_iranian_physicians_wikipedia.csv\n",
      "â€¢ Category:Iranian_journalists -> iranian_journalists: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_journalists_wikipedia.csv\n",
      "â€¢ Category:Iranian_women_journalists -> iranian_women_journalists: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_women_journalists_wikipedia.csv\n",
      "â€¢ Category:Iranian_editors -> iranian_editors: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_editors_wikipedia.csv\n",
      "â€¢ Category:Iranian_activists -> iranian_activists: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_activists_wikipedia.csv\n",
      "â€¢ Category:Iranian_human_rights_activists -> iranian_human_rights_activists: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_human_rights_activists_wikipedia.csv\n",
      "â€¢ Category:Iranian_women_activists -> iranian_women_activists: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_women_activists_wikipedia.csv\n",
      "â€¢ Category:Iranian_feminists -> iranian_feminists: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_feminists_wikipedia.csv\n",
      "â€¢ Category:Iranian_dissidents -> iranian_dissidents: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_dissidents_wikipedia.csv\n",
      "â€¢ Category:Iranian_bloggers -> iranian_bloggers: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_bloggers_wikipedia.csv\n",
      "â€¢ Category:Iranian_footballers -> iranian_footballers: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_footballers_wikipedia.csv\n",
      "â€¢ Category:Iranian_athletes -> iranian_athletes: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_athletes_wikipedia.csv\n",
      "â€¢ Category:Iranian_wrestlers -> iranian_wrestlers: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_wrestlers_wikipedia.csv\n",
      "â€¢ Category:Iranian_taekwondo_practitioners -> iranian_taekwondo_practitioners: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_taekwondo_practitioners_wikipedia.csv\n",
      "â€¢ Category:Iranian_volleyball_players -> iranian_volleyball_players: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_volleyball_players_wikipedia.csv\n",
      "â€¢ Category:Iranian_weightlifters -> iranian_weightlifters: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_weightlifters_wikipedia.csv\n",
      "â€¢ Category:Iranian_television_actors -> iranian_television_actors: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_television_actors_wikipedia.csv\n",
      "â€¢ Category:Iranian_film_actors -> iranian_film_actors: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_film_actors_wikipedia.csv\n",
      "â€¢ Category:Iranian_pop_singers -> iranian_pop_singers: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_pop_singers_wikipedia.csv\n",
      "â€¢ Category:Iranian_rappers -> iranian_rappers: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_rappers_wikipedia.csv\n",
      "â€¢ Category:Iranian_comedians -> iranian_comedians: SKIPPED ×¨×©×•××•×ª  |  ×§×•×‘×¥: iranian_comedians_wikipedia.csv\n",
      "\n",
      "ğŸ“Š Total across files: 0 rows\n",
      "===========================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Slug</th>\n",
       "      <th>SavedRows</th>\n",
       "      <th>CSV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Category:Iranian_news_readers</td>\n",
       "      <td>iranian_news_readers</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Category:Iranian_correspondents</td>\n",
       "      <td>iranian_correspondents</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Category:Iranian_reporters</td>\n",
       "      <td>iranian_reporters</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Category:Iranian_political_activists</td>\n",
       "      <td>iranian_political_activists</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Category:Government_ministers_of_Iran</td>\n",
       "      <td>government_ministers_of_iran</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Category:Iranian_singers</td>\n",
       "      <td>iranian_singers</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Category:Iranian_scientists</td>\n",
       "      <td>iranian_scientists</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Category:Iranian_economists</td>\n",
       "      <td>iranian_economists</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Category:Iranian_writers</td>\n",
       "      <td>iranian_writers</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Category:Presidents_of_Iran</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Category:Vice_presidents_of_Iran</td>\n",
       "      <td>vice_presidents_of_iran</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Category:Iranian_ayatollahs</td>\n",
       "      <td>iranian_ayatollahs</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Category:Iranian_actors</td>\n",
       "      <td>iranian_actors</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Category:Teaching_hospitals_in_Iran</td>\n",
       "      <td>teaching_hospitals_in_iran</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Category:Private_hospitals_in_Iran</td>\n",
       "      <td>private_hospitals_in_iran</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Category:Hospitals_in_Iran</td>\n",
       "      <td>hospitals_in_iran</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Category:Iranian_football_managers</td>\n",
       "      <td>iranian_football_managers</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Category:Iranian_physicians</td>\n",
       "      <td>iranian_physicians</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Category:Iranian_cardiologists</td>\n",
       "      <td>iranian_cardiologists</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Category:Iranian_women_physicians</td>\n",
       "      <td>iranian_women_physicians</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Category:21st-century_Iranian_physicians</td>\n",
       "      <td>21st-century_iranian_physicians</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Category:Medicine_in_Iran</td>\n",
       "      <td>medicine_in_iran</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Category:Healthcare_in_Iran</td>\n",
       "      <td>healthcare_in_iran</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Category:Medical_and_health_organisations_base...</td>\n",
       "      <td>medical_and_health_organisations_based_in_iran</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Category:19th-century_Iranian_physicians</td>\n",
       "      <td>19th-century_iranian_physicians</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Category:Iranian_journalists</td>\n",
       "      <td>iranian_journalists</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Category:Iranian_women_journalists</td>\n",
       "      <td>iranian_women_journalists</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Category:Iranian_editors</td>\n",
       "      <td>iranian_editors</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Category:Iranian_activists</td>\n",
       "      <td>iranian_activists</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Category:Iranian_human_rights_activists</td>\n",
       "      <td>iranian_human_rights_activists</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Category:Iranian_women_activists</td>\n",
       "      <td>iranian_women_activists</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Category:Iranian_feminists</td>\n",
       "      <td>iranian_feminists</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Category:Iranian_dissidents</td>\n",
       "      <td>iranian_dissidents</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Category:Iranian_bloggers</td>\n",
       "      <td>iranian_bloggers</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Category:Iranian_footballers</td>\n",
       "      <td>iranian_footballers</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Category:Iranian_athletes</td>\n",
       "      <td>iranian_athletes</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Category:Iranian_wrestlers</td>\n",
       "      <td>iranian_wrestlers</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Category:Iranian_taekwondo_practitioners</td>\n",
       "      <td>iranian_taekwondo_practitioners</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Category:Iranian_volleyball_players</td>\n",
       "      <td>iranian_volleyball_players</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Category:Iranian_weightlifters</td>\n",
       "      <td>iranian_weightlifters</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Category:Iranian_television_actors</td>\n",
       "      <td>iranian_television_actors</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Category:Iranian_film_actors</td>\n",
       "      <td>iranian_film_actors</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Category:Iranian_pop_singers</td>\n",
       "      <td>iranian_pop_singers</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Category:Iranian_rappers</td>\n",
       "      <td>iranian_rappers</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Category:Iranian_comedians</td>\n",
       "      <td>iranian_comedians</td>\n",
       "      <td>SKIPPED</td>\n",
       "      <td>C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Category  \\\n",
       "0                       Category:Iranian_news_readers   \n",
       "1                     Category:Iranian_correspondents   \n",
       "2                          Category:Iranian_reporters   \n",
       "3                Category:Iranian_political_activists   \n",
       "4               Category:Government_ministers_of_Iran   \n",
       "5                            Category:Iranian_singers   \n",
       "6                         Category:Iranian_scientists   \n",
       "7                         Category:Iranian_economists   \n",
       "8                            Category:Iranian_writers   \n",
       "9                         Category:Presidents_of_Iran   \n",
       "10                   Category:Vice_presidents_of_Iran   \n",
       "11                        Category:Iranian_ayatollahs   \n",
       "12                            Category:Iranian_actors   \n",
       "13                Category:Teaching_hospitals_in_Iran   \n",
       "14                 Category:Private_hospitals_in_Iran   \n",
       "15                         Category:Hospitals_in_Iran   \n",
       "16                 Category:Iranian_football_managers   \n",
       "17                        Category:Iranian_physicians   \n",
       "18                     Category:Iranian_cardiologists   \n",
       "19                  Category:Iranian_women_physicians   \n",
       "20           Category:21st-century_Iranian_physicians   \n",
       "21                          Category:Medicine_in_Iran   \n",
       "22                        Category:Healthcare_in_Iran   \n",
       "23  Category:Medical_and_health_organisations_base...   \n",
       "24           Category:19th-century_Iranian_physicians   \n",
       "25                       Category:Iranian_journalists   \n",
       "26                 Category:Iranian_women_journalists   \n",
       "27                           Category:Iranian_editors   \n",
       "28                         Category:Iranian_activists   \n",
       "29            Category:Iranian_human_rights_activists   \n",
       "30                   Category:Iranian_women_activists   \n",
       "31                         Category:Iranian_feminists   \n",
       "32                        Category:Iranian_dissidents   \n",
       "33                          Category:Iranian_bloggers   \n",
       "34                       Category:Iranian_footballers   \n",
       "35                          Category:Iranian_athletes   \n",
       "36                         Category:Iranian_wrestlers   \n",
       "37           Category:Iranian_taekwondo_practitioners   \n",
       "38                Category:Iranian_volleyball_players   \n",
       "39                     Category:Iranian_weightlifters   \n",
       "40                 Category:Iranian_television_actors   \n",
       "41                       Category:Iranian_film_actors   \n",
       "42                       Category:Iranian_pop_singers   \n",
       "43                           Category:Iranian_rappers   \n",
       "44                         Category:Iranian_comedians   \n",
       "\n",
       "                                              Slug SavedRows  \\\n",
       "0                             iranian_news_readers         0   \n",
       "1                           iranian_correspondents         0   \n",
       "2                                iranian_reporters         0   \n",
       "3                      iranian_political_activists         0   \n",
       "4                     government_ministers_of_iran   SKIPPED   \n",
       "5                                  iranian_singers   SKIPPED   \n",
       "6                               iranian_scientists   SKIPPED   \n",
       "7                               iranian_economists   SKIPPED   \n",
       "8                                  iranian_writers   SKIPPED   \n",
       "9                               presidents_of_iran   SKIPPED   \n",
       "10                         vice_presidents_of_iran   SKIPPED   \n",
       "11                              iranian_ayatollahs   SKIPPED   \n",
       "12                                  iranian_actors   SKIPPED   \n",
       "13                      teaching_hospitals_in_iran   SKIPPED   \n",
       "14                       private_hospitals_in_iran   SKIPPED   \n",
       "15                               hospitals_in_iran   SKIPPED   \n",
       "16                       iranian_football_managers   SKIPPED   \n",
       "17                              iranian_physicians   SKIPPED   \n",
       "18                           iranian_cardiologists   SKIPPED   \n",
       "19                        iranian_women_physicians   SKIPPED   \n",
       "20                 21st-century_iranian_physicians   SKIPPED   \n",
       "21                                medicine_in_iran   SKIPPED   \n",
       "22                              healthcare_in_iran   SKIPPED   \n",
       "23  medical_and_health_organisations_based_in_iran   SKIPPED   \n",
       "24                 19th-century_iranian_physicians   SKIPPED   \n",
       "25                             iranian_journalists   SKIPPED   \n",
       "26                       iranian_women_journalists   SKIPPED   \n",
       "27                                 iranian_editors   SKIPPED   \n",
       "28                               iranian_activists   SKIPPED   \n",
       "29                  iranian_human_rights_activists   SKIPPED   \n",
       "30                         iranian_women_activists   SKIPPED   \n",
       "31                               iranian_feminists   SKIPPED   \n",
       "32                              iranian_dissidents   SKIPPED   \n",
       "33                                iranian_bloggers   SKIPPED   \n",
       "34                             iranian_footballers   SKIPPED   \n",
       "35                                iranian_athletes   SKIPPED   \n",
       "36                               iranian_wrestlers   SKIPPED   \n",
       "37                 iranian_taekwondo_practitioners   SKIPPED   \n",
       "38                      iranian_volleyball_players   SKIPPED   \n",
       "39                           iranian_weightlifters   SKIPPED   \n",
       "40                       iranian_television_actors   SKIPPED   \n",
       "41                             iranian_film_actors   SKIPPED   \n",
       "42                             iranian_pop_singers   SKIPPED   \n",
       "43                                 iranian_rappers   SKIPPED   \n",
       "44                               iranian_comedians   SKIPPED   \n",
       "\n",
       "                                                  CSV  \n",
       "0                                                None  \n",
       "1                                                None  \n",
       "2                                                None  \n",
       "3                                                None  \n",
       "4   C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "5   C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "6   C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "7   C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "8   C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "9   C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "10  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "11  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "12  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "13  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "14  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "15  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "16  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "17  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "18  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "19  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "20  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "21  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "22  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "23  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "24  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "25  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "26  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "27  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "28  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "29  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "30  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "31  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "32  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "33  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "34  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "35  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "36  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "37  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "38  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "39  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "40  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "41  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "42  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "43  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  \n",
       "44  C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ No CSV files were created â†’ no ZIP.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# File: step1_collect_wikipedia_categories.py (UPDATED)\n",
    "#\n",
    "# Description:\n",
    "#   Recursively collects people/pages from Wikipedia categories (including subcategories),\n",
    "#   and saves a CSV per category with columns: Name, Wikipedia_Link.\n",
    "#   Output structure (per course spec):\n",
    "#       <IRAN_DIR>/\n",
    "#         POIs/\n",
    "#           <slug>/\n",
    "#             <slug>_wikipedia.csv\n",
    "#   Also produces a summary table and a ZIP with all CSVs.\n",
    "# ============================================\n",
    "\n",
    "import requests, time, os, zipfile\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# ---------------- IRAN_DIR resolution ----------------\n",
    "# Set the Iran directory path (adjust if needed)\n",
    "IRAN_DIR = r\"C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\"\n",
    "\n",
    "if not os.path.isdir(IRAN_DIR):\n",
    "    raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran' ×‘× ×ª×™×‘: {IRAN_DIR}\")\n",
    "\n",
    "# Base POIs directory (as per course structure)\n",
    "POIS_DIR = os.path.join(IRAN_DIR, \"POIs\")\n",
    "os.makedirs(POIS_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------- API config ----------------\n",
    "BASE_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "HEADERS = {\"User-Agent\": \"SCE-DataScience-FinalProject/1.0 (contact: student@example.com)\"}\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=0.8, status_forcelist=[403,429,500,502,503,504])\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def api_get(params):\n",
    "    \"\"\"Thin wrapper around MediaWiki API GET with a fallback UA on 403.\"\"\"\n",
    "    r = session.get(BASE_API, params=params, headers=HEADERS, timeout=30)\n",
    "    if r.status_code == 403:\n",
    "        time.sleep(1)\n",
    "        r = session.get(BASE_API, params=params, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def get_category_members(category_title, cmtype=\"page\"):\n",
    "    \"\"\"\n",
    "    Fetches up to all members of a category, paging through 'continue'.\n",
    "    cmtype can be \"page\" (actual pages) or \"subcat\" (subcategories).\n",
    "    \"\"\"\n",
    "    members, params = [], {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": category_title,\n",
    "        \"cmlimit\": \"500\",\n",
    "        \"cmtype\": cmtype,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    while True:\n",
    "        data = api_get(params)\n",
    "        members.extend(data.get(\"query\", {}).get(\"categorymembers\", []))\n",
    "        if \"continue\" in data:\n",
    "            params[\"cmcontinue\"] = data[\"continue\"][\"cmcontinue\"]\n",
    "        else:\n",
    "            break\n",
    "    return members\n",
    "\n",
    "def collect_people_from_category(root_category):\n",
    "    \"\"\"\n",
    "    Breadth-first traversal from a root category:\n",
    "      - collects 'page' members as rows {Name, Wikipedia_Link}\n",
    "      - enqueues 'subcat' members for further traversal\n",
    "    Returns a deduplicated DataFrame by Name.\n",
    "    \"\"\"\n",
    "    seen, pages, queue = set(), [], [root_category]\n",
    "    while queue:\n",
    "        cat = queue.pop(0)\n",
    "        if cat in seen:\n",
    "            continue\n",
    "        seen.add(cat)\n",
    "        print(f\"ğŸ“‚ ×¡×•×¨×§: {cat}\")\n",
    "        # pages\n",
    "        for m in get_category_members(cat, \"page\"):\n",
    "            title = m[\"title\"]\n",
    "            link  = \"https://en.wikipedia.org/wiki/\" + quote(title.replace(\" \", \"_\"))\n",
    "            pages.append({\"Name\": title, \"Wikipedia_Link\": link})\n",
    "        # subcategories\n",
    "        for sc in get_category_members(cat, \"subcat\"):\n",
    "            queue.append(sc[\"title\"])\n",
    "        time.sleep(0.15)  # be polite\n",
    "    return pd.DataFrame(pages).drop_duplicates(subset=[\"Name\"]).reset_index(drop=True)\n",
    "\n",
    "def safe_slug(cat):\n",
    "    \"\"\"\n",
    "    Converts a category title to a folder/filename-safe slug.\n",
    "    Example:\n",
    "      \"Category:Iranian physicians\" -> \"iranian_physicians\"\n",
    "    \"\"\"\n",
    "    name = cat.replace(\"Category:\", \"\").strip()\n",
    "    name = name.replace(\" \", \"_\")\n",
    "    for bad in ['\"', \"'\", \"'\", \"\"\", \"\"\", \"(\", \")\", \"/\", \"\\\\\", \":\", \"*\", \"?\", \"<\", \">\", \"|\", \",\", \";\", \"â€”\", \"â€“\"]:\n",
    "        name = name.replace(bad, \"\")\n",
    "    # compress consecutive underscores\n",
    "    while \"__\" in name:\n",
    "        name = name.replace(\"__\", \"_\")\n",
    "    return name.lower()\n",
    "\n",
    "# ---------------- Categories to collect (expanded incl. healthcare) ----------------\n",
    "categories = [\n",
    "    # Existing:\n",
    "    \"Category:Government_ministers_of_Iran\",\n",
    "    \"Category:Presidents_of_Iran\",\n",
    "    \"Category:Vice_presidents_of_Iran\",\n",
    "    \"Category:Iranian_ayatollahs\",\n",
    "    \"Category:Iranian_actors\",\n",
    "    \"Category:Iranian_singers\",\n",
    "    \"Category:Iranian_scientists\",\n",
    "    \"Category:Iranian_economists\",\n",
    "    \"Category:Iranian_writers\",\n",
    "    \"Category:Iranian_football_managers\",\n",
    "\n",
    "    # NEW â€” Healthcare related:\n",
    "    \"Category:Hospitals_in_Iran\",\n",
    "    \"Category:Private_hospitals_in_Iran\",\n",
    "    \"Category:Teaching_hospitals_in_Iran\",\n",
    "    \"Category:Iranian_physicians\",\n",
    "    \"Category:Iranian_cardiologists\",\n",
    "    \"Category:Iranian_women_physicians\",\n",
    "    \"Category:21st-century_Iranian_physicians\",\n",
    "    \"Category:19th-century_Iranian_physicians\",\n",
    "    \"Category:Medical_and_health_organisations_based_in_Iran\",\n",
    "    \"Category:Healthcare_in_Iran\",\n",
    "    \"Category:Medicine_in_Iran\",\n",
    "\n",
    "    # NEW â€” Journalists and Media:\n",
    "    \"Category:Iranian_journalists\",\n",
    "    \"Category:Iranian_women_journalists\",\n",
    "    \"Category:Iranian_reporters\",\n",
    "    \"Category:Iranian_correspondents\",\n",
    "    \"Category:Iranian_editors\",\n",
    "    \"Category:Iranian_news_readers\",\n",
    "\n",
    "    # NEW â€” Activists and Political:\n",
    "    \"Category:Iranian_activists\",\n",
    "    \"Category:Iranian_political_activists\",\n",
    "    \"Category:Iranian_human_rights_activists\",\n",
    "    \"Category:Iranian_women_activists\",\n",
    "    \"Category:Iranian_feminists\",\n",
    "    \"Category:Iranian_dissidents\",\n",
    "    \"Category:Iranian_bloggers\",\n",
    "\n",
    "    # NEW â€” Sports:\n",
    "    \"Category:Iranian_footballers\",\n",
    "    \"Category:Iranian_athletes\",\n",
    "    \"Category:Iranian_wrestlers\",\n",
    "    \"Category:Iranian_taekwondo_practitioners\",\n",
    "    \"Category:Iranian_volleyball_players\",\n",
    "    \"Category:Iranian_weightlifters\",\n",
    "\n",
    "    # NEW â€” Entertainment:\n",
    "    \"Category:Iranian_television_actors\",\n",
    "    \"Category:Iranian_film_actors\",\n",
    "    \"Category:Iranian_pop_singers\",\n",
    "    \"Category:Iranian_rappers\",\n",
    "    \"Category:Iranian_comedians\",\n",
    "]\n",
    "\n",
    "# ---------------- Main loop: collect, save per-spec, summarize ----------------\n",
    "csv_files = []\n",
    "summary_rows = []\n",
    "\n",
    "for cat in categories:\n",
    "    try:\n",
    "        slug = safe_slug(cat)\n",
    "        cat_dir = os.path.join(POIS_DIR, slug)\n",
    "        os.makedirs(cat_dir, exist_ok=True)\n",
    "\n",
    "        csv_name = f\"{slug}_wikipedia.csv\"\n",
    "        csv_path = os.path.join(cat_dir, csv_name)\n",
    "\n",
    "        # Skip if file already exists and is not empty\n",
    "        if os.path.exists(csv_path) and os.path.getsize(csv_path) > 0:\n",
    "            print(f\"â© Skipping {slug}: file already exists.\")\n",
    "            summary_rows.append({\"Category\": cat, \"Slug\": slug, \"SavedRows\": \"SKIPPED\", \"CSV\": csv_path})\n",
    "            continue\n",
    "\n",
    "        df = collect_people_from_category(cat)\n",
    "        n = len(df)\n",
    "\n",
    "        if n == 0:\n",
    "            print(f\"âš ï¸ ××™×Ÿ × ×ª×•× ×™× ×¢×‘×•×¨ {cat}\")\n",
    "            summary_rows.append({\"Category\": cat, \"Slug\": slug, \"SavedRows\": 0, \"CSV\": None})\n",
    "            continue\n",
    "\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        csv_files.append(csv_path)\n",
    "\n",
    "        print(f\"âœ… × ×©××¨: {csv_path} | ×¨×©×•××•×ª: {n}\")\n",
    "        summary_rows.append({\"Category\": cat, \"Slug\": slug, \"SavedRows\": n, \"CSV\": csv_path})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘×§×˜×’×•×¨×™×” {cat}: {e}\")\n",
    "        summary_rows.append({\"Category\": cat, \"Slug\": safe_slug(cat), \"SavedRows\": 0, \"CSV\": None})\n",
    "\n",
    "# --- Summary table ---\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Add numeric column for sorting (convert SKIPPED to -1 for sorting purposes)\n",
    "summary_df['SavedRows_Numeric'] = summary_df['SavedRows'].apply(\n",
    "    lambda x: -1 if x == \"SKIPPED\" else (int(x) if isinstance(x, (int, float)) else 0)\n",
    ")\n",
    "\n",
    "# Sort by the numeric column and drop it\n",
    "summary_df = summary_df.sort_values(\"SavedRows_Numeric\", ascending=False).reset_index(drop=True)\n",
    "summary_df = summary_df.drop(columns=['SavedRows_Numeric'])\n",
    "\n",
    "# Calculate total (only numeric values)\n",
    "total_saved = sum(x for x in summary_df[\"SavedRows\"] if isinstance(x, (int, float)))\n",
    "\n",
    "print(\"\\n================= Summary =================\")\n",
    "for _, r in summary_df.iterrows():\n",
    "    base = os.path.basename(r['CSV']) if (pd.notna(r['CSV']) and r['CSV']) else \"-\"\n",
    "    saved_str = str(r['SavedRows'])\n",
    "    print(f\"â€¢ {r['Category']} -> {r['Slug']}: {saved_str} ×¨×©×•××•×ª  |  ×§×•×‘×¥: {base}\")\n",
    "print(f\"\\nğŸ“Š Total across files: {total_saved} rows\")\n",
    "print(\"===========================================\\n\")\n",
    "\n",
    "# Display summary table\n",
    "display(summary_df)\n",
    "\n",
    "# ---------------- ZIP all CSVs ----------------\n",
    "if csv_files:\n",
    "    zip_path = os.path.join(IRAN_DIR, \"Iran_POIs_Wikipedia_Categories.zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file in csv_files:\n",
    "            # Save relative path in zip\n",
    "            arcname = os.path.relpath(file, start=IRAN_DIR)\n",
    "            zipf.write(file, arcname)\n",
    "    print(f\"ğŸ’¾ Created ZIP with all categories: {zip_path}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No CSV files were created â†’ no ZIP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImWc_TSkWUSQ"
   },
   "source": [
    "# **Step 3: Wikidata Enrichment for ALL POI Folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nEId6T_TvQT7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ROOT_DIR: c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\\POIs\n",
      "ğŸ—‚ï¸ × ××¦××• 49 ×ª×™×§×™×•×ª POI ×œ×¢×™×‘×•×“.\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: 19th-century_iranian_physicians\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: 19th-century_iranian_physicians_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: 21st-century_iranian_physicians\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: 21st-century_iranian_physicians_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: Candidates\n",
      "âš ï¸ ×œ× ××¦××ª×™ ×¢××•×“×ª ×§×™×©×•×¨ ×œ×•×•×™×§×™×¤×“×™×”, ×•××™×Ÿ ×’× ×¢××•×“×ª 'Name' ×œ×‘× ×™×™×ª ×§×™×©×•×¨×™×. â€” ×× ×¡×” ×œ×‘× ×•×ª ×§×™×©×•×¨×™ ×•×™×§×™×¤×“×™×” ×Ö¾'Name'...\n",
      "âŒ ××™×Ÿ ×¢××•×“×ª ×§×™×©×•×¨/Name â€” ××“×œ×’ ×¢×œ ×”×ª×™×§×™×™×”.\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: figures\n",
      "âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: government_ministers_of_iran\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: government_ministers_of_iran_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: healthcare_in_iran\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: healthcare_in_iran_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: hospitals_in_iran\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: hospitals_in_iran_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: hospitals_of_iran_by_city\n",
      "âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_activists\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_activists_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 1010)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_activists_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 1010)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_actors\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: iranian_actors_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_athletes\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_athletes_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 115)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_athletes_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 115)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_ayatollahs\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: iranian_ayatollahs_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_bloggers\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_bloggers_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 35)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_bloggers_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 35)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_cardiologists\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: iranian_cardiologists_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_comedians\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_comedians_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 40)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_comedians_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 40)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_correspondents\n",
      "âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_dissidents\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_dissidents_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 110)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_dissidents_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 110)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_economists\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: iranian_economists_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_editors\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_editors_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 38)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_editors_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 38)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_feminists\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_feminists_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 113)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_feminists_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 113)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_film_actors\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_film_actors_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 467)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_film_actors_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 467)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_football_managers\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: iranian_football_managers_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_footballers\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_footballers_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 1629)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_footballers_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 1629)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_human_rights_activists\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_human_rights_activists_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 207)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_human_rights_activists_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 207)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_journalists\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_journalists_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 306)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_journalists_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 306)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_news_readers\n",
      "âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_physicians\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: iranian_physicians_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_political_activists\n",
      "âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_pop_singers\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_pop_singers_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 93)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_pop_singers_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 93)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_rappers\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_rappers_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 16)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_rappers_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 16)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_reporters\n",
      "âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_scientists\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: iranian_scientists_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_singers\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: iranian_singers_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_taekwondo_practitioners\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_taekwondo_practitioners_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 71)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_taekwondo_practitioners_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 71)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_television_actors\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_television_actors_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 331)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_television_actors_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 331)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_volleyball_players\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_volleyball_players_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 97)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_volleyball_players_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 97)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_weightlifters\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_weightlifters_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 103)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_weightlifters_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 103)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_women_activists\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_women_activists_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 62)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_women_activists_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 62)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_women_journalists\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_women_journalists_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 41)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_women_journalists_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 41)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_women_physicians\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: iranian_women_physicians_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_wrestlers\n",
      "ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”: Wikipedia_Link\n",
      "ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: iranian_wrestlers_wikipedia_with_wikidata_ids_and_links.csv  (×©×•×¨×•×ª: 261)\n",
      "âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: iranian_wrestlers_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv  (×©×•×¨×•×ª: 261)\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: iranian_writers\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: iranian_writers_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: medical_and_health_organisations_based_in_iran\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: medical_and_health_organisations_based_in_iran_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: medicine_in_iran\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: medicine_in_iran_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: presidents_of_iran\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: presidents_of_iran_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: private_hospitals_in_iran\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: private_hospitals_in_iran_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: teaching_hospitals_in_iran\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: teaching_hospitals_in_iran_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: tools\n",
      "âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\n",
      "\n",
      "ğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: vice_presidents_of_iran\n",
      "âœ… SKIP: Wikidata enrichment already complete\n",
      "   File: vice_presidents_of_iran_wikipedia_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
      "   Found 7 Wikidata columns with data\n",
      "\n",
      "ğŸ‰ ×”×•×©×œ× ×¢×™×‘×•×“ ×œ×›×œ ×”×ª×™×§×™×•×ª!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Step 3: Wikidata Enrichment for ALL POI Folders (UPDATED)\n",
    "# ============================================\n",
    "# For each subfolder under POIs:\n",
    "#  1) Locate *_wikipedia.csv (or the first .csv)\n",
    "#  2) Detect the Wikipedia link column (fallback to Name)\n",
    "#  3) Resolve wikidata_qid (+ wikidata_url)\n",
    "#  4) Fetch details via SPARQL in batches\n",
    "#  5) Save:\n",
    "#     a) *_with_wikidata_ids_and_links.csv\n",
    "#     b) *_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
    "# ============================================\n",
    "\n",
    "import os, re, time, glob, json, math, random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------- Dynamic path detection ----------------\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "ROOT_DIR = os.path.join(IRAN_DIR, \"POIs\")\n",
    "os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "print(f\"ğŸ“ ROOT_DIR: {ROOT_DIR}\")\n",
    "\n",
    "# ---------------- HTTP / Endpoints ----------------\n",
    "UA = {\"User-Agent\": \"SCE-DS-FinalProject/1.0 (contact: student@example.com)\"}\n",
    "MEDIAWIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "SPARQL = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# ---------------- Helpers: retries ----------------\n",
    "def http_get(url, params=None, headers=None, timeout=30, max_tries=4, sleep_base=0.8):\n",
    "    \"\"\"\n",
    "    Simple GET with retries/backoff. Jitters a bit to be polite.\n",
    "    \"\"\"\n",
    "    headers = headers or UA\n",
    "    for attempt in range(1, max_tries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "            if r.status_code == 403 and headers is UA:\n",
    "                # fallback UA\n",
    "                r = requests.get(url, params=params, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            if attempt == max_tries:\n",
    "                # last attempt: raise\n",
    "                raise\n",
    "            sleep_s = sleep_base * attempt + random.uniform(0, 0.3)\n",
    "            time.sleep(sleep_s)\n",
    "    # Shouldn't reach here\n",
    "    raise RuntimeError(\"GET retries exhausted\")\n",
    "\n",
    "# ---------------- Column detection ----------------\n",
    "def find_wikipedia_column(df: pd.DataFrame) -> str:\n",
    "    # First pass: headers with hints\n",
    "    for name in df.columns:\n",
    "        n = str(name).strip().lower()\n",
    "        if any(k in n for k in [\"wikipedia\", \"wiki\", \"link\", \"url\"]):\n",
    "            return name\n",
    "    # Second pass: sample content\n",
    "    for name in df.columns:\n",
    "        sample = \" \".join(map(str, df[name].dropna().astype(str).head(20).tolist())).lower()\n",
    "        if \"wikipedia.org\" in sample or sample.startswith(\"http\"):\n",
    "            return name\n",
    "    # Fallback\n",
    "    if \"Wikipedia_Link\" in df.columns:\n",
    "        return \"Wikipedia_Link\"\n",
    "    # As a last resort: if Name exists, we'll construct enwiki URLs from it\n",
    "    if \"Name\" in df.columns:\n",
    "        return None  # signal to construct from Name\n",
    "    raise ValueError(\"×œ× ××¦××ª×™ ×¢××•×“×ª ×§×™×©×•×¨ ×œ×•×•×™×§×™×¤×“×™×”, ×•××™×Ÿ ×’× ×¢××•×“×ª 'Name' ×œ×‘× ×™×™×ª ×§×™×©×•×¨×™×.\")\n",
    "\n",
    "def name_to_enwiki_url(name: str) -> str:\n",
    "    from urllib.parse import quote\n",
    "    if not isinstance(name, str) or not name.strip():\n",
    "        return None\n",
    "    title = name.strip().replace(\" \", \"_\")\n",
    "    return f\"https://en.wikipedia.org/wiki/{quote(title)}\"\n",
    "\n",
    "def wikipedia_url_to_title(url: str) -> str | None:\n",
    "    if not isinstance(url, str) or not url:\n",
    "        return None\n",
    "    try:\n",
    "        url = url.split(\"?\")[0].split(\"#\")[0]\n",
    "        title = url.rstrip(\"/\").split(\"/\")[-1]\n",
    "        return title if title else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------- QID resolution (with cache) ----------------\n",
    "qid_cache = {}\n",
    "\n",
    "def get_qid_from_wikipedia_url(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Resolve QID for a single Wikipedia page using the pageprops endpoint.\n",
    "    \"\"\"\n",
    "    title = wikipedia_url_to_title(url)\n",
    "    if not title:\n",
    "        return None\n",
    "    if title in qid_cache:\n",
    "        return qid_cache[title]\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"pageprops\",\n",
    "        \"redirects\": 1,\n",
    "        \"titles\": title,\n",
    "    }\n",
    "    try:\n",
    "        r = http_get(MEDIAWIKI_API, params=params, headers=UA, timeout=25)\n",
    "        data = r.json()\n",
    "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "        qid = page.get(\"pageprops\", {}).get(\"wikibase_item\")\n",
    "        if qid:\n",
    "            qid_cache[title] = qid\n",
    "        return qid\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------- SPARQL batch fetch ----------------\n",
    "def batch_fetch_wikidata_details(qids: list[str], batch_size: int = 50) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch properties for many QIDs via SPARQL using VALUES batching.\n",
    "    Returns dict: { QID: {fields...}, ... }\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # Normalize & unique\n",
    "    qids = [q for q in qids if isinstance(q, str) and q.strip()]\n",
    "    uniq = sorted(set(qids))\n",
    "    if not uniq:\n",
    "        return results\n",
    "\n",
    "    def run_batch(subset):\n",
    "        values = \" \".join(f\"wd:{q}\" for q in subset)\n",
    "        query = f\"\"\"\n",
    "        SELECT ?item ?genderLabel ?occupationLabel ?countryLabel ?placeOfBirthLabel ?dateOfBirth WHERE {{\n",
    "          VALUES ?item {{ {values} }}\n",
    "          OPTIONAL {{ ?item wdt:P21 ?gender. }}\n",
    "          OPTIONAL {{ ?item wdt:P106 ?occupation. }}\n",
    "          OPTIONAL {{ ?item wdt:P27 ?country. }}\n",
    "          OPTIONAL {{ ?item wdt:P19 ?placeOfBirth. }}\n",
    "          OPTIONAL {{ ?item wdt:P569 ?dateOfBirth. }}\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en,fa,en-gb\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        r = http_get(SPARQL, params={\"query\": query, \"format\": \"json\"}, headers=UA, timeout=45)\n",
    "        rows = r.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "        # collect multiple rows per item\n",
    "        tmp = {}\n",
    "        for b in rows:\n",
    "            uri = b.get(\"item\", {}).get(\"value\", \"\")\n",
    "            q = uri.rsplit(\"/\", 1)[-1] if uri else None\n",
    "            if not q:\n",
    "                continue\n",
    "            cur = tmp.setdefault(q, {\"wikidata_gender\": set(),\n",
    "                                     \"wikidata_occupation\": set(),\n",
    "                                     \"wikidata_country_of_citizenship\": set(),\n",
    "                                     \"wikidata_place_of_birth\": None,\n",
    "                                     \"wikidata_date_of_birth\": None})\n",
    "            if \"genderLabel\" in b:\n",
    "                cur[\"wikidata_gender\"].add(b[\"genderLabel\"][\"value\"])\n",
    "            if \"occupationLabel\" in b:\n",
    "                cur[\"wikidata_occupation\"].add(b[\"occupationLabel\"][\"value\"])\n",
    "            if \"countryLabel\" in b:\n",
    "                cur[\"wikidata_country_of_citizenship\"].add(b[\"countryLabel\"][\"value\"])\n",
    "            if \"placeOfBirthLabel\" in b and not cur[\"wikidata_place_of_birth\"]:\n",
    "                cur[\"wikidata_place_of_birth\"] = b[\"placeOfBirthLabel\"][\"value\"]\n",
    "            if \"dateOfBirth\" in b and not cur[\"wikidata_date_of_birth\"]:\n",
    "                cur[\"wikidata_date_of_birth\"] = b[\"dateOfBirth\"][\"value\"]\n",
    "        # flatten sets\n",
    "        for q, d in tmp.items():\n",
    "            results[q] = {\n",
    "                \"wikidata_gender\": \"; \".join(sorted(d[\"wikidata_gender\"])) or None,\n",
    "                \"wikidata_occupation\": \"; \".join(sorted(d[\"wikidata_occupation\"])) or None,\n",
    "                \"wikidata_country_of_citizenship\": \"; \".join(sorted(d[\"wikidata_country_of_citizenship\"])) or None,\n",
    "                \"wikidata_place_of_birth\": d[\"wikidata_place_of_birth\"],\n",
    "                \"wikidata_date_of_birth\": d[\"wikidata_date_of_birth\"],\n",
    "            }\n",
    "\n",
    "    for i in range(0, len(uniq), batch_size):\n",
    "        subset = uniq[i:i+batch_size]\n",
    "        # polite pacing\n",
    "        try:\n",
    "            run_batch(subset)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ SPARQL batch failed ({subset[0]}..): {e}. ×× ×¡×” ×©×•×‘ ×‘×§×‘×•×¦×•×ª ×§×˜× ×•×ª ×™×•×ª×¨...\")\n",
    "            # fallback: try half batch to circumvent transient errors\n",
    "            mid = len(subset)//2 or 1\n",
    "            for chunk in (subset[:mid], subset[mid:]):\n",
    "                try:\n",
    "                    run_batch(chunk)\n",
    "                except Exception as ee:\n",
    "                    print(f\"âŒ SPARQL sub-batch failed ({chunk[0]}..): {ee}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ---------------- Main processing ----------------\n",
    "folders = [f for f in sorted(os.listdir(ROOT_DIR)) if os.path.isdir(os.path.join(ROOT_DIR, f))]\n",
    "print(f\"ğŸ—‚ï¸ × ××¦××• {len(folders)} ×ª×™×§×™×•×ª POI ×œ×¢×™×‘×•×“.\")\n",
    "\n",
    "for folder in folders:\n",
    "    FOLDER_PATH = os.path.join(ROOT_DIR, folder)\n",
    "    print(f\"\\nğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: {folder}\")\n",
    "\n",
    "    # Locate input CSV (prefer *_wikipedia.csv)\n",
    "    candidates = sorted(glob.glob(os.path.join(FOLDER_PATH, \"*_wikipedia.csv\")))\n",
    "    if not candidates:\n",
    "        candidates = sorted(glob.glob(os.path.join(FOLDER_PATH, \"*.csv\")))\n",
    "    if not candidates:\n",
    "        print(\"âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\")\n",
    "        continue\n",
    "\n",
    "    INPUT_CSV = candidates[0]\n",
    "    base_name = re.sub(r\"\\.csv$\", \"\", os.path.basename(INPUT_CSV))\n",
    "    \n",
    "    # âš¡ SMART SKIP: Check if Wikidata enrichment already complete\n",
    "    detailed_path = os.path.join(FOLDER_PATH, f\"{base_name}_with_wikidata_ids_and_links_wikidata_detailed.csv\")\n",
    "    if os.path.exists(detailed_path):\n",
    "        try:\n",
    "            df_check = pd.read_csv(detailed_path, nrows=5)\n",
    "            wd_cols = [c for c in df_check.columns if c.startswith('wikidata_')]\n",
    "            if wd_cols and df_check[wd_cols].notna().any().any():\n",
    "                print(f\"âœ… SKIP: Wikidata enrichment already complete\")\n",
    "                print(f\"   File: {os.path.basename(detailed_path)}\")\n",
    "                print(f\"   Found {len(wd_cols)} Wikidata columns with data\")\n",
    "                continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    candidates = sorted(glob.glob(os.path.join(FOLDER_PATH, \"*_wikipedia.csv\")))\n",
    "    if not candidates:\n",
    "        candidates = sorted(glob.glob(os.path.join(FOLDER_PATH, \"*.csv\")))\n",
    "    if not candidates:\n",
    "        print(\"âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\")\n",
    "        continue\n",
    "\n",
    "    INPUT_CSV = candidates[0]\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×œ× ×”×¦×œ×—×ª×™ ×œ×§×¨×•× ××ª ×”×§×•×‘×¥ {os.path.basename(INPUT_CSV)}: {e} â€” ××“×œ×’.\")\n",
    "        continue\n",
    "\n",
    "    # Detect / construct Wikipedia links\n",
    "    try:\n",
    "        wiki_col = find_wikipedia_column(df)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ {e} â€” ×× ×¡×” ×œ×‘× ×•×ª ×§×™×©×•×¨×™ ×•×™×§×™×¤×“×™×” ×Ö¾'Name'...\")\n",
    "        wiki_col = None\n",
    "\n",
    "    if wiki_col is None:\n",
    "        if \"Name\" in df.columns:\n",
    "            df[\"Wikipedia_Link\"] = df[\"Name\"].apply(name_to_enwiki_url)\n",
    "            wiki_col = \"Wikipedia_Link\"\n",
    "        else:\n",
    "            print(\"âŒ ××™×Ÿ ×¢××•×“×ª ×§×™×©×•×¨/Name â€” ××“×œ×’ ×¢×œ ×”×ª×™×§×™×™×”.\")\n",
    "            continue\n",
    "\n",
    "    print(\"ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”:\", wiki_col)\n",
    "\n",
    "    # ---- A) Resolve QIDs (with simple in-memory cache) ----\n",
    "    qids = []\n",
    "    for url in df[wiki_col].fillna(\"\"):\n",
    "        qids.append(get_qid_from_wikipedia_url(url))\n",
    "        time.sleep(0.08)  # polite throttle\n",
    "\n",
    "    df[\"wikidata_qid\"] = qids\n",
    "    df[\"wikidata_url\"] = df[\"wikidata_qid\"].apply(lambda q: f\"https://www.wikidata.org/wiki/{q}\" if isinstance(q, str) and q else None)\n",
    "\n",
    "    # Save mid file\n",
    "    mid_path = os.path.join(\n",
    "        FOLDER_PATH,\n",
    "        re.sub(r\"\\.csv$\", \"\", os.path.basename(INPUT_CSV)) + \"_with_wikidata_ids_and_links.csv\"\n",
    "    )\n",
    "    try:\n",
    "        df.to_csv(mid_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: {os.path.basename(mid_path)}  (×©×•×¨×•×ª: {len(df)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘×©××™×¨×ª ×§×•×‘×¥ ×‘×™× ×™×™×: {e}\")\n",
    "\n",
    "    # ---- B) Fetch details via SPARQL in batches ----\n",
    "    uniq_qids = [q for q in pd.Series(df[\"wikidata_qid\"]).dropna().astype(str).unique().tolist() if q]\n",
    "    details_map = batch_fetch_wikidata_details(uniq_qids, batch_size=50)\n",
    "\n",
    "    details_rows = []\n",
    "    for q in df[\"wikidata_qid\"]:\n",
    "        if isinstance(q, str) and q in details_map:\n",
    "            details_rows.append(details_map[q])\n",
    "        else:\n",
    "            details_rows.append({\n",
    "                \"wikidata_gender\": None,\n",
    "                \"wikidata_occupation\": None,\n",
    "                \"wikidata_country_of_citizenship\": None,\n",
    "                \"wikidata_place_of_birth\": None,\n",
    "                \"wikidata_date_of_birth\": None,\n",
    "            })\n",
    "\n",
    "    details_df = pd.DataFrame(details_rows)\n",
    "    enriched = pd.concat([df, details_df], axis=1)\n",
    "\n",
    "    out_path = os.path.join(\n",
    "        FOLDER_PATH,\n",
    "        re.sub(r\"\\.csv$\", \"\", os.path.basename(INPUT_CSV)) + \"_with_wikidata_ids_and_links_wikidata_detailed.csv\"\n",
    "    )\n",
    "    try:\n",
    "        enriched.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: {os.path.basename(out_path)}  (×©×•×¨×•×ª: {len(enriched)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘×©××™×¨×ª ×”×¤×œ×˜ ×”×¡×•×¤×™: {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ ×”×•×©×œ× ×¢×™×‘×•×“ ×œ×›×œ ×”×ª×™×§×™×•×ª!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv2SvQhGWtAI"
   },
   "source": [
    "# Step 4: Find Twitter handles for ALL POI folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsVDW3ZmG45R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ROOT: c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\\POIs\n",
      "\n",
      "ğŸ“‚ 19th-century_iranian_physicians\n",
      "âœ… Saved: 19th-century_iranian_physicians_wikipedia_with_wikidata_ids_and_links_wikidata_detailed_with_twitter.csv | found 0 / 11\n",
      "\n",
      "ğŸ“‚ 21st-century_iranian_physicians\n",
      "âœ… SKIP: Twitter enrichment already complete\n",
      "   File: 21st-century_iranian_physicians_wikipedia_with_wikidata_ids_and_links_wikidata_detailed_with_twitter.csv (1+ handles)\n",
      "\n",
      "ğŸ“‚ Candidates\n",
      "âœ… SKIP: Twitter enrichment already complete\n",
      "   File: Candidates_statistics_with_twitter.csv (5+ handles)\n",
      "âš ï¸ ××™×Ÿ CSV ×‘×ª×™×§×™×™×” figures â€” ×“×™×œ×•×’\n",
      "\n",
      "ğŸ“‚ government_ministers_of_iran\n",
      "âœ… SKIP: Twitter enrichment already complete\n",
      "   File: government_ministers_of_iran_wikipedia_with_wikidata_ids_and_links_wikidata_detailed_with_twitter.csv (1+ handles)\n",
      "\n",
      "ğŸ“‚ healthcare_in_iran\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Step 4: Find Twitter handles for ALL POI folders\n",
    "# Adds only: Twitter_username + Twitter_url (no twitter_source)\n",
    "# ============================================\n",
    "\n",
    "import os, re, time, json, glob, requests, pandas as pd, random\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, unquote\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ---- Dynamic path detection ----\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "# ×™×¨×•×¥ ×¢×œ ×›×œ ×”×ª×™×§×™×•×ª ×ª×—×ª POIs\n",
    "REL_ROOT = \"POIs\"\n",
    "ROOT = os.path.join(IRAN_DIR, REL_ROOT)\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# ---------- HTTP session with retries ----------\n",
    "UA = {\"User-Agent\": \"SCE-DS-FinalProject/1.0 (contact: student@example.com)\"}\n",
    "session = requests.Session()\n",
    "session.headers.update(UA)\n",
    "\n",
    "# urllib3 v2: allowed_methods (not method_whitelist). ×¢×“×™×£ frozenset\n",
    "retry_cfg = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=frozenset({\"GET\"}),\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retry_cfg))\n",
    "session.mount(\"http://\", HTTPAdapter(max_retries=retry_cfg))\n",
    "\n",
    "SPARQL = \"https://query.wikidata.org/sparql\"\n",
    "MEDIAWIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def pick_csv(folder):\n",
    "    for pat in [\"*_with_wikidata_ids_and_links_wikidata_detailed*.csv\",\n",
    "                \"*_with_wikidata_ids_and_links*.csv\",\n",
    "                \"*_wikipedia.csv\",\n",
    "                \"*.csv\"]:\n",
    "        cand = sorted(glob.glob(os.path.join(folder, pat)))\n",
    "        # ×¡× ×Ÿ ×§×‘×¦×™× ×©×›×‘×¨ ××›×™×œ×™× _with_twitter (××œ×” ×”× ×¤×œ×˜×™×, ×œ× ×§×œ×˜×™×)\n",
    "        cand = [c for c in cand if not c.endswith(\"_with_twitter.csv\")]\n",
    "        if cand: return cand[0]\n",
    "    return None\n",
    "\n",
    "def clean_handle(h):\n",
    "    if not isinstance(h, str):\n",
    "        return None\n",
    "    h = h.strip().lstrip(\"@\")\n",
    "    h = re.sub(r\"[/?#].*$\", \"\", h)\n",
    "    m = re.match(r\"^[A-Za-z0-9_]{1,15}$\", h)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def extract_handle_from_url(url):\n",
    "    \"\"\"\n",
    "    ××—×œ×¥ ×™×“×™×ª ××›×ª×•×‘×ª twitter/x ×× ×§×™×™××ª. ×œ×™× ×§×™× ××¡×•×’ /i/user/12345 ×œ× ××›×™×œ×™× ×™×“×™×ª => × ×—×–×™×¨ None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        u = url.split(\"?\")[0].split(\"#\")[0]\n",
    "        host = urlparse(u).netloc.lower()\n",
    "        if any(d in host for d in [\"twitter.com\",\"x.com\",\"mobile.twitter.com\",\"www.twitter.com\",\"www.x.com\"]):\n",
    "            parts = urlparse(u).path.strip(\"/\").split(\"/\")\n",
    "            if parts and parts[0].lower() not in {\"i\",\"intent\",\"share\",\"home\"}:\n",
    "                return clean_handle(parts[0])\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def build_twitter_url(handle):\n",
    "    if not isinstance(handle, str) or not handle.strip():\n",
    "        return None\n",
    "    return f\"https://x.com/{handle.strip().lstrip('@')}\"\n",
    "\n",
    "def find_wikipedia_column(df):\n",
    "    # ×¨××©×™×ª: ×œ×¤×™ ×©× ×¢××•×“×”\n",
    "    for c in df.columns:\n",
    "        n = str(c).lower()\n",
    "        if any(k in n for k in [\"wikipedia\", \"wiki\", \"link\", \"url\"]):\n",
    "            return c\n",
    "    # ×©× ×™×ª: ×œ×¤×™ ×ª×•×›×Ÿ\n",
    "    for c in df.columns:\n",
    "        vals = \" \".join(map(str, df[c].dropna().astype(str).head(15).tolist())).lower()\n",
    "        if \"wikipedia.org\" in vals or \"https://\" in vals or \"http://\" in vals:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def http_get(url, params=None, headers=None, timeout=30, tries=3, backoff=0.7):\n",
    "    headers = headers or UA\n",
    "    for attempt in range(1, tries+1):\n",
    "        try:\n",
    "            r = session.get(url, params=params, headers=headers, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            if attempt == tries:\n",
    "                raise\n",
    "            time.sleep(backoff * attempt + random.uniform(0, 0.2))\n",
    "\n",
    "def guess_twitter_from_wiki(title_or_url):\n",
    "    \"\"\"\n",
    "    × ×¡×” ×œ×—×œ×¥ ×™×“×™×ª ××”×“×£ ×‘×•×•×™×§×™×¤×“×™×”:\n",
    "    - External links\n",
    "    - ×˜×§×¡×˜ ×”××§×•×¨ (wikitext) ×¢× regex\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(title_or_url, str) and title_or_url.startswith((\"http://\", \"https://\")):\n",
    "            title = unquote(title_or_url.split(\"?\")[0].split(\"#\")[0].rstrip(\"/\").split(\"/\")[-1])\n",
    "            title = title.replace(\"_\", \" \")\n",
    "        else:\n",
    "            title = str(title_or_url).strip().replace(\"_\", \" \")\n",
    "        if not title:\n",
    "            return None\n",
    "\n",
    "        r = http_get(MEDIAWIKI_API, params={\n",
    "            \"action\": \"parse\",\n",
    "            \"format\": \"json\",\n",
    "            \"page\": title,\n",
    "            \"prop\": \"externallinks|wikitext\",\n",
    "            \"redirects\": 1\n",
    "        }, timeout=30, tries=3)\n",
    "        data = r.json()\n",
    "\n",
    "        links = data.get(\"parse\", {}).get(\"externallinks\", []) or []\n",
    "        for ln in links:\n",
    "            h = extract_handle_from_url(ln)\n",
    "            if h:\n",
    "                return h\n",
    "\n",
    "        wt = data.get(\"parse\", {}).get(\"wikitext\", {}).get(\"*\", \"\")\n",
    "        # ×ª×•×¤×¡ ×’× twitter ×•×’× x.com\n",
    "        for m in re.finditer(r\"(?:https?://)?(?:www\\.)?(?:twitter|x)\\.com/([A-Za-z0-9_]{1,15})\", wt, re.I):\n",
    "            h = clean_handle(m.group(1))\n",
    "            if h:\n",
    "                return h\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def batch_p2002(qids):\n",
    "    \"\"\"\n",
    "    ××—×–×™×¨ ××¤×” { QID: handle or None } ×¢× retry.\n",
    "    \"\"\"\n",
    "    qids = [q for q in qids if isinstance(q, str) and q]\n",
    "    if not qids:\n",
    "        return {}\n",
    "    values = \" \".join(f\"(wd:{q})\" for q in qids)\n",
    "    q = f\"\"\"\n",
    "    SELECT ?item ?twitter WHERE {{\n",
    "      VALUES (?item) {{ {values} }}\n",
    "      OPTIONAL {{ ?item wdt:P2002 ?twitter. }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    # retry ×“×¨×š http_get\n",
    "    r = http_get(SPARQL, params={\"query\": q, \"format\": \"json\"}, timeout=45, tries=3, backoff=0.9)\n",
    "    rows = r.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "    out = {}\n",
    "    for b in rows:\n",
    "        qid = b[\"item\"][\"value\"].rsplit(\"/\",1)[-1]\n",
    "        tw = b.get(\"twitter\", {}).get(\"value\")\n",
    "        out[qid] = clean_handle(tw) if tw else None\n",
    "    return out\n",
    "\n",
    "# cache (QID -> handle)\n",
    "cache_file = os.path.join(ROOT, \"_twitter_cache.json\")\n",
    "twitter_cache = {}\n",
    "if os.path.exists(cache_file):\n",
    "    try:\n",
    "        twitter_cache = json.load(open(cache_file, \"r\", encoding=\"utf-8\"))\n",
    "    except:\n",
    "        twitter_cache = {}\n",
    "\n",
    "# ---------- process all subfolders ----------\n",
    "folders = [os.path.join(ROOT, d) for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))]\n",
    "folders.sort()\n",
    "\n",
    "for FOLDER in folders:\n",
    "    in_csv = pick_csv(FOLDER)\n",
    "    if not in_csv:\n",
    "        print(f\"âš ï¸ ××™×Ÿ CSV ×‘×ª×™×§×™×™×” {os.path.basename(FOLDER)} â€” ×“×™×œ×•×’\")\n",
    "        continue\n",
    "\n",
    "    # âš¡ SMART SKIP: Check if Twitter enrichment already complete\n",
    "    # ×‘×“×™×§×ª ×›×œ ×”×¤×•×¨××˜×™× ×”××¤×©×¨×™×™× ×©×œ ×§×‘×¦×™ Twitter\n",
    "    base_name = os.path.splitext(os.path.basename(in_csv))[0]\n",
    "    possible_outputs = [\n",
    "        os.path.join(FOLDER, base_name + \"_with_twitter.csv\"),\n",
    "        os.path.join(FOLDER, base_name.replace(\"_wikidata_detailed\", \"\") + \"_with_twitter.csv\"),\n",
    "        # ×× ×”×§×•×‘×¥ × ×§×¨× *_detailed.csv, × ×¡×” ×’× ×œ×œ× detailed\n",
    "        os.path.join(FOLDER, base_name.replace(\"_detailed\", \"\") + \"_with_twitter.csv\")\n",
    "    ]\n",
    "    \n",
    "    skip = False\n",
    "    for out_path in possible_outputs:\n",
    "        if os.path.exists(out_path):\n",
    "            try:\n",
    "                df_check = pd.read_csv(out_path, nrows=5)\n",
    "                if 'Twitter_username' in df_check.columns:\n",
    "                    twitter_count = df_check['Twitter_username'].notna().sum()\n",
    "                    if twitter_count > 0:\n",
    "                        print(f\"\\nğŸ“‚ {os.path.basename(FOLDER)}\")\n",
    "                        print(f\"âœ… SKIP: Twitter enrichment already complete\")\n",
    "                        print(f\"   File: {os.path.basename(out_path)} ({twitter_count}+ handles)\")\n",
    "                        skip = True\n",
    "                        break\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    if skip:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ“‚ {os.path.basename(FOLDER)}\")\n",
    "    try:\n",
    "        df = pd.read_csv(in_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×œ× ×”×¦×œ×—×ª×™ ×œ×§×¨×•× ××ª ×”×§×•×‘×¥: {e} â€” ×“×™×œ×•×’\")\n",
    "        continue\n",
    "\n",
    "    wiki_col = find_wikipedia_column(df)\n",
    "    if \"wikidata_qid\" not in df.columns:\n",
    "        df[\"wikidata_qid\"] = None\n",
    "\n",
    "    if \"Twitter_username\" not in df.columns:\n",
    "        df[\"Twitter_username\"] = None\n",
    "\n",
    "    # 1) Wikidata P2002 (with cache)\n",
    "    qids = [q for q in df[\"wikidata_qid\"].dropna().astype(str).unique() if q]\n",
    "    p2002_map = {}\n",
    "    # ×§×•×“× ××”-cache\n",
    "    for q in qids:\n",
    "        if q in twitter_cache:\n",
    "            p2002_map[q] = twitter_cache[q]\n",
    "    # ××” ×©×—×¡×¨ â€” ×œ×©××™×œ×ª× (×‘×§×‘×•×¦×•×ª)\n",
    "    to_query = [q for q in qids if q not in p2002_map]\n",
    "    for i in range(0, len(to_query), 60):\n",
    "        part = to_query[i:i+60]\n",
    "        try:\n",
    "            m = batch_p2002(part)\n",
    "            p2002_map.update(m)\n",
    "            # ×œ×¢×“×›×Ÿ cache ×¨×§ ×›×©×™×© ×™×“×™×ª (×›×“×™ ×œ× â€œ×œ×§×‘×¢â€ None)\n",
    "            twitter_cache.update({k: v for k, v in m.items() if v})\n",
    "            time.sleep(0.25)\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ SPARQL batch failed:\", e)\n",
    "\n",
    "    # ×›×ª×™×‘×” ×œ×¤×™ QID\n",
    "    for idx, row in df.iterrows():\n",
    "        qid = row.get(\"wikidata_qid\")\n",
    "        if isinstance(qid, str) and qid in p2002_map and p2002_map[qid]:\n",
    "            if not df.at[idx, \"Twitter_username\"]:\n",
    "                df.at[idx, \"Twitter_username\"] = p2002_map[qid]\n",
    "\n",
    "    # 2) Wikipedia fallback\n",
    "    if wiki_col:\n",
    "        missing = df[\"Twitter_username\"].isna()\n",
    "        for idx, row in df[missing].iterrows():\n",
    "            url = row[wiki_col]\n",
    "            if isinstance(url, str) and url.strip():\n",
    "                h = guess_twitter_from_wiki(url)\n",
    "                if h:\n",
    "                    df.at[idx, \"Twitter_username\"] = h\n",
    "                    # ×× ×™×© ×’× QID â€” ×¢×“×›×Ÿ cache (× ×—×¡×•×š ×‘×”×¤×¢×œ×•×ª ×¢×ª×™×“×™×•×ª)\n",
    "                    qid = row.get(\"wikidata_qid\")\n",
    "                    if isinstance(qid, str) and qid:\n",
    "                        twitter_cache[qid] = h\n",
    "\n",
    "    # × ×™×§×•×™ + URL\n",
    "    df[\"Twitter_username\"] = df[\"Twitter_username\"].apply(lambda h: clean_handle(h) if isinstance(h, str) else None)\n",
    "    df[\"Twitter_url\"] = df[\"Twitter_username\"].apply(build_twitter_url)\n",
    "\n",
    "    # save (UTF-8-SIG to be Excel-friendly)\n",
    "    out_path = os.path.join(FOLDER, os.path.splitext(os.path.basename(in_csv))[0] + \"_with_twitter.csv\")\n",
    "    try:\n",
    "        df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"âœ… Saved: {os.path.basename(out_path)} | found {df['Twitter_username'].notna().sum()} / {len(df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©××™×¨×” × ×›×©×œ×”: {e}\")\n",
    "\n",
    "# save cache\n",
    "try:\n",
    "    with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(twitter_cache, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\nğŸ’¾ cache saved:\", cache_file)\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ couldn't save cache:\", e)\n",
    "\n",
    "print(\"\\nğŸ‰ Done â€” Step 4 completed for all POI folders (username + url only).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLh9s1prXDtv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AwD-8mnAmj_z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ROOT: c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\\POIs\n",
      "âœ… Saved: c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\\POIs\\Manual_Search_POIs.csv | rows: 13470\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>poi_name</th>\n",
       "      <th>Twitter_username</th>\n",
       "      <th>Twitter_url</th>\n",
       "      <th>source_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Iran activist</td>\n",
       "      <td>Execution of Majid Kazemi</td>\n",
       "      <td>1500tasvir</td>\n",
       "      <td>https://x.com/1500tasvir</td>\n",
       "      <td>iranian_activists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>ã¾ã‚†ã¿ã‚“</td>\n",
       "      <td>2011</td>\n",
       "      <td>https://x.com/2011</td>\n",
       "      <td>Candidates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Iran writer</td>\n",
       "      <td>Mehrdad Afsari</td>\n",
       "      <td>2011</td>\n",
       "      <td>https://x.com/2011</td>\n",
       "      <td>iranian_writers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>2012 ABC</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://x.com/2012</td>\n",
       "      <td>Candidates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Iran writer</td>\n",
       "      <td>Behdad Sami</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://x.com/2012</td>\n",
       "      <td>iranian_writers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Iran actor</td>\n",
       "      <td>Michael Shahrestani</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>iranian_actors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>Candidates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Iran film actor</td>\n",
       "      <td>Michael Shahrestani</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>iranian_film_actors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Iran president</td>\n",
       "      <td>Joint Comprehensive Plan of Action</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Iran president</td>\n",
       "      <td>Reactions to the Joint Comprehensive Plan of A...</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Iran writer</td>\n",
       "      <td>Michael Shahrestani</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>iranian_writers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Iran activist</td>\n",
       "      <td>Iran hostage crisis</td>\n",
       "      <td>2016</td>\n",
       "      <td>https://x.com/2016</td>\n",
       "      <td>iranian_activists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>https://x.com/2016</td>\n",
       "      <td>Candidates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Iran president</td>\n",
       "      <td>Criticism of the Joint Comprehensive Plan of A...</td>\n",
       "      <td>2016</td>\n",
       "      <td>https://x.com/2016</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>Sam Beck</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://x.com/2017</td>\n",
       "      <td>Candidates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Iran president</td>\n",
       "      <td>Aftermath of the Joint Comprehensive Plan of A...</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://x.com/2017</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Iran activist</td>\n",
       "      <td>Azadeh Shafiq</td>\n",
       "      <td>2020</td>\n",
       "      <td>https://x.com/2020</td>\n",
       "      <td>iranian_activists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Iran activist</td>\n",
       "      <td>Mahsa Amini protests</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://x.com/2022</td>\n",
       "      <td>iranian_activists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://x.com/2022</td>\n",
       "      <td>Candidates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Iran president</td>\n",
       "      <td>Mahsa Amini protests</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://x.com/2022</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            keyword                                           poi_name  \\\n",
       "0     Iran activist                          Execution of Majid Kazemi   \n",
       "1    Iran candidate                                               ã¾ã‚†ã¿ã‚“   \n",
       "2       Iran writer                                     Mehrdad Afsari   \n",
       "3    Iran candidate                                           2012 ABC   \n",
       "4       Iran writer                                        Behdad Sami   \n",
       "5        Iran actor                                Michael Shahrestani   \n",
       "6    Iran candidate                                                NaN   \n",
       "7   Iran film actor                                Michael Shahrestani   \n",
       "8    Iran president                 Joint Comprehensive Plan of Action   \n",
       "9    Iran president  Reactions to the Joint Comprehensive Plan of A...   \n",
       "10      Iran writer                                Michael Shahrestani   \n",
       "11    Iran activist                                Iran hostage crisis   \n",
       "12   Iran candidate                                               2016   \n",
       "13   Iran president  Criticism of the Joint Comprehensive Plan of A...   \n",
       "14   Iran candidate                                           Sam Beck   \n",
       "15   Iran president  Aftermath of the Joint Comprehensive Plan of A...   \n",
       "16    Iran activist                                      Azadeh Shafiq   \n",
       "17    Iran activist                               Mahsa Amini protests   \n",
       "18   Iran candidate                                                NaN   \n",
       "19   Iran president                               Mahsa Amini protests   \n",
       "\n",
       "   Twitter_username               Twitter_url        source_folder  \n",
       "0        1500tasvir  https://x.com/1500tasvir    iranian_activists  \n",
       "1              2011        https://x.com/2011           Candidates  \n",
       "2              2011        https://x.com/2011      iranian_writers  \n",
       "3              2012        https://x.com/2012           Candidates  \n",
       "4              2012        https://x.com/2012      iranian_writers  \n",
       "5              2015        https://x.com/2015       iranian_actors  \n",
       "6              2015        https://x.com/2015           Candidates  \n",
       "7              2015        https://x.com/2015  iranian_film_actors  \n",
       "8              2015        https://x.com/2015   presidents_of_iran  \n",
       "9              2015        https://x.com/2015   presidents_of_iran  \n",
       "10             2015        https://x.com/2015      iranian_writers  \n",
       "11             2016        https://x.com/2016    iranian_activists  \n",
       "12             2016        https://x.com/2016           Candidates  \n",
       "13             2016        https://x.com/2016   presidents_of_iran  \n",
       "14             2017        https://x.com/2017           Candidates  \n",
       "15             2017        https://x.com/2017   presidents_of_iran  \n",
       "16             2020        https://x.com/2020    iranian_activists  \n",
       "17             2022        https://x.com/2022    iranian_activists  \n",
       "18             2022        https://x.com/2022           Candidates  \n",
       "19             2022        https://x.com/2022   presidents_of_iran  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# Build Manual_Search_POIs.csv from all *_with_twitter.csv\n",
    "# ×××œ× ××•×˜×•××˜×™×ª ××ª ×”×§×•×‘×¥ ×”××¨×›×–×™ ××›×œ ×”×ª×™×§×™×•×ª (××”×¢××•×“×” Twitter_username)\n",
    "# ×‘×”×ª×× ×œ×”× ×—×™×•×ª ×”×§×•×¨×¡: ×©× ×”×§×•×‘×¥ Manual_Search_POIs.csv ×•×©××™×¨×” ×ª×—×ª POIs/\n",
    "# ============================================\n",
    "\n",
    "import os, re, glob, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Dynamic path detection ----\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "REL_ROOT = \"POIs\"\n",
    "ROOT = os.path.join(IRAN_DIR, REL_ROOT)\n",
    "assert os.path.isdir(ROOT), f\"âŒ ×œ× ×§×™×™××ª ×”×ª×™×§×™×™×”: {ROOT}\"\n",
    "print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# ×©× ×”×§×•×‘×¥ ×œ×¤×™ ×”×”× ×—×™×•×ª (×©×œ×‘ 5)\n",
    "OUTPUT_NAME = \"Manual_Search_POIs.csv\"\n",
    "OUTPUT_PATH = os.path.join(ROOT, OUTPUT_NAME)\n",
    "\n",
    "# -------- Keyword map (folder_slug -> keyword) --------\n",
    "# ×”×¢×¨×”: ×”×©××•×ª ×›××Ÿ ×‘-lowercase ×›×™ ×”×ª×™×§×™×•×ª × ×•×¦×¨×• ×¢× slug ×§×˜×Ÿ (safe_slug)\n",
    "KEYWORD_MAP = {\n",
    "    # Existing\n",
    "    \"government_ministers_of_iran\": \"Iran minister\",\n",
    "    \"presidents_of_iran\": \"Iran president\",\n",
    "    \"vice_presidents_of_iran\": \"Iran vice president\",\n",
    "    \"iranian_ayatollahs\": \"Iran ayatollah\",\n",
    "    \"iranian_actors\": \"Iran actor\",\n",
    "    \"iranian_singers\": \"Iran singer\",\n",
    "    \"iranian_scientists\": \"Iran scientist\",\n",
    "    \"iranian_economists\": \"Iran economist\",\n",
    "    \"iranian_writers\": \"Iran writer\",\n",
    "    \"iranian_football_managers\": \"Iran football manager\",\n",
    "\n",
    "    # Healthcare related\n",
    "    \"hospitals_in_iran\": \"Iran hospital\",\n",
    "    \"private_hospitals_in_iran\": \"Iran hospital\",\n",
    "    \"teaching_hospitals_in_iran\": \"Iran teaching hospital\",\n",
    "    \"iranian_physicians\": \"Iran physician\",\n",
    "    \"iranian_cardiologists\": \"Iran cardiologist\",\n",
    "    \"iranian_women_physicians\": \"Iran physician\",\n",
    "    \"21st-century_iranian_physicians\": \"Iran physician\",\n",
    "    \"19th-century_iranian_physicians\": \"Iran physician\",\n",
    "    \"medical_and_health_organisations_based_in_iran\": \"Iran health org\",\n",
    "    \"healthcare_in_iran\": \"Iran healthcare\",\n",
    "    \"medicine_in_iran\": \"Iran medicine\",\n",
    "}\n",
    "\n",
    "CENTURY_PREFIX = re.compile(r\"^\\d{1,2}(st|nd|rd|th)-century_\")\n",
    "\n",
    "def base_slug(folder_slug: str) -> str:\n",
    "    \"\"\"××¡×™×¨ ×§×™×“×•××ª ×©×œ ×××” ××”×¡×œ××’ (e.g., 19th-century_...)\"\"\"\n",
    "    return CENTURY_PREFIX.sub(\"\", folder_slug)\n",
    "\n",
    "def infer_keyword_from_folder(folder_slug: str) -> str:\n",
    "    \"\"\"××¤×™×§ ××™×œ×ª ×—×™×¤×•×© ×”×’×™×•× ×™×ª ××”×¡×œ××’, ×¢× ××¤×” ×™×“× ×™×ª ×œ×§×™×™×¡×™× ×—×©×•×‘×™×.\"\"\"\n",
    "    slug = folder_slug.lower().strip()\n",
    "    if slug in KEYWORD_MAP:\n",
    "        return KEYWORD_MAP[slug]\n",
    "    slug = base_slug(slug)\n",
    "    if slug in KEYWORD_MAP:\n",
    "        return KEYWORD_MAP[slug]\n",
    "    # ×›×œ×œ ×‘×¨×™×¨×ª ××—×“×œ: ×”×•×¨×“ \"iranian_\" ×•×”Ö¾s ×”×¡×•×¤×™×ª, ×•×”×—×œ×£ _ ×‘×¨×•×•×—\n",
    "    token = slug.replace(\"iranian_\", \"\").replace(\"_\", \" \").strip()\n",
    "    token = token[:-1] if token.endswith(\"s\") else token\n",
    "    return f\"Iran {token}\".strip()\n",
    "\n",
    "def detect_name_col(df: pd.DataFrame) -> str:\n",
    "    \"\"\"×××ª×¨ ×¢××•×“×ª ×©× ×¡×‘×™×¨×”.\"\"\"\n",
    "    for c in df.columns:\n",
    "        n = str(c).strip().lower()\n",
    "        if n in {\"name\", \"poi name\", \"poi_name\", \"full_name\", \"full name\"}:\n",
    "            return c\n",
    "    # fallback: ×”×¢××•×“×” ×”×¨××©×•× ×”\n",
    "    return df.columns[0]\n",
    "\n",
    "def detect_twitter_col(df: pd.DataFrame) -> str | None:\n",
    "    \"\"\"×××ª×¨ ×¢××•×“×ª Twitter_username (××• ×“×•××”).\"\"\"\n",
    "    for c in df.columns:\n",
    "        n = str(c).strip().lower()\n",
    "        if n in {\"twitter_username\", \"twitter\", \"handle\", \"username\"}:\n",
    "            return c\n",
    "    # ×—×¤×© ×¢××•×“×” ×©×™×© ×‘×” ×”×¨×‘×” ×¢×¨×›×™× ×©× ×¨××™× ×›××• ×™×“×™×•×ª\n",
    "    for c in df.columns:\n",
    "        vals = df[c].dropna().astype(str).head(20).tolist()\n",
    "        hits = sum(1 for v in vals if re.match(r\"^@?[A-Za-z0-9_]{1,15}$\", v))\n",
    "        if hits >= max(3, len(vals)//3):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def build_twitter_url(handle: str) -> str | None:\n",
    "    if not isinstance(handle, str) or not handle.strip():\n",
    "        return None\n",
    "    h = handle.strip()\n",
    "    h = h[1:] if h.startswith(\"@\") else h\n",
    "    return f\"https://x.com/{h}\"\n",
    "\n",
    "rows = []\n",
    "folders = [d for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))]\n",
    "folders.sort()\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(ROOT, folder)\n",
    "    # ×—×¤×© ××ª ×§×•×‘×¥ ×”×™×¢×“ ×©× ×•×¦×¨ ×‘×©×œ×‘ 4\n",
    "    cands = sorted(glob.glob(os.path.join(folder_path, \"*_with_twitter.csv\")))\n",
    "    if not cands:\n",
    "        continue\n",
    "    path = cands[0]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ can't read:\", path, e)\n",
    "        continue\n",
    "\n",
    "    twitter_col = detect_twitter_col(df)\n",
    "    if not twitter_col:\n",
    "        continue\n",
    "\n",
    "    name_col = detect_name_col(df)\n",
    "    kw = infer_keyword_from_folder(folder)\n",
    "\n",
    "    sub = df[[name_col, twitter_col]].copy()\n",
    "    sub.rename(columns={name_col: \"poi_name\", twitter_col: \"Twitter_username\"}, inplace=True)\n",
    "\n",
    "    # × ×™×§×•×™, ×¡×™× ×•×Ÿ, ×•×”×©×œ××ª URL\n",
    "    sub[\"Twitter_username\"] = sub[\"Twitter_username\"].astype(str).str.strip()\n",
    "    sub = sub[sub[\"Twitter_username\"].str.len() > 0]\n",
    "    sub[\"Twitter_username\"] = sub[\"Twitter_username\"].str.lstrip(\"@\")\n",
    "    sub[\"Twitter_url\"] = sub[\"Twitter_username\"].apply(build_twitter_url)\n",
    "    sub[\"keyword\"] = kw\n",
    "    sub[\"source_folder\"] = folder  # ×××™×–×” ×§×˜×’×•×¨×™×” ×”×’×™×¢\n",
    "\n",
    "    rows.append(sub[[\"keyword\", \"poi_name\", \"Twitter_username\", \"Twitter_url\", \"source_folder\"]])\n",
    "\n",
    "# ××™×—×•×“ ×•×©××™×¨×”\n",
    "if rows:\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    # ×¡×™× ×•×Ÿ ×›×¤×™×œ×•×™×•×ª ×¢×œ ×‘×¡×™×¡ ×©×œ×™×©×™×™×” (×©× ××©×ª××© + ××™×œ×ª ×—×™×¤×•×© + ×©× POI)\n",
    "    out.drop_duplicates(subset=[\"Twitter_username\", \"keyword\", \"poi_name\"], inplace=True, ignore_index=True)\n",
    "    # ×•×¨×™×× ×˜ × ×•×¡×£: ×× ××•×ª×• handle ×”×•×¤×™×¢ ×‘×›××” ×ª×™×§×™×•×ª â€” × ×©××•×¨ ××ª ×”×”×•×¤×¢×” ×”×¨××©×•× ×” ×‘×œ×‘×“\n",
    "    out.sort_values([\"Twitter_username\", \"keyword\", \"poi_name\"], inplace=True, ignore_index=True)\n",
    "    out.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… Saved: {OUTPUT_PATH} | rows: {len(out)}\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(out.head(20))\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"â„¹ï¸ ×œ× × ××¦××• *_with_twitter.csv ×‘×ª×™×§×™×•×ª, ××• ×©××™×Ÿ ×™×“×™×•×ª ×œ××œ×.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9B: Enhanced Wikipedia Search via API\n",
    "×—×™×¤×•×© ××©×•×¤×¨ ×©×œ ×¢××•×“×™ ×•×™×§×™×¤×“×™×” - ×©×™××•×© ×‘-Wikipedia API ×œ×—×™×¤×•×© ×™×©×™×¨ ×‘××§×•× ×œ×”×¡×ª××š ×¨×§ ×¢×œ ×”×§×‘×¦×™× ×”××§×•××™×™×"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Total missing users to search: 81\n",
      "\n",
      "ğŸ” Searching Wikipedia API for all 81 users...\n",
      "â³ This may take a few minutes (0.2s delay per search)...\n",
      "\n",
      "  âœ… [1] ã¾ã‚†ã¿ã‚“\n",
      "  âœ… [2] 2012 ABC\n",
      "  âœ… [3] Sam Beck\n",
      "  âœ… [4] Catherine Manso\n",
      "  âœ… [5] Iran News Wire\n",
      "  âœ… [6] Javad Zarif\n",
      "  âœ… [7] Janez LenarÄiÄ\n",
      "  âœ… [8] John Kerry\n",
      "  âœ… [9] Kaveh Akbar _ AUTHOR\n",
      "  âœ… [10] Ù…Ø­Ù…Ø¯ Ù†Ù‡Ø§ÙˆÙ†Ø¯ÛŒØ§Ù†\n",
      "  ğŸ“Š Progress: 10/81 searches, 10 found\n",
      "  âœ… [11] Nisha Biswal\n",
      "  âœ… [12] Mohsen Namjoo | Ù…Ø­Ø³Ù† Ù†Ø§Ù…Ø¬Ùˆ\n",
      "  âœ… [13] ØªÙˆÙ…Ø§Ø¬ ØµØ§Ù„Ø­ÛŒ\n",
      "  âœ… [14] porochista khakpour she/her Ù¾ÙˆØ±ÙˆÚ†ÛŒØ³ØªØ§ Ø®Ø§Ú©Ù¾ÙˆØ±\n",
      "  âœ… [15] Penn History\n",
      "  âœ… [16] REZA\n",
      "  âœ… [17] SAGES is in Tampa in 2026!\n",
      "  ğŸ“Š Progress: 20/81 searches, 17 found\n",
      "  âœ… [18] Joseph S Blatter\n",
      "  âœ… [19] Shaparak Khorsandi Ø´Ø§Ù¾Ø±Ú© Ø®Ø±Ø³Ù†Ø¯ÛŒ\n",
      "  âœ… [20] ØªÙˆØ§Ù†Ø§ Tavaana\n",
      "  âœ… [21] V&A\n",
      "  âœ… [22] Ali Larijani | Ø¹Ù„ÛŒ Ù„Ø§Ø±ÛŒØ¬Ø§Ù†ÛŒ\n",
      "  âœ… [23] Ó˜Ğ»Ğ¸Ñ  ĞĞ°Ğ·Ğ°Ñ€Ğ±Ğ°ĞµĞ²Ğ°\n",
      "  âœ… [24] Arash Sobhani Ø¢Ø±Ø´ Ø³Ø¨Ø­Ø§Ù†ÛŒ\n",
      "  âœ… [25] Arthur Bullard\n",
      "  âœ… [26] asa\n",
      "  âœ… [27] Ø¹ØµØ± Ù…Ø¬Ù„Ø³\n",
      "  ğŸ“Š Progress: 30/81 searches, 27 found\n",
      "  âœ… [28] Ø­Ø³ÛŒÙ† ØµÙ…ØµØ§Ù…ÛŒ\n",
      "  âœ… [29] Elina Valtonen\n",
      "  âœ… [30] enã¡ã‚ƒã‚“\n",
      "  âœ… [31] Fatemeh Ekhtesari ÙØ§Ø·Ù…Ù‡ Ø§Ø®ØªØµØ§Ø±ÛŒ\n",
      "  âœ… [32] Tajikistan Football\n",
      "  âœ… [33] Ø­Ù…ÛŒØ¯ Ø±Ø³Ø§ÛŒÛŒ\n",
      "  âœ… [34] Ø¹Ø¨Ø¯Ø§Ù„Ø±Ø¶Ø§ Ù‡Ù„Ø§Ù„ÛŒ\n",
      "  ğŸ“Š Progress: 40/81 searches, 34 found\n",
      "  âœ… [35] Ù…Ø­Ù…Ø¯Ø±Ø¶Ø§ Ø¹Ø§Ø±Ù\n",
      "  âœ… [36] ISSUES\n",
      "  âœ… [37] La Biennale di Venezia\n",
      "  âœ… [38] La Toya Jackson\n",
      "  âœ… [39] ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ† Ù…Ù†ÙˆØªÙˆ\n",
      "  âœ… [40] M.H. Ansari\n",
      "  âœ… [41] 10th Armoured Division (Iraq)\n",
      "  âœ… [42] nazanin\n",
      "  âœ… [43] n\n",
      "  ğŸ“Š Progress: 50/81 searches, 43 found\n",
      "  âœ… [44] Ù¾ÛŒØ±ÙˆØ² Ø­Ù†Ø§Ú†ÛŒ\n",
      "  âœ… [45] Ø±Ø¶Ø§ Ø­Ù‚ÙŠÙ‚Øªâ€ŒÙ†Ú˜Ø§Ø¯\n",
      "  âœ… [46] Reza Rohani Official\n",
      "  âœ… [47] Ø±ÙˆØ­ Ø§Ù„Ù„Ù‡ Ø¬Ù…Ø¹Ù‡ Ø§ÛŒ\n",
      "  âœ… [48] ã€Sã€\n",
      "  âœ… [49] Ø³Ø¹ÛŒØ¯ Ù…Ø­Ù…Ø¯\n",
      "  âœ… [50] Ø´Ù‡Ø§Ø¨â€ŒØ§Ù„Ø¯ÙŠÙ† Ø·Ø¨Ø§Ø·Ø¨Ø§ÙŠÙ‰\n",
      "  âœ… [51] Snoh\n",
      "  ğŸ“Š Progress: 60/81 searches, 51 found\n",
      "  âœ… [52] Stronger\n",
      "  âœ… [53] Ğ¢Ğ“ĞœĞ£ Ğ¸Ğ¼ĞµĞ½Ğ¸ ĞĞ±ÑƒĞ°Ğ»Ğ¸ Ğ¸Ğ±Ğ½Ğ¸ Ğ¡Ğ¸Ğ½Ğ¾\n",
      "  âœ… [54] thebest\n",
      "  âœ… [55] Zal\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ RESULTS: Found 55/81 Wikipedia pages (67.9%)\n",
      "======================================================================\n",
      "\n",
      "ğŸ’¾ Saved all results to: c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\\POIs\\Manual_Search_POIs_Missing_WikiAPI_Results.csv\n",
      "\n",
      "âœ… Found 55 users with Wikipedia pages!\n",
      "ğŸ’¾ Saved found users to: c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\\POIs\\Manual_Search_POIs_Missing_WithWikiAPI.csv\n",
      "\n",
      "ğŸ“‹ Top 20 found users:\n",
      "  1. Mayumi Sada (@2011)\n",
      "  2. ABC Me (@2012)\n",
      "  3. Avatar: The Last Airbender (franchise) (@2017)\n",
      "  7. La casa de las sombras (@CathyManso)\n",
      "  9. News agency (@IranNW)\n",
      "  11. Mohammad Javad Zarif (@JZarif)\n",
      "  12. Janez LenarÄiÄ (@JanezLenarcic)\n",
      "  13. John Kerry (@JohnKerry)\n",
      "  14. Kaveh Akbar (@KavehAkbar)\n",
      "  15. Mohammad Nahavandian (@Nahavandian_ir)\n",
      "  17. Nisha Desai Biswal (@NishaBiswal)\n",
      "  18. Mohsen Namjoo (@OfficialMNamjoo)\n",
      "  19. Toomaj Salehi (@OfficialToomaj)\n",
      "  21. Porochista Khakpour (@PKhakpour)\n",
      "  22. University of Pennsylvania (@PennHistory)\n",
      "  23. Reza (@REZAphotography)\n",
      "  25. United States (@SAGES_Updates)\n",
      "  29. Sepp Blatter (@SeppBlatter)\n",
      "  30. Shaparak Khorsandi (@ShappiKhorsandi)\n",
      "  31. Nematollah Aghasi (@Tavaana)\n",
      "\n",
      "======================================================================\n",
      "ğŸ’¡ ×”×¦×¢×”: ×œ×”×•×¡×™×£ ××ª 55 ×”××©×ª××©×™× ×”××œ×” ×œ-Manual_Search_POIs_Unique.csv\n",
      "   ×”×¨×™×¦×• ××ª ×”×ª× ×”×‘× ×›×“×™ ×œ×‘×¦×¢ ××ª ×”×”×•×¡×¤×”\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Breakdown by source folder:\n",
      "source_folder\n",
      "Candidates           54\n",
      "iranian_activists     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "\n",
    "# ×˜×¢×™× ×ª ×”××©×ª××©×™× ×”×—×¡×¨×™× (×©×œ× × ××¦××• ×‘×§×•×‘×¥ Unique)\n",
    "missing_file = os.path.join(ROOT, \"Manual_Search_POIs_Missing_From_Unique.csv\")\n",
    "\n",
    "if not os.path.exists(missing_file):\n",
    "    print(f\"âš ï¸ ×§×•×‘×¥ ×œ× × ××¦×: {missing_file}\")\n",
    "    print(\"ğŸ“ ×§×•×“× ×¦×¨×™×š ×œ×”×¨×™×¥ ××ª ×”×§×•×“ ×©×™×•×¦×¨ ××ª ×”×§×•×‘×¥ ×”×–×”\")\n",
    "else:\n",
    "    df_missing = pd.read_csv(missing_file, encoding='utf-8-sig')\n",
    "    print(f\"ğŸ“Š Total missing users to search: {len(df_missing)}\")\n",
    "    \n",
    "    # ×¤×•× ×§×¦×™×” ×œ×—×™×¤×•×© ×™×©×™×¨×•×ª ×‘-Wikipedia API\n",
    "    def search_wikipedia_api(name, language='en'):\n",
    "        \"\"\"×—×™×¤×•×© ×™×©×™×¨ ×‘-Wikipedia API\"\"\"\n",
    "        try:\n",
    "            search_url = f\"https://{language}.wikipedia.org/w/api.php\"\n",
    "            params = {\n",
    "                'action': 'query',\n",
    "                'format': 'json',\n",
    "                'list': 'search',\n",
    "                'srsearch': name,\n",
    "                'srlimit': 3,\n",
    "                'utf8': 1\n",
    "            }\n",
    "            \n",
    "            # ×”×•×¡×¤×ª User-Agent header - ×—×•×‘×” ×¢×‘×•×¨ Wikipedia API\n",
    "            headers = {\n",
    "                'User-Agent': 'Iranian_POIs_Research/1.0 (Educational Research Project)'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(search_url, params=params, headers=headers, timeout=5)\n",
    "            \n",
    "            # ×‘×“×™×§×” ×©×”×ª×’×•×‘×” ×ª×§×™× ×”\n",
    "            if response.status_code != 200:\n",
    "                return None, None\n",
    "            \n",
    "            # × ×™×¡×™×•×Ÿ ×œ×¤×¨×¡×¨ ××ª ×”-JSON\n",
    "            try:\n",
    "                data = response.json()\n",
    "            except:\n",
    "                return None, None\n",
    "            \n",
    "            if 'query' in data and 'search' in data['query'] and len(data['query']['search']) > 0:\n",
    "                # ××—×–×™×¨ ××ª ×”×ª×•×¦××” ×”×¨××©×•× ×”\n",
    "                result = data['query']['search'][0]\n",
    "                title = result['title']\n",
    "                wiki_link = f\"https://{language}.wikipedia.org/wiki/{quote(title.replace(' ', '_'))}\"\n",
    "                return wiki_link, title\n",
    "            \n",
    "            return None, None\n",
    "        except requests.exceptions.Timeout:\n",
    "            return None, None\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None, None\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    \n",
    "    # ×—×™×¤×•×© ×¢×‘×•×¨ ×›×œ ×”××©×ª××©×™× ×”×—×¡×¨×™×\n",
    "    print(f\"\\nğŸ” Searching Wikipedia API for all {len(df_missing)} users...\")\n",
    "    print(\"â³ This may take a few minutes (0.2s delay per search)...\\n\")\n",
    "    \n",
    "    df_missing['Wikipedia_Link_API'] = None\n",
    "    df_missing['Wikipedia_Title_API'] = None\n",
    "    found_count = 0\n",
    "    search_count = 0\n",
    "    \n",
    "    for idx, row in df_missing.iterrows():\n",
    "        if pd.isna(row['poi_name']):\n",
    "            continue\n",
    "        \n",
    "        poi_name = str(row['poi_name']).strip()\n",
    "        search_count += 1\n",
    "        \n",
    "        # ×—×™×¤×•×© ×‘-API\n",
    "        wiki_link, wiki_title = search_wikipedia_api(poi_name)\n",
    "        \n",
    "        if wiki_link:\n",
    "            df_missing.at[idx, 'Wikipedia_Link_API'] = wiki_link\n",
    "            df_missing.at[idx, 'Wikipedia_Title_API'] = wiki_title\n",
    "            found_count += 1\n",
    "            print(f\"  âœ… [{found_count}] {poi_name[:50]}\")\n",
    "        \n",
    "        # ×¢×“×›×•×Ÿ ×”×ª×§×“××•×ª ×›×œ 10 ×—×™×¤×•×©×™×\n",
    "        if search_count % 10 == 0:\n",
    "            print(f\"  ğŸ“Š Progress: {search_count}/{len(df_missing)} searches, {found_count} found\")\n",
    "        \n",
    "        # ×”××ª× ×” ×§×¦×¨×” ×œ×× ×™×¢×ª ×—×¡×™××”\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ¯ RESULTS: Found {found_count}/{len(df_missing)} Wikipedia pages ({found_count/len(df_missing)*100:.1f}%)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # ×©××™×¨×ª ×”×ª×•×¦××•×ª ×”××œ××•×ª\n",
    "    output_file = os.path.join(ROOT, \"Manual_Search_POIs_Missing_WikiAPI_Results.csv\")\n",
    "    df_missing.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ Saved all results to: {output_file}\")\n",
    "    \n",
    "    # ×¡×™× ×•×Ÿ ×¨×§ ××™ ×©× ××¦× ×œ×• ×•×™×§×™×¤×“×™×”\n",
    "    df_found = df_missing[df_missing['Wikipedia_Link_API'].notna()].copy()\n",
    "    \n",
    "    if len(df_found) > 0:\n",
    "        print(f\"\\nâœ… Found {len(df_found)} users with Wikipedia pages!\")\n",
    "        \n",
    "        # ×”×¢×ª×§×ª ×”×§×™×©×•×¨ ×œ×¢××•×“×” ×”×¨×’×™×œ×”\n",
    "        df_found['Wikipedia_Link'] = df_found['Wikipedia_Link_API']\n",
    "        \n",
    "        # ×©××™×¨×” ×œ×§×•×‘×¥ × ×¤×¨×“\n",
    "        output_found = os.path.join(ROOT, \"Manual_Search_POIs_Missing_WithWikiAPI.csv\")\n",
    "        df_found.to_csv(output_found, index=False, encoding='utf-8-sig')\n",
    "        print(f\"ğŸ’¾ Saved found users to: {output_found}\")\n",
    "        \n",
    "        # ×”×¦×’×ª ×“×•×’×××•×ª\n",
    "        print(f\"\\nğŸ“‹ Top 20 found users:\")\n",
    "        for i, row in df_found.head(20).iterrows():\n",
    "            twitter = row['Twitter_username'] if pd.notna(row['Twitter_username']) else 'N/A'\n",
    "            title = row['Wikipedia_Title_API'] if pd.notna(row['Wikipedia_Title_API']) else row['poi_name']\n",
    "            print(f\"  {i+1}. {title[:60]} (@{twitter})\")\n",
    "        \n",
    "        # ×©××œ×” ×”×× ×œ×”×•×¡×™×£ ×œ××©×ª××©×™× ×”×™×™×—×•×“×™×™×\n",
    "        print(f\"\\n\" + \"=\"*70)\n",
    "        print(f\"ğŸ’¡ ×”×¦×¢×”: ×œ×”×•×¡×™×£ ××ª {len(df_found)} ×”××©×ª××©×™× ×”××œ×” ×œ-Manual_Search_POIs_Unique.csv\")\n",
    "        print(f\"   ×”×¨×™×¦×• ××ª ×”×ª× ×”×‘× ×›×“×™ ×œ×‘×¦×¢ ××ª ×”×”×•×¡×¤×”\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        print(\"\\nâŒ No Wikipedia pages found via API search\")\n",
    "    \n",
    "    # ×¡×˜×˜×™×¡×˜×™×§×” ×œ×¤×™ ×ª×™×§×™×•×ª\n",
    "    if found_count > 0:\n",
    "        print(f\"\\nğŸ“Š Breakdown by source folder:\")\n",
    "        print(df_found['source_folder'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9C: Add Found Users to Unique File\n",
    "×”×•×¡×¤×ª ×”××©×ª××©×™× ×©× ××¦××• ×œ-Manual_Search_POIs_Unique.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Found users to add: 55\n",
      "ğŸ“Š Current unique users: 326\n",
      "\n",
      "âœ… After merge: 378 users\n",
      "â• Net new users added: 52\n",
      "\n",
      "ğŸ’¾ Updated: c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\\POIs\\Manual_Search_POIs_Unique.csv\n",
      "\n",
      "ğŸ“Š Final statistics:\n",
      "   Total users: 378\n",
      "   With Wikipedia: 378 (100.0%)\n",
      "   Without Wikipedia: 0\n",
      "\n",
      "âœ¨ Done! Manual_Search_POIs_Unique.csv updated successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ×˜×¢×™× ×ª ×”×§×‘×¦×™×\n",
    "found_file = os.path.join(ROOT, \"Manual_Search_POIs_Missing_WithWikiAPI.csv\")\n",
    "unique_file = os.path.join(ROOT, \"Manual_Search_POIs_Unique.csv\")\n",
    "\n",
    "if not os.path.exists(found_file):\n",
    "    print(f\"âš ï¸ ×§×•×‘×¥ ×œ× × ××¦×: {found_file}\")\n",
    "    print(\"ğŸ“ ×§×•×“× ×¦×¨×™×š ×œ×”×¨×™×¥ ××ª Step 9B\")\n",
    "elif not os.path.exists(unique_file):\n",
    "    print(f\"âš ï¸ ×§×•×‘×¥ ×œ× × ××¦×: {unique_file}\")\n",
    "else:\n",
    "    df_found = pd.read_csv(found_file, encoding='utf-8-sig')\n",
    "    df_unique = pd.read_csv(unique_file, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"ğŸ“Š Found users to add: {len(df_found)}\")\n",
    "    print(f\"ğŸ“Š Current unique users: {len(df_unique)}\")\n",
    "    \n",
    "    # × ×™×§×•×™ ×¢××•×“×•×ª ××™×•×ª×¨×•×ª ××”×§×•×‘×¥ ×©×œ ×”××©×ª××©×™× ×©× ××¦××•\n",
    "    cols_to_keep = ['keyword', 'poi_name', 'Twitter_username', 'Twitter_url', 'source_folder', 'Wikipedia_Link']\n",
    "    df_found_clean = df_found[cols_to_keep].copy()\n",
    "    \n",
    "    # ××™×—×•×“\n",
    "    df_combined = pd.concat([df_unique, df_found_clean], ignore_index=True)\n",
    "    \n",
    "    # ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª ×œ×¤×™ Twitter_username (case-insensitive)\n",
    "    df_combined['twitter_lower'] = df_combined['Twitter_username'].str.lower()\n",
    "    df_final = df_combined.drop_duplicates(subset=['twitter_lower'], keep='first')\n",
    "    df_final = df_final.drop(columns=['twitter_lower'])\n",
    "    \n",
    "    # ×ª×™×§×•×Ÿ ××¡×¤×•×¨\n",
    "    df_final['Row_Number'] = range(1, len(df_final) + 1)\n",
    "    \n",
    "    # ×¡×™×“×•×¨ ×”×¢××•×“×•×ª\n",
    "    cols_order = ['Row_Number'] + [col for col in df_final.columns if col != 'Row_Number']\n",
    "    df_final = df_final[cols_order]\n",
    "    \n",
    "    print(f\"\\nâœ… After merge: {len(df_final)} users\")\n",
    "    print(f\"â• Net new users added: {len(df_final) - len(df_unique)}\")\n",
    "    \n",
    "    # ×©××™×¨×”\n",
    "    df_final.to_csv(unique_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ Updated: {unique_file}\")\n",
    "    \n",
    "    # ×¡×˜×˜×™×¡×˜×™×§×”\n",
    "    with_wiki = df_final['Wikipedia_Link'].notna().sum()\n",
    "    print(f\"\\nğŸ“Š Final statistics:\")\n",
    "    print(f\"   Total users: {len(df_final)}\")\n",
    "    print(f\"   With Wikipedia: {with_wiki} ({with_wiki/len(df_final)*100:.1f}%)\")\n",
    "    print(f\"   Without Wikipedia: {len(df_final) - with_wiki}\")\n",
    "    \n",
    "    print(f\"\\nâœ¨ Done! Manual_Search_POIs_Unique.csv updated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFtZ7fiyyIAe"
   },
   "source": [
    "#**Cleaning up suspended users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYqDtkG_C-gz"
   },
   "outputs": [],
   "source": [
    "# # ============================================\n",
    "# # Update POIs_Search_Manual.csv:\n",
    "# # - Find the correct Twitter handle column (robust name detection)\n",
    "# # - Replace numeric-only handles with \"Suspended\"\n",
    "# # - Rebuild Twitter_url accordingly\n",
    "# # ============================================\n",
    "\n",
    "# import os, re, pandas as pd\n",
    "\n",
    "# BASES = [\n",
    "#     \"/content/drive/MyDrive/Iran\",\n",
    "#     \"/content/drive/MyDrive/data_science_semester_A/Iran\",\n",
    "# ]\n",
    "# REL_ROOT   = \"POIs\"\n",
    "# OUTPUT_NAME = \"POIs_Search_Manual.csv\"\n",
    "\n",
    "# # 1) Locate file\n",
    "# BASE = next((b for b in BASES if os.path.exists(b)), None)\n",
    "# assert BASE, \"Drive base not found. Mount Google Drive first.\"\n",
    "# ROOT = os.path.join(BASE, REL_ROOT)\n",
    "# assert os.path.isdir(ROOT), f\"Folder not found: {ROOT}\"\n",
    "# path = os.path.join(ROOT, OUTPUT_NAME)\n",
    "# assert os.path.exists(path), f\"File not found: {path}\"\n",
    "# print(\"ğŸ“„ Using file:\", path)\n",
    "\n",
    "# # 2) Load\n",
    "# df = pd.read_csv(path)\n",
    "\n",
    "# # 3) Detect the twitter username column robustly (case-insensitive, common variants)\n",
    "# cands = [c for c in df.columns if re.sub(r'\\s+', '', c.strip().lower()) in {\n",
    "#     \"twitter_username\",\"twitteruser\",\"twitter_user\",\"twitterhandle\",\"handle\",\"username\"\n",
    "# }]\n",
    "# if not cands:\n",
    "#     # fallback: try any column containing 'twitter' and 'name'\n",
    "#     cands = [c for c in df.columns if \"twitter\" in c.lower() and \"name\" in c.lower()]\n",
    "# assert cands, f\"Could not find Twitter username column in {list(df.columns)}\"\n",
    "# tw_col = cands[0]\n",
    "# print(\"âœ… Detected username column:\", tw_col)\n",
    "\n",
    "# # 4) Replace numeric-only usernames with \"Suspended\"\n",
    "# before = df[tw_col].astype(str)\n",
    "# df[tw_col] = df[tw_col].apply(lambda v: \"Suspended\" if isinstance(v, str) and re.fullmatch(r\"\\d+\", v.strip() or \"\") else v)\n",
    "# changed_usernames = (before != df[tw_col].astype(str)).sum()\n",
    "\n",
    "# # 5) Rebuild Twitter_url consistently (create if missing)\n",
    "# url_col = \"Twitter_url\" if \"Twitter_url\" in df.columns else None\n",
    "# if url_col is None:\n",
    "#     df[\"Twitter_url\"] = None\n",
    "#     url_col = \"Twitter_url\"\n",
    "\n",
    "# def url_from_handle(h):\n",
    "#     if not isinstance(h, str): return None\n",
    "#     h = h.strip().lstrip(\"@\")\n",
    "#     if h == \"\" or h.lower() == \"suspended\": return None\n",
    "#     return f\"https://x.com/{h}\"\n",
    "\n",
    "# before_urls = df[url_col].copy()\n",
    "# df[url_col] = df[tw_col].apply(url_from_handle)\n",
    "# fixed_urls = (before_urls != df[url_col]).sum()\n",
    "\n",
    "# # 6) Save\n",
    "# df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "# print(f\"âœ… Done. Replaced {changed_usernames} usernames with 'Suspended', updated {fixed_urls} URLs.\")\n",
    "\n",
    "# # Peek\n",
    "# try:\n",
    "#     from IPython.display import display\n",
    "#     display(df.head(15))\n",
    "# except:\n",
    "#     print(df.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SYH1sj55gew"
   },
   "outputs": [],
   "source": [
    "# # ============================================\n",
    "# # Build Manual_Search_POIs.csv from all *_with_twitter.csv\n",
    "# # ×××œ× ××•×˜×•××˜×™×ª ××ª ×”×§×•×‘×¥ ×”××¨×›×–×™ ××›×œ ×”×ª×™×§×™×•×ª (Twitter_username)\n",
    "# # ============================================\n",
    "\n",
    "# from IPython.display import display\n",
    "# import os, glob, re, pandas as pd\n",
    "# from pathlib import Path\n",
    "\n",
    "# # ---- Dynamic path detection ----\n",
    "# IRAN_DIR = os.getcwd()\n",
    "# if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "#     for parent in Path(IRAN_DIR).parents:\n",
    "#         if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "#             IRAN_DIR = str(parent)\n",
    "#             break\n",
    "#     else:\n",
    "#         raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "# REL_ROOT    = \"POIs\"\n",
    "# ROOT        = os.path.join(IRAN_DIR, REL_ROOT)\n",
    "# os.makedirs(ROOT, exist_ok=True)\n",
    "# OUTPUT_NAME = \"Manual_Search_POIs.csv\"   # ×©× ×œ×¤×™ ×”×”× ×—×™×•×ª\n",
    "# OUTPUT_PATH = os.path.join(ROOT, OUTPUT_NAME)\n",
    "# print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# # ---- KEYWORD MAP: ×¨×§ ×”×§×˜×’×•×¨×™×•×ª ×©×‘×™×§×©×ª (×œ×¤×™ slug ×‘×ª×™×§×™×•×ª - lowercase) ----\n",
    "# KEYWORD_MAP = {\n",
    "#     # Existing\n",
    "#     \"government_ministers_of_iran\": \"Iran minister\",\n",
    "#     \"presidents_of_iran\": \"Iran president\",\n",
    "#     \"vice_presidents_of_iran\": \"Iran vice president\",\n",
    "#     \"iranian_ayatollahs\": \"Iran ayatollah\",\n",
    "#     \"iranian_actors\": \"Iran actor\",\n",
    "#     \"iranian_singers\": \"Iran singer\",\n",
    "#     \"iranian_scientists\": \"Iran scientist\",\n",
    "#     \"iranian_economists\": \"Iran economist\",\n",
    "#     \"iranian_writers\": \"Iran writer\",\n",
    "#     \"iranian_football_managers\": \"Iran football manager\",\n",
    "\n",
    "#     # Healthcare\n",
    "#     \"hospitals_in_iran\": \"Iran hospital\",\n",
    "#     \"private_hospitals_in_iran\": \"Iran hospital\",\n",
    "#     \"teaching_hospitals_in_iran\": \"Iran teaching hospital\",\n",
    "#     \"iranian_physicians\": \"Iran physician\",\n",
    "#     \"iranian_cardiologists\": \"Iran cardiologist\",\n",
    "#     \"iranian_women_physicians\": \"Iran physician\",\n",
    "#     \"21st-century_iranian_physicians\": \"Iran physician\",\n",
    "#     \"19th-century_iranian_physicians\": \"Iran physician\",\n",
    "#     \"medical_and_health_organisations_based_in_iran\": \"Iran health org\",\n",
    "#     \"healthcare_in_iran\": \"Iran healthcare\",\n",
    "#     \"medicine_in_iran\": \"Iran medicine\",\n",
    "# }\n",
    "\n",
    "# CENTURY_PREFIX = re.compile(r\"^\\d{1,2}(st|nd|rd|th)-century_\")\n",
    "\n",
    "# def normalize_folder_slug(folder_name: str) -> str:\n",
    "#     \"\"\"××•×¨×™×“ ×¨×•×•×—×™×/×¨×™×©×™×•×ª, ××¡×™×¨ ×§×™×“×•××ª ×××”.\"\"\"\n",
    "#     slug = folder_name.strip().lower()\n",
    "#     slug = CENTURY_PREFIX.sub(lambda m: m.group(0), slug)  # ××©××™×¨ ××ª ×§×™×“×•××ª ×”×××” ×× ×§×™×™××ª (×›×“×™ ×œ×”×ª××™× ×œ-map)\n",
    "#     return slug\n",
    "\n",
    "# def base_slug(folder_slug: str) -> str:\n",
    "#     \"\"\"××¡×™×¨ ×§×™×“×•××ª ×××” ×œ×©×™××•×© ×‘×¨×™×¨×ª ××—×“×œ.\"\"\"\n",
    "#     return CENTURY_PREFIX.sub(\"\", folder_slug)\n",
    "\n",
    "# def infer_keyword_from_folder(folder: str) -> str:\n",
    "#     slug = normalize_folder_slug(folder)\n",
    "#     # ×§×•×“× ××™×¤×•×™ ××“×•×™×§\n",
    "#     if slug in KEYWORD_MAP:\n",
    "#         return KEYWORD_MAP[slug]\n",
    "#     # × ×¡×” ×‘×œ×™ ×§×™×“×•××ª ×××”\n",
    "#     bslug = base_slug(slug)\n",
    "#     if bslug in KEYWORD_MAP:\n",
    "#         return KEYWORD_MAP[bslug]\n",
    "#     # ×‘×¨×™×¨×ª ××—×“×œ: ×”×¤×•×š ×œÖ¾\"Iran <token>\"\n",
    "#     token = bslug.replace(\"iranian_\", \"\").replace(\"_\", \" \").strip()\n",
    "#     token = token[:-1] if token.endswith(\"s\") else token\n",
    "#     return f\"Iran {token}\".strip()\n",
    "\n",
    "# def detect_name_col(df: pd.DataFrame) -> str:\n",
    "#     for c in df.columns:\n",
    "#         n = str(c).strip().lower()\n",
    "#         if n in {\"name\", \"poi name\", \"poi_name\", \"full_name\", \"full name\"}:\n",
    "#             return c\n",
    "#     return df.columns[0]\n",
    "\n",
    "# # ---- Aggregate from all *_with_twitter.csv under ROOT/* ----\n",
    "# rows = []\n",
    "# folders = sorted([d for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))])\n",
    "# print(f\"ğŸ“‚ × ××¦××• {len(folders)} ×ª×™×§×™×•×ª ××ª×—×ª ×œ-{REL_ROOT}\")\n",
    "\n",
    "# for folder in folders:\n",
    "#     folder_path = os.path.join(ROOT, folder)\n",
    "#     cands = sorted(glob.glob(os.path.join(folder_path, \"*_with_twitter.csv\")))\n",
    "#     if not cands:\n",
    "#         continue\n",
    "\n",
    "#     csv_path = cands[0]\n",
    "#     try:\n",
    "#         df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "#     except UnicodeDecodeError:\n",
    "#         df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "#     except Exception as e:\n",
    "#         print(\"âš ï¸ can't read:\", csv_path, e)\n",
    "#         continue\n",
    "\n",
    "#     if \"Twitter_username\" not in df.columns:\n",
    "#         continue\n",
    "\n",
    "#     name_col = detect_name_col(df)\n",
    "#     kw = infer_keyword_from_folder(folder)\n",
    "\n",
    "#     sub = df[[name_col, \"Twitter_username\"]].dropna(subset=[\"Twitter_username\"]).copy()\n",
    "#     sub.rename(columns={name_col: \"poi_name\"}, inplace=True)\n",
    "#     sub[\"Twitter_username\"] = sub[\"Twitter_username\"].astype(str).str.strip().str.lstrip(\"@\")\n",
    "#     sub = sub[sub[\"Twitter_username\"].str.len() > 0]\n",
    "#     sub[\"Twitter_url\"] = \"https://x.com/\" + sub[\"Twitter_username\"]\n",
    "#     sub[\"keyword\"] = kw\n",
    "#     sub[\"source_folder\"] = folder\n",
    "\n",
    "#     rows.append(sub[[\"keyword\", \"poi_name\", \"Twitter_username\", \"Twitter_url\", \"source_folder\"]])\n",
    "\n",
    "# # ---- Save merged CSV ----\n",
    "# if rows:\n",
    "#     out = pd.concat(rows, ignore_index=True)\n",
    "#     out.drop_duplicates(subset=[\"Twitter_username\", \"keyword\", \"poi_name\"], inplace=True, ignore_index=True)\n",
    "#     out.sort_values([\"keyword\", \"poi_name\"], inplace=True, ignore_index=True)\n",
    "#     out.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "#     print(f\"âœ… Saved: {OUTPUT_PATH} | rows: {len(out)}\")\n",
    "#     display(out.head(20))\n",
    "# else:\n",
    "#     print(\"â„¹ï¸ ×œ× × ××¦××• *_with_twitter.csv ×‘×ª×™×§×™×•×ª, ××• ×©××™×Ÿ ×™×“×™×•×ª ×œ××œ×.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQoTr36h-5af"
   },
   "source": [
    "×”×•× ×¦×¢×“ ×¢×–×¨ ×˜×•×‘ ×œ×¤× ×™ ×©×œ×‘ 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GtQf4kgx6Z5Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ROOT: c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\\POIs\n",
      "âœ… Updated file: c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\\POIs\\Manual_Search_POIs.csv (8325 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row_Number</th>\n",
       "      <th>keyword</th>\n",
       "      <th>poi_name</th>\n",
       "      <th>Twitter_username</th>\n",
       "      <th>Twitter_url</th>\n",
       "      <th>source_folder</th>\n",
       "      <th>Twitter_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>ã¾ã‚†ã¿ã‚“</td>\n",
       "      <td>2011</td>\n",
       "      <td>https://x.com/2011</td>\n",
       "      <td>Candidates</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Iran writer</td>\n",
       "      <td>Mehrdad Afsari</td>\n",
       "      <td>2011</td>\n",
       "      <td>https://x.com/2011</td>\n",
       "      <td>iranian_writers</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>2012 ABC</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://x.com/2012</td>\n",
       "      <td>Candidates</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Iran writer</td>\n",
       "      <td>Behdad Sami</td>\n",
       "      <td>2012</td>\n",
       "      <td>https://x.com/2012</td>\n",
       "      <td>iranian_writers</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Iran actor</td>\n",
       "      <td>Michael Shahrestani</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>iranian_actors</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>Candidates</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Iran president</td>\n",
       "      <td>Joint Comprehensive Plan of Action</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Iran president</td>\n",
       "      <td>Reactions to the Joint Comprehensive Plan of A...</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Iran writer</td>\n",
       "      <td>Michael Shahrestani</td>\n",
       "      <td>2015</td>\n",
       "      <td>https://x.com/2015</td>\n",
       "      <td>iranian_writers</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>https://x.com/2016</td>\n",
       "      <td>Candidates</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Iran president</td>\n",
       "      <td>Criticism of the Joint Comprehensive Plan of A...</td>\n",
       "      <td>2016</td>\n",
       "      <td>https://x.com/2016</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>Sam Beck</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://x.com/2017</td>\n",
       "      <td>Candidates</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Iran president</td>\n",
       "      <td>Aftermath of the Joint Comprehensive Plan of A...</td>\n",
       "      <td>2017</td>\n",
       "      <td>https://x.com/2017</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://x.com/2022</td>\n",
       "      <td>Candidates</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Iran president</td>\n",
       "      <td>Mahsa Amini protests</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://x.com/2022</td>\n",
       "      <td>presidents_of_iran</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Iran writer</td>\n",
       "      <td>Ela (Andromache song)</td>\n",
       "      <td>2022</td>\n",
       "      <td>https://x.com/2022</td>\n",
       "      <td>iranian_writers</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Iran actor</td>\n",
       "      <td>Arian Moayed</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://x.com/2023</td>\n",
       "      <td>iranian_actors</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Iran actor</td>\n",
       "      <td>Manouchehr Farid</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://x.com/2023</td>\n",
       "      <td>iranian_actors</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Iran candidate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://x.com/2023</td>\n",
       "      <td>Candidates</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Iran writer</td>\n",
       "      <td>Arian Moayed</td>\n",
       "      <td>2023</td>\n",
       "      <td>https://x.com/2023</td>\n",
       "      <td>iranian_writers</td>\n",
       "      <td>suspended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Row_Number         keyword  \\\n",
       "0            1  Iran candidate   \n",
       "1            2     Iran writer   \n",
       "2            3  Iran candidate   \n",
       "3            4     Iran writer   \n",
       "4            5      Iran actor   \n",
       "5            6  Iran candidate   \n",
       "6            7  Iran president   \n",
       "7            8  Iran president   \n",
       "8            9     Iran writer   \n",
       "9           10  Iran candidate   \n",
       "10          11  Iran president   \n",
       "11          12  Iran candidate   \n",
       "12          13  Iran president   \n",
       "13          14  Iran candidate   \n",
       "14          15  Iran president   \n",
       "15          16     Iran writer   \n",
       "16          17      Iran actor   \n",
       "17          18      Iran actor   \n",
       "18          19  Iran candidate   \n",
       "19          20     Iran writer   \n",
       "\n",
       "                                             poi_name Twitter_username  \\\n",
       "0                                                ã¾ã‚†ã¿ã‚“             2011   \n",
       "1                                      Mehrdad Afsari             2011   \n",
       "2                                            2012 ABC             2012   \n",
       "3                                         Behdad Sami             2012   \n",
       "4                                 Michael Shahrestani             2015   \n",
       "5                                                 NaN             2015   \n",
       "6                  Joint Comprehensive Plan of Action             2015   \n",
       "7   Reactions to the Joint Comprehensive Plan of A...             2015   \n",
       "8                                 Michael Shahrestani             2015   \n",
       "9                                                2016             2016   \n",
       "10  Criticism of the Joint Comprehensive Plan of A...             2016   \n",
       "11                                           Sam Beck             2017   \n",
       "12  Aftermath of the Joint Comprehensive Plan of A...             2017   \n",
       "13                                                NaN             2022   \n",
       "14                               Mahsa Amini protests             2022   \n",
       "15                              Ela (Andromache song)             2022   \n",
       "16                                       Arian Moayed             2023   \n",
       "17                                   Manouchehr Farid             2023   \n",
       "18                                                NaN             2023   \n",
       "19                                       Arian Moayed             2023   \n",
       "\n",
       "           Twitter_url       source_folder Twitter_status  \n",
       "0   https://x.com/2011          Candidates      suspended  \n",
       "1   https://x.com/2011     iranian_writers      suspended  \n",
       "2   https://x.com/2012          Candidates      suspended  \n",
       "3   https://x.com/2012     iranian_writers      suspended  \n",
       "4   https://x.com/2015      iranian_actors      suspended  \n",
       "5   https://x.com/2015          Candidates      suspended  \n",
       "6   https://x.com/2015  presidents_of_iran      suspended  \n",
       "7   https://x.com/2015  presidents_of_iran      suspended  \n",
       "8   https://x.com/2015     iranian_writers      suspended  \n",
       "9   https://x.com/2016          Candidates      suspended  \n",
       "10  https://x.com/2016  presidents_of_iran      suspended  \n",
       "11  https://x.com/2017          Candidates      suspended  \n",
       "12  https://x.com/2017  presidents_of_iran      suspended  \n",
       "13  https://x.com/2022          Candidates      suspended  \n",
       "14  https://x.com/2022  presidents_of_iran      suspended  \n",
       "15  https://x.com/2022     iranian_writers      suspended  \n",
       "16  https://x.com/2023      iranian_actors      suspended  \n",
       "17  https://x.com/2023      iranian_actors      suspended  \n",
       "18  https://x.com/2023          Candidates      suspended  \n",
       "19  https://x.com/2023     iranian_writers      suspended  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# Add \"Twitter_status\" column to mark suspended/invalid/missing users\n",
    "# Works on Manual_Search_POIs.csv\n",
    "# ============================================\n",
    "\n",
    "import os, re, pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# ---- Dynamic path detection ----\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "# ---- Config ----\n",
    "REL_ROOT  = \"POIs\"\n",
    "FILE_NAME = \"Manual_Search_POIs.csv\"   # <-- ××¢×•×“×›×Ÿ ×œ×©× ×”×§×•×‘×¥ ×”×—×“×©\n",
    "\n",
    "ROOT = os.path.join(IRAN_DIR, REL_ROOT)\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "path = os.path.join(ROOT, FILE_NAME)\n",
    "assert os.path.exists(path), f\"âŒ File not found: {path}\"\n",
    "print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# ---- Load ----\n",
    "try:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---- Helpers ----\n",
    "HANDLE_RE = re.compile(r\"^[A-Za-z0-9_]{1,15}$\")\n",
    "\n",
    "def classify_status(username: str) -> str:\n",
    "    if pd.isna(username):\n",
    "        return \"missing\"\n",
    "    s = str(username).strip().lstrip(\"@\")\n",
    "    if not s:\n",
    "        return \"missing\"\n",
    "    # ××¡×¤×¨ ×‘×œ×‘×“: ×œ×¢×™×ª×™× ××¦×‘×™×¢ ×¢×œ ×—×©×‘×•×Ÿ ××•×©×¢×”/××•××¨ ×œ××–×”×”\n",
    "    if s.isdigit():\n",
    "        return \"suspended\"\n",
    "    # ×™×“×™×ª ×œ× ×—×•×§×™×ª (××•×¨×š >15 ××• ×ª×•×•×™× ×œ× ×ª×§×™× ×™×/×¨×•×•×—×™×)\n",
    "    if (len(s) > 15) or (HANDLE_RE.fullmatch(s) is None):\n",
    "        return \"invalid\"\n",
    "    return \"active\"\n",
    "\n",
    "# ---- Compute & Save ----\n",
    "df[\"Twitter_status\"] = df[\"Twitter_username\"].apply(classify_status)\n",
    "\n",
    "df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… Updated file: {path} ({len(df)} rows)\")\n",
    "try:\n",
    "    display(df.head(20))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEiA_7JbyFvl"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ğŸ§® Step 6 â€“ Enhanced Statistics & Visuals (UPDATED)\n",
    "# Builds robust stats table + coverage + charts (portable)\n",
    "# ============================================\n",
    "\n",
    "from IPython.display import display\n",
    "import os, glob, re, pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Dynamic path detection ----------\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "REL_ROOT     = \"POIs\"\n",
    "OUT_BASENAME = \"POI_statistics.csv\"             # basic\n",
    "OUT_ENHANCED = \"POI_statistics_enhanced.csv\"    # enhanced with coverage\n",
    "FIG_DIR      = \"figures\"\n",
    "\n",
    "ROOT = os.path.join(IRAN_DIR, REL_ROOT)\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def safe_read_csv(path, nrows=None):\n",
    "    for enc in (None, \"utf-8\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, nrows=nrows, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # × ×™×¡×™×•×Ÿ ××—×¨×•×Ÿ ×¢× engine=python ×•-skip\n",
    "    try:\n",
    "        return pd.read_csv(path, nrows=nrows, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def is_wikipedia_csv(path):\n",
    "    \"\"\"Detect Wikipedia CSV by content (columns/values), not filename alone.\"\"\"\n",
    "    name = os.path.basename(path).lower()\n",
    "    if any(k in name for k in [\"wikidata\", \"with_twitter\"]):\n",
    "        return False\n",
    "    df = safe_read_csv(path, nrows=10)\n",
    "    if df is None or df.empty:\n",
    "        return False\n",
    "    cols = [str(c).strip().lower() for c in df.columns]\n",
    "    if any(any(k in c for k in [\"wikipedia\", \"wiki\", \"link\", \"url\"]) for c in cols):\n",
    "        return True\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            sample = \" \".join(map(str, df[c].dropna().astype(str).head(10).tolist())).lower()\n",
    "            if \"wikipedia.org\" in sample or sample.startswith(\"http\"):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def detect_twitter_col(df):\n",
    "    candidates = {\"twitter_username\",\"twitter user\",\"twitter_user\",\"twitter handle\",\"handle\",\"twitter\"}\n",
    "    for c in df.columns:\n",
    "        if str(c).strip().lower() in candidates:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        norm = re.sub(r'[^a-z]', '', str(c).lower())\n",
    "        if \"twitter\" in norm and (\"username\" in norm or \"handle\" in norm or \"user\" in norm or \"name\" in norm):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def detect_qid_col(df):\n",
    "    for c in df.columns:\n",
    "        if str(c).strip().lower() in {\"wikidata_qid\",\"qid\",\"wikidata id\",\"wikidataid\"}:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ---------- Build basic stats ----------\n",
    "rows = []\n",
    "folders = sorted([d for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))])\n",
    "\n",
    "for folder in folders:\n",
    "    fpath = os.path.join(ROOT, folder)\n",
    "    csvs = sorted(glob.glob(os.path.join(fpath, \"*.csv\")))\n",
    "    if not csvs:\n",
    "        continue\n",
    "\n",
    "    wiki_file = None\n",
    "    wikidata_file = None\n",
    "    twitter_file = None\n",
    "\n",
    "    # Twitter file by name\n",
    "    for p in csvs:\n",
    "        if re.search(r\"with_twitter\", os.path.basename(p), re.I):\n",
    "            twitter_file = p\n",
    "            break\n",
    "\n",
    "    # Wikidata: prefer detailed\n",
    "    for p in csvs:\n",
    "        if re.search(r\"wikidata.*detailed\", os.path.basename(p), re.I):\n",
    "            wikidata_file = p\n",
    "            break\n",
    "    if wikidata_file is None:\n",
    "        for p in csvs:\n",
    "            if re.search(r\"with_wikidata_ids_and_links\", os.path.basename(p), re.I):\n",
    "                wikidata_file = p\n",
    "                break\n",
    "\n",
    "    # Wikipedia by content (fallback to \"<folder>.csv\")\n",
    "    for p in csvs:\n",
    "        if is_wikipedia_csv(p):\n",
    "            wiki_file = p\n",
    "            break\n",
    "    if wiki_file is None:\n",
    "        fallback = os.path.join(fpath, f\"{folder}.csv\")\n",
    "        if os.path.exists(fallback):\n",
    "            wiki_file = fallback\n",
    "\n",
    "    wikipedia_count = 0\n",
    "    wikidata_count  = 0\n",
    "    twitter_count   = 0\n",
    "\n",
    "    # Wikipedia count\n",
    "    if wiki_file:\n",
    "        dfw = safe_read_csv(wiki_file)\n",
    "        if dfw is not None:\n",
    "            wikipedia_count = len(dfw)\n",
    "\n",
    "    # Wikidata count (count non-empty QIDs if column exists)\n",
    "    if wikidata_file:\n",
    "        dfd = safe_read_csv(wikidata_file)\n",
    "        if dfd is not None:\n",
    "            qcol = detect_qid_col(dfd)\n",
    "            wikidata_count = dfd[qcol].notna().sum() if qcol else len(dfd)\n",
    "\n",
    "    # Twitter count (valid handles only)\n",
    "    if twitter_file:\n",
    "        dft = safe_read_csv(twitter_file)\n",
    "        if dft is not None:\n",
    "            tw_col = detect_twitter_col(dft)\n",
    "            if tw_col:\n",
    "                s = dft[tw_col].astype(str).str.strip()\n",
    "                valid = s[(s != \"\") &\n",
    "                          (~s.str.fullmatch(r\"\\d+\")) &\n",
    "                          (~s.str.lower().isin({\"nan\",\"none\",\"null\",\"suspended\"}))]\n",
    "                twitter_count = valid.shape[0]\n",
    "\n",
    "    rows.append({\n",
    "        \"Category\": folder,\n",
    "        \"Wikipedia count\": wikipedia_count,\n",
    "        \"Wikidata count\": wikidata_count,\n",
    "        \"Twitter user count\": twitter_count\n",
    "    })\n",
    "\n",
    "basic = pd.DataFrame(rows).sort_values(\"Category\").reset_index(drop=True)\n",
    "basic_path = os.path.join(ROOT, OUT_BASENAME)\n",
    "basic.to_csv(basic_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… Saved basic table: {basic_path}\")\n",
    "display(basic)\n",
    "\n",
    "# ---------- Enhanced: coverage, gaps, totals ----------\n",
    "enh = basic.copy()\n",
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    enh[\"Wikidata coverage (%)\"] = (100 * enh[\"Wikidata count\"] / enh[\"Wikipedia count\"]).fillna(0).round(1)\n",
    "    enh[\"Twitter coverage (%)\"]  = (100 * enh[\"Twitter user count\"] / enh[\"Wikipedia count\"]).fillna(0).round(1)\n",
    "\n",
    "enh[\"Wikidata gap\"] = (enh[\"Wikipedia count\"] - enh[\"Wikidata count\"]).clip(lower=0)\n",
    "enh[\"Twitter gap\"]  = (enh[\"Wikipedia count\"] - enh[\"Twitter user count\"]).clip(lower=0)\n",
    "\n",
    "tot = pd.DataFrame([{\n",
    "    \"Category\": \"TOTAL\",\n",
    "    \"Wikipedia count\": enh[\"Wikipedia count\"].sum(),\n",
    "    \"Wikidata count\": enh[\"Wikidata count\"].sum(),\n",
    "    \"Twitter user count\": enh[\"Twitter user count\"].sum(),\n",
    "}])\n",
    "tot[\"Wikidata coverage (%)\"] = (100 * tot[\"Wikidata count\"] / tot[\"Wikipedia count\"]).round(1) if tot[\"Wikipedia count\"].iloc[0] else 0.0\n",
    "tot[\"Twitter coverage (%)\"]  = (100 * tot[\"Twitter user count\"] / tot[\"Wikipedia count\"]).round(1) if tot[\"Wikipedia count\"].iloc[0] else 0.0\n",
    "tot[\"Wikidata gap\"] = tot[\"Wikipedia count\"] - tot[\"Wikidata count\"]\n",
    "tot[\"Twitter gap\"]  = tot[\"Wikipedia count\"] - tot[\"Twitter user count\"]\n",
    "\n",
    "enhanced = pd.concat([enh, tot], ignore_index=True)\n",
    "enhanced_path = os.path.join(ROOT, OUT_ENHANCED)\n",
    "enhanced.to_csv(enhanced_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… Saved enhanced table: {enhanced_path}\")\n",
    "display(enhanced)\n",
    "\n",
    "# ---------- Charts ----------\n",
    "fig_dir = os.path.join(ROOT, FIG_DIR)\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# ××¡× × ×™× ××ª TOTAL ×¢×‘×•×¨ ×’×¨×¤×™× ×¤×¨-×§×˜×’×•×¨×™×”\n",
    "per_cat = enhanced[enhanced[\"Category\"] != \"TOTAL\"].reset_index(drop=True)\n",
    "\n",
    "# 1) Counts by category (lines) â€” ×œ×œ× TOTAL\n",
    "plt.figure(figsize=(12,5))\n",
    "for col in [\"Wikipedia count\", \"Wikidata count\", \"Twitter user count\"]:\n",
    "    plt.plot(per_cat[\"Category\"], per_cat[col], marker='o', label=col)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"POIs â€“ Counts by Source\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig1_path = os.path.join(fig_dir, \"counts_by_category.png\")\n",
    "plt.savefig(fig1_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ Saved: {fig1_path}\")\n",
    "\n",
    "# 2) Coverage by category (%) â€” ×œ×œ× TOTAL\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(per_cat[\"Category\"], per_cat[\"Wikidata coverage (%)\"], alpha=0.8, label=\"Wikidata coverage (%)\")\n",
    "plt.bar(per_cat[\"Category\"], per_cat[\"Twitter coverage (%)\"], alpha=0.6, label=\"Twitter coverage (%)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"POIs â€“ Coverage by Category (%)\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Coverage (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig2_path = os.path.join(fig_dir, \"coverage_by_category.png\")\n",
    "plt.savefig(fig2_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ Saved: {fig2_path}\")\n",
    "\n",
    "# 3) Stacked bars: gaps to Wikipedia â€” ×œ×œ× TOTAL\n",
    "plt.figure(figsize=(12,5))\n",
    "bar_x = range(len(per_cat))\n",
    "plt.bar(bar_x, per_cat[\"Wikidata count\"], label=\"Wikidata count\")\n",
    "plt.bar(bar_x, per_cat[\"Wikidata gap\"], bottom=per_cat[\"Wikidata count\"], label=\"Wikidata gap\")\n",
    "plt.xticks(bar_x, per_cat[\"Category\"], rotation=45, ha='right')\n",
    "plt.title(\"Wikidata Count + Gap to Wikipedia\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Entities\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig3_path = os.path.join(fig_dir, \"wikidata_gap_stacked.png\")\n",
    "plt.savefig(fig3_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ Saved: {fig3_path}\")\n",
    "\n",
    "print(\"\\nâœ… Done. Files written to:\")\n",
    "print(f\"- {basic_path}\")\n",
    "print(f\"- {enhanced_path}\")\n",
    "print(f\"- {fig1_path}\")\n",
    "print(f\"- {fig2_path}\")\n",
    "print(f\"- {fig3_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyxENkekGZG_"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y snscrape\n",
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git@master\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCiCJWiLY8OM"
   },
   "source": [
    "# *Build POI_twitter_users_data.csv from POIs_Search_Manual*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HAqzyi1ZdW7"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 7 (UPDATED) â€” Build POI_twitter_users_data.csv\n",
    "# Source: Manual_Search_POIs.csv  (not POIs_Search_Manual*)\n",
    "# Portable: dynamic Drive path detection, robust CSV reading\n",
    "# ============================================================\n",
    "\n",
    "# 0.1) Dynamic path detection + POIs root\n",
    "import os, re, glob\n",
    "from pathlib import Path\n",
    "\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "REL = \"POIs\"\n",
    "POIs_DIR = os.path.join(IRAN_DIR, REL)\n",
    "os.makedirs(POIs_DIR, exist_ok=True)\n",
    "print(\"ğŸ“ POIs_DIR:\", POIs_DIR)\n",
    "\n",
    "# 1) Quiet logs + progress bar\n",
    "import logging, warnings\n",
    "logging.getLogger(\"snscrape\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"snscrape.modules.twitter\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# 2) SSL & deps\n",
    "import sys, subprocess, importlib, site, pathlib, time, random\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"certifi\", \"requests\", \"urllib3\", \"idna\", \"tqdm\"], check=True)\n",
    "import certifi\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "\n",
    "# 3) Install/patch snscrape for Py3.12 (if needed)\n",
    "def ensure_snscrape():\n",
    "    try:\n",
    "        import snscrape.modules.twitter as sntwitter\n",
    "        return sntwitter\n",
    "    except Exception:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"git+https://github.com/JustAnotherArchivist/snscrape.git@master\"], check=True)\n",
    "        # patch legacy importer if needed\n",
    "        import pkgutil\n",
    "        for sp in set(site.getsitepackages() + [site.getusersitepackages()]):\n",
    "            p = pathlib.Path(sp) / \"snscrape\" / \"modules\" / \"__init__.py\"\n",
    "            if p.exists():\n",
    "                txt = p.read_text(encoding=\"utf-8\")\n",
    "                if \"find_module(\" in txt or \"load_module(\" in txt:\n",
    "                    import re as _re\n",
    "                    patched = _re.sub(\n",
    "                        r'(\\s*)module\\s*=\\s*importer\\.find_module\\(moduleName\\)\\.load_module\\(moduleName\\)',\n",
    "                        r'\\1import importlib\\n\\1module = importlib.import_module(moduleName)',\n",
    "                        txt\n",
    "                    )\n",
    "                    if \"prefixLen\" not in patched:\n",
    "                        patched = _re.sub(\n",
    "                            r'for importer, moduleName, ispkg in pkgutil\\.iter_modules\\(__path__, prefix\\):',\n",
    "                            'prefixLen = len(prefix)\\n    for importer, moduleName, ispkg in pkgutil.iter_modules(__path__, prefix):',\n",
    "                            patched\n",
    "                        )\n",
    "                    p.write_text(patched, encoding=\"utf-8\")\n",
    "        importlib.invalidate_caches()\n",
    "        import snscrape.modules.twitter as sntwitter\n",
    "        return sntwitter\n",
    "\n",
    "sntwitter = ensure_snscrape()\n",
    "\n",
    "# 4) Locate Manual_Search_POIs.csv under POIs_DIR\n",
    "import pandas as pd\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    for enc in (None, \"utf-8\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        return pd.read_csv(path, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "def find_manual_csv(root_dir: str) -> str:\n",
    "    # ×—×¤×© ×‘×“×™×•×§ Manual_Search_POIs.csv ×§×•×“×\n",
    "    exact = os.path.join(root_dir, \"Manual_Search_POIs.csv\")\n",
    "    if os.path.exists(exact):\n",
    "        return exact\n",
    "    # fallback: ×›×œ ×§×•×‘×¥ ×‘×©× Manual_Search_POIs*.csv\n",
    "    candidates = []\n",
    "    for dp, _, files in os.walk(root_dir):\n",
    "        for fn in files:\n",
    "            if re.match(r\"Manual_Search_POIs.*\\.csv$\", fn, flags=re.I):\n",
    "                full = os.path.join(dp, fn)\n",
    "                try:\n",
    "                    mtime = os.path.getmtime(full)\n",
    "                except Exception:\n",
    "                    mtime = 0\n",
    "                candidates.append((mtime, full))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"×œ× × ××¦× ×§×•×‘×¥ ×‘×©× 'Manual_Search_POIs*.csv' ×ª×—×ª: {root_dir}\")\n",
    "    candidates.sort(reverse=True)\n",
    "    return candidates[0][1]\n",
    "\n",
    "MANUAL_CSV = find_manual_csv(POIs_DIR)\n",
    "print(\"ğŸ“„ Manual CSV:\", MANUAL_CSV)\n",
    "\n",
    "# 5) Output side-by-side to POIs root\n",
    "OUT_CSV = os.path.join(POIs_DIR, \"POI_twitter_users_data.csv\")\n",
    "print(\"ğŸ“ Output CSV :\", OUT_CSV)\n",
    "\n",
    "# 6) Load manual CSV & detect twitter/name/status columns\n",
    "df = safe_read_csv(MANUAL_CSV)\n",
    "\n",
    "def looks_like_twitter_col(name: str) -> bool:\n",
    "    n = name.lower().strip()\n",
    "    keys = [\"twitter\", \"x\", \"handle\", \"username\", \"tw\", \"×˜×•×•×™×˜×¨\"]\n",
    "    return any(k == n or k in n for k in keys)\n",
    "\n",
    "twitter_col = next((c for c in df.columns if looks_like_twitter_col(str(c))), None)\n",
    "if twitter_col is None:\n",
    "    for c in df.columns:\n",
    "        sample = \" \".join(map(str, df[c].dropna().astype(str).head(40).tolist())).lower()\n",
    "        if \"twitter.com\" in sample or \"x.com/\" in sample or \"@\" in sample:\n",
    "            twitter_col = c\n",
    "            break\n",
    "assert twitter_col, \"×œ× ××¦××ª×™ ×¢××•×“×ª ×˜×•×•×™×˜×¨.\"\n",
    "\n",
    "status_col = next((c for c in df.columns if str(c).lower().strip() in [\"twitter_status\", \"status\", \"tw_status\"]), None)\n",
    "name_col   = next((c for c in df.columns if str(c).lower().strip() in [\"poi_name\", \"name\", \"full name\", \"fullname\"]), None)\n",
    "\n",
    "# 7) Extract usernames (max 15 chars per X rules)\n",
    "import re as _re\n",
    "HANDLE_RE = _re.compile(r\"^[A-Za-z0-9_]{1,15}$\")\n",
    "\n",
    "def extract_username(val) -> str | None:\n",
    "    if pd.isna(val): return None\n",
    "    s = str(val).strip()\n",
    "    if not s: return None\n",
    "    s = s.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"www.\", \"\")\n",
    "    m = _re.search(r\"(?:twitter\\.com|x\\.com)/([A-Za-z0-9_]{1,15})\", s, flags=_re.I)\n",
    "    if m: return m.group(1)\n",
    "    m = _re.search(r\"@([A-Za-z0-9_]{1,15})\", s)\n",
    "    if m: return m.group(1)\n",
    "    if HANDLE_RE.fullmatch(s): return s\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    u = extract_username(row.get(twitter_col))\n",
    "    if not u:\n",
    "        continue\n",
    "    rows.append({\n",
    "        \"username\": u,\n",
    "        \"poi_name\": row.get(name_col),\n",
    "        \"twitter_status\": (str(row.get(status_col)).lower().strip() if status_col else None)\n",
    "    })\n",
    "\n",
    "assert rows, \"×œ× × ××¦××• ×©××•×ª ××©×ª××© ×ª×§×™× ×™×.\"\n",
    "\n",
    "# 8) Helpers: scraping / stub rows\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def g(obj, name, default=None):\n",
    "    try: return getattr(obj, name, default)\n",
    "    except Exception: return default\n",
    "\n",
    "def safe_attr(obj, path, default=None):\n",
    "    cur = obj\n",
    "    try:\n",
    "        for part in path.split(\".\"):\n",
    "            cur = getattr(cur, part, None)\n",
    "            if cur is None: return default\n",
    "        return cur\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def build_stub(u, poi, status, note):\n",
    "    return {\n",
    "        \"full name\": None,\n",
    "        \"description / bio\": None,\n",
    "        \"followers_count\": None,\n",
    "        \"following_count\": None,\n",
    "        \"statuses_count\": None,\n",
    "        \"created_at\": None,\n",
    "        \"profile_image_url\": None,\n",
    "        \"banner_url\": None,\n",
    "        \"username\": u,\n",
    "        \"external_url\": None,\n",
    "        \"location\": None,\n",
    "        \"verified\": None,\n",
    "        \"protected\": None,\n",
    "        \"language\": None,\n",
    "        \"scraped_at\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"poi_name\": poi,\n",
    "        \"twitter_status\": status,\n",
    "        \"source\": \"csv_status_only\",\n",
    "        \"error\": note,\n",
    "    }\n",
    "\n",
    "def fetch_user_metadata(u: str):\n",
    "    user_obj, last_err = None, None\n",
    "    for ctor in [\n",
    "        lambda x: sntwitter.TwitterUserScraper(x),           # positional\n",
    "        lambda x: sntwitter.TwitterUserScraper(user=x),      # keyword (some builds)\n",
    "        lambda x: sntwitter.XUserScraper(x) if hasattr(sntwitter, \"XUserScraper\") else (_ for _ in ()).throw(AttributeError(\"XUserScraper missing\")),\n",
    "    ]:\n",
    "        try:\n",
    "            user_obj = ctor(u).entity\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if user_obj is None:\n",
    "        raise RuntimeError(repr(last_err))\n",
    "\n",
    "    return {\n",
    "        \"full name\": g(user_obj, \"displayname\"),\n",
    "        \"description / bio\": g(user_obj, \"renderedDescription\"),\n",
    "        \"followers_count\": g(user_obj, \"followersCount\"),\n",
    "        \"following_count\": g(user_obj, \"friendsCount\"),\n",
    "        \"statuses_count\": g(user_obj, \"statusesCount\"),\n",
    "        \"created_at\": g(user_obj, \"created\"),\n",
    "        \"profile_image_url\": g(user_obj, \"profileImageUrl\"),\n",
    "        \"banner_url\": g(user_obj, \"bannerImageUrl\"),\n",
    "        \"username\": g(user_obj, \"username\"),\n",
    "        \"external_url\": safe_attr(user_obj, \"link.url\"),\n",
    "        \"location\": g(user_obj, \"location\"),\n",
    "        \"verified\": g(user_obj, \"verified\"),\n",
    "        \"protected\": g(user_obj, \"protected\"),\n",
    "        \"language\": g(user_obj, \"lang\"),\n",
    "        \"scraped_at\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"poi_name\": None,  # × ××œ× ×‘×”××©×š\n",
    "        \"twitter_status\": None,  # × ××œ× ×‘×”××©×š\n",
    "        \"source\": \"snscrape\",\n",
    "        \"error\": None,\n",
    "    }\n",
    "\n",
    "# 9) Run with progress, retries, periodic saves\n",
    "MAX_USERS  = None     # ×œ×“×•×’××” 100 ×œ×‘×“×™×§×”; None = ×”×›×œ\n",
    "RETRIES    = 3\n",
    "BASE_SLEEP = 1.0\n",
    "SAVE_EVERY = 25       # ×©××™×¨×ª ×‘×™× ×™×™× ×›×œ N ××©×ª××©×™×\n",
    "\n",
    "to_process = rows[:MAX_USERS] if MAX_USERS else rows\n",
    "records = []\n",
    "ok = suspended_cnt = failed = 0\n",
    "\n",
    "cols_order = [\n",
    "    \"full name\", \"description / bio\", \"followers_count\", \"following_count\",\n",
    "    \"statuses_count\", \"created_at\", \"profile_image_url\", \"banner_url\",\n",
    "    \"username\", \"external_url\", \"location\", \"verified\", \"protected\",\n",
    "    \"language\", \"scraped_at\", \"poi_name\", \"twitter_status\", \"source\", \"error\"\n",
    "]\n",
    "\n",
    "pbar = tqdm(to_process, desc=\"Scraping users\", unit=\"user\")\n",
    "for idx, r in enumerate(pbar, start=1):\n",
    "    u = r[\"username\"]; poi = r[\"poi_name\"]; st = r[\"twitter_status\"]\n",
    "\n",
    "    # Respect suspended/invalid/missing from Step 5/6\n",
    "    if st and any(k in st for k in (\"suspend\", \"invalid\", \"missing\")):\n",
    "        records.append(build_stub(u, poi, st, note=f\"status_from_manual_csv:{st}\"))\n",
    "        suspended_cnt += 1 if \"suspend\" in (st or \"\") else suspended_cnt\n",
    "        pbar.set_postfix(ok=ok, suspended=suspended_cnt, failed=failed)\n",
    "        continue\n",
    "\n",
    "    for a in range(RETRIES):\n",
    "        try:\n",
    "            rec = fetch_user_metadata(u)\n",
    "            rec[\"poi_name\"] = poi\n",
    "            rec[\"twitter_status\"] = st\n",
    "            records.append(rec)\n",
    "            ok += 1\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if a == RETRIES - 1:\n",
    "                records.append(build_stub(u, poi, st, note=f\"scrape_failed: {e}\"))\n",
    "                failed += 1\n",
    "            time.sleep(BASE_SLEEP + 0.6 * a + random.random() * 0.4)\n",
    "\n",
    "    pbar.set_postfix(ok=ok, suspended=suspended_cnt, failed=failed)\n",
    "\n",
    "    # periodic save\n",
    "    if idx % SAVE_EVERY == 0:\n",
    "        tmp_df = pd.DataFrame.from_records(records)\n",
    "        tmp_df = tmp_df[[c for c in cols_order if c in tmp_df.columns]]\n",
    "        if os.path.exists(OUT_CSV):\n",
    "            old = safe_read_csv(OUT_CSV)\n",
    "            merged = (pd.concat([old, tmp_df], ignore_index=True)\n",
    "                      .drop_duplicates(subset=[\"username\"], keep=\"last\"))\n",
    "        else:\n",
    "            merged = tmp_df\n",
    "        merged.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 10) Final save/merge\n",
    "out_df = pd.DataFrame.from_records(records)\n",
    "out_df = out_df[[c for c in cols_order if c in out_df.columns]]\n",
    "\n",
    "if os.path.exists(OUT_CSV):\n",
    "    old = safe_read_csv(OUT_CSV)\n",
    "    final = (pd.concat([old, out_df], ignore_index=True)\n",
    "             .drop_duplicates(subset=[\"username\"], keep=\"last\"))\n",
    "else:\n",
    "    final = out_df\n",
    "\n",
    "final.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nâœ… DONE. Saved -> {OUT_CSV}\")\n",
    "print(f\"   Totals: scraped={ok}, suspended/invalid/missing(stub)={suspended_cnt}, failed(stub)={failed}, total_written={len(final)}\")\n",
    "\n",
    "# Preview\n",
    "try:\n",
    "    display(final.tail(5))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndMxspAAeD7r"
   },
   "source": [
    "# fill LANGUAGE with full names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQZ0o6FXZvJ-"
   },
   "outputs": [],
   "source": [
    "# ========= Post-process: fill LANGUAGE with full names (no abbreviations) =========\n",
    "# ×××œ× ×©×¤×ª ××©×ª××©×™× ××©×“×” ×”-bio ×”×™×›×Ÿ ×©×—×¡×¨, ×•×××™×¨ ×§×•×“×™× (en/fa/â€¦) ×œ×©××•×ª ××œ××™×\n",
    "\n",
    "# ×”×ª×§× ×•×ª ×¨×§ ×× ×¦×¨×™×š\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "except Exception:\n",
    "    !pip -q install langdetect\n",
    "    from langdetect import detect, DetectorFactory\n",
    "\n",
    "try:\n",
    "    import pycountry\n",
    "except Exception:\n",
    "    !pip -q install pycountry\n",
    "    import pycountry\n",
    "\n",
    "import os, re, glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "DetectorFactory.seed = 0  # ×™×¦×™×‘×•×ª ×‘×–×™×”×•×™\n",
    "\n",
    "# ---- Dynamic path detection ----\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "POIs_DIR = os.path.join(IRAN_DIR, \"POIs\")\n",
    "os.makedirs(POIs_DIR, exist_ok=True)\n",
    "\n",
    "# ×”×§×•×‘×¥ ×”××¨×›×–×™ ×©× ×•×¦×¨ ×‘×©×œ×‘ 7\n",
    "CSV = Path(os.path.join(POIs_DIR, \"POI_twitter_users_data.csv\"))\n",
    "if not CSV.exists():\n",
    "    raise FileNotFoundError(f\"âŒ ×œ× × ××¦× ×”×§×•×‘×¥: {CSV}\")\n",
    "print(f\"ğŸ“ Working with: {CSV}\")\n",
    "\n",
    "# ×§×¨×™××” ×¢××™×“×”\n",
    "try:\n",
    "    df = pd.read_csv(CSV, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(CSV, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ×•×“× ×©×§×™×™××ª ×¢××•×“×ª language ×›××—×¨×•×–×ª\n",
    "if \"language\" not in df.columns:\n",
    "    df[\"language\"] = \"\"\n",
    "df[\"language\"] = df[\"language\"].astype(\"string\")\n",
    "\n",
    "# ---- ×–×™×”×•×™ ×©×¤×” ××”-bio ×¨×§ ×”×™×›×Ÿ ×©×—×¡×¨/×¨×™×§ ----\n",
    "BIO_COL = \"description / bio\" if \"description / bio\" in df.columns else None\n",
    "\n",
    "def detect_lang_from_bio(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    s = str(text).strip()\n",
    "    if len(s) < 6:  # ×§×¦×¨ ××“×™, ×œ× × ×–×”×” ×›×“×™ ×œ×”×¤×—×™×ª false positives\n",
    "        return None\n",
    "    try:\n",
    "        return detect(s)  # ××—×–×™×¨ ×§×•×“ ×›××• 'en','fa','ar',...\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "if BIO_COL:\n",
    "    mask_missing = df[\"language\"].isna() | (df[\"language\"].str.strip().fillna(\"\") == \"\")\n",
    "    df.loc[mask_missing, \"language\"] = df.loc[mask_missing, BIO_COL].apply(detect_lang_from_bio)\n",
    "\n",
    "# ---- ×”××¨×” ×œ×©××•×ª ××œ××™× (×œ× ××§×¦×¨ ×©××•×ª ××œ××™× ×§×™×™××™×) ----\n",
    "# ×ª×•××š alpha-2/alpha-3 + × ×¤×•×¦×™×\n",
    "MANUAL_MAP = {\n",
    "    \"en\": \"English\",\n",
    "    \"fa\": \"Persian (Farsi)\",\n",
    "    \"ar\": \"Arabic\",\n",
    "    \"fr\": \"French\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"de\": \"German\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"tr\": \"Turkish\",\n",
    "    \"ur\": \"Urdu\",\n",
    "    \"pt\": \"Portuguese\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"bg\": \"Bulgarian\",\n",
    "    \"ro\": \"Romanian\",\n",
    "    \"et\": \"Estonian\",\n",
    "    \"sq\": \"Albanian\",\n",
    "    \"so\": \"Somali\",\n",
    "    \"id\": \"Indonesian\",\n",
    "    \"da\": \"Danish\",\n",
    "    \"sl\": \"Slovenian\",\n",
    "    \"ca\": \"Catalan\",\n",
    "    \"af\": \"Afrikaans\",\n",
    "    # alpha-3 × ×¤×•×¦×™×\n",
    "    \"fas\": \"Persian (Farsi)\",\n",
    "    \"ara\": \"Arabic\",\n",
    "    \"eng\": \"English\",\n",
    "    \"spa\": \"Spanish\",\n",
    "    \"deu\": \"German\",\n",
    "    \"ger\": \"German\",   # old biblio code\n",
    "    \"fra\": \"French\",\n",
    "    \"fre\": \"French\",   # old biblio code\n",
    "    \"rus\": \"Russian\",\n",
    "    \"tur\": \"Turkish\",\n",
    "    \"urd\": \"Urdu\",\n",
    "}\n",
    "\n",
    "def is_code_like(v: str) -> bool:\n",
    "    if not isinstance(v, str):\n",
    "        return False\n",
    "    s = v.strip()\n",
    "    # ×§×•×“ × ×¨××” ×›××• 2â€“3 ××•×ª×™×•×ª ×œ×˜×™× ×™×•×ª; ×œ× ×›×•×œ×œ×™× ×©××•×ª ××œ××™×/××™×œ×™× ×¢× ×¨×•×•×—\n",
    "    return bool(re.fullmatch(r\"[A-Za-z]{2,3}\", s))\n",
    "\n",
    "def to_full_language_name(val):\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    s = str(val).strip()\n",
    "    if s == \"\" or s.lower() == \"none\":\n",
    "        return \"\"\n",
    "    # ×× ×–×” ×œ× × ×¨××” ×›××• ×§×•×“ (×›×‘×¨ ×©× ××œ×?) â€” × ×©××™×¨ ×›××• ×©×”×•×\n",
    "    if not is_code_like(s):\n",
    "        return s\n",
    "    c = s.lower()\n",
    "    # ××¤×” ×™×“× ×™×ª ×§×•×“×\n",
    "    if c in MANUAL_MAP:\n",
    "        return MANUAL_MAP[c]\n",
    "    # pycountry: alpha-2\n",
    "    try:\n",
    "        lang = pycountry.languages.get(alpha_2=c)\n",
    "        if lang and getattr(lang, \"name\", None):\n",
    "            return lang.name\n",
    "    except Exception:\n",
    "        pass\n",
    "    # pycountry: alpha-3\n",
    "    try:\n",
    "        lang = pycountry.languages.get(alpha_3=c)\n",
    "        if lang and getattr(lang, \"name\", None):\n",
    "            return lang.name\n",
    "    except Exception:\n",
    "        pass\n",
    "    # × ×¤×™×œ×” ×—×–×¨×” â€” × ×©××™×¨ ×›××• ×©×”×•× (×œ× × ×›×ª×•×‘ ×§×•×“ ×¨×™×§)\n",
    "    return s\n",
    "\n",
    "# ×”××¨ ×¨×§ ×¢×¨×›×™× ×©×××© ×§×•×“/×§×•×“ ××–×•×”×”\n",
    "df[\"language\"] = df[\"language\"].apply(to_full_language_name).fillna(\"\").astype(\"string\")\n",
    "\n",
    "# ğŸ”’ ×’×™×‘×•×™ ×œ×¤× ×™ ×›×ª×™×‘×”\n",
    "backup = CSV.with_suffix(\".backup.csv\")\n",
    "df.to_csv(backup, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"ğŸ›Ÿ Backup written: {backup}\")\n",
    "\n",
    "# ×›×ª×™×‘×” ×—×–×¨×”\n",
    "df.to_csv(CSV, index=False, encoding=\"utf-8-sig\")\n",
    "filled = (df[\"language\"].str.len() > 0).sum()\n",
    "print(f\"âœ… language ×”×•××¨ ×œ×©××•×ª ××œ××™× ×•× ×©××¨. ×©×•×¨×•×ª ×¢× ×©×¤×” ×œ×-×¨×™×§×”: {filled} ××ª×•×š {len(df)}\")\n",
    "\n",
    "try:\n",
    "    display(df.head(10)[[\"username\",\"poi_name\",\"language\"]])\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eI4cpube54x"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 8 â€” POI Twitter statistics & summaries (portable, updated)\n",
    "# ============================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "import os, re, glob, math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# ---------- Dynamic path detection ----------\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "REL = \"POIs\"\n",
    "POIs_DIR = os.path.join(IRAN_DIR, REL)\n",
    "os.makedirs(POIs_DIR, exist_ok=True)\n",
    "print(\"ğŸ“ POIs_DIR:\", POIs_DIR)\n",
    "\n",
    "# ---------- ×¢×–×¨×”: ××¦×™××ª ×§×‘×¦×™× + ×§×¨×™××” ×¢××™×“×” ----------\n",
    "def newest(pattern: str):\n",
    "    files = glob.glob(os.path.join(POIs_DIR, pattern))\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda f: os.path.getmtime(f), reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "def safe_read_csv(path, nrows=None):\n",
    "    for enc in (None, \"utf-8\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, nrows=nrows, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        return pd.read_csv(path, nrows=nrows, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "DATA_CSV   = newest(\"POI_twitter_users_data*.csv\")\n",
    "# ×©× ×—×“×© ×‘×”×ª×× ×œ×©×™× ×•×™×™× ×”×§×•×“××™×\n",
    "MANUAL_CSV = newest(\"Manual_Search_POIs*.csv\")  # ××•×¤×¦×™×•× ×œ×™, ×œ×”×¤×§×ª ×§×˜×’×•×¨×™×”\n",
    "\n",
    "assert DATA_CSV is not None, \"×œ× × ××¦× ×§×•×‘×¥ POI_twitter_users_data*.csv â€“ ×™×© ×œ×”×¨×™×¥ ××ª ×©×œ×‘ 7 ×§×•×“×.\"\n",
    "\n",
    "print(\"ğŸ“„ DATA :\", DATA_CSV)\n",
    "print(\"ğŸ“„ MANUAL (optional):\", MANUAL_CSV)\n",
    "\n",
    "# ---------- ×˜×¢×™× ×ª ×”× ×ª×•× ×™× ----------\n",
    "df = safe_read_csv(DATA_CSV)\n",
    "if df is None:\n",
    "    raise RuntimeError(f\"×œ× ×”×¦×œ×—×ª×™ ×œ×§×¨×•× ××ª ×”×§×•×‘×¥: {DATA_CSV}\")\n",
    "\n",
    "# × ×•×•×“× ×¢××•×“×•×ª ×™×¡×•×“ â€“ × ×•×¦×¨×• ×‘×©×œ×‘ 7\n",
    "for col in [\"username\", \"followers_count\", \"following_count\", \"statuses_count\", \"description / bio\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "# ---------- ×”×•×¡×¤×ª ×§×˜×’×•×¨×™×” (×× ×™×© Manual) ----------\n",
    "HANDLE_RE = re.compile(r\"^[A-Za-z0-9_]{1,15}$\")\n",
    "\n",
    "def extract_username(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    s = str(val).strip()\n",
    "    s = s.replace(\"https://\",\"\").replace(\"http://\",\"\").replace(\"www.\",\"\")\n",
    "    m = re.search(r\"(?:twitter\\.com|x\\.com)/([A-Za-z0-9_]{1,15})\", s, re.I)\n",
    "    if m: return m.group(1)\n",
    "    m = re.search(r\"@([A-Za-z0-9_]{1,15})\", s)\n",
    "    if m: return m.group(1)\n",
    "    if HANDLE_RE.fullmatch(s): return s\n",
    "    return None\n",
    "\n",
    "category_col = None\n",
    "if MANUAL_CSV:\n",
    "    manual = safe_read_csv(MANUAL_CSV)\n",
    "    if manual is not None and not manual.empty:\n",
    "        # ×¢××•×“×ª ×™×•×–×¨ ××¤×©×¨×™×ª: Twitter_username / username / ...\n",
    "        cand_user_cols = [c for c in manual.columns if str(c).lower().strip() in\n",
    "                          {\"twitter_username\",\"username\",\"tw_username\",\"twitter user\",\"user\"}]\n",
    "        if not cand_user_cols:\n",
    "            # × ×™×¡×™×•×Ÿ ×—×™×œ×•×¥ ××”×¢××•×“×” ×¢× ×§×™×©×•×¨\n",
    "            url_like = []\n",
    "            for c in manual.columns:\n",
    "                try:\n",
    "                    sample = \" \".join(map(str, manual[c].dropna().astype(str).head(100).tolist())).lower()\n",
    "                except Exception:\n",
    "                    sample = \"\"\n",
    "                if \"twitter\" in str(c).lower() or \"x.com/\" in sample or \"twitter.com/\" in sample:\n",
    "                    url_like.append(c)\n",
    "            cand_user_cols = url_like[:1]\n",
    "\n",
    "        if cand_user_cols:\n",
    "            mu = manual.copy()\n",
    "            mu[\"__user_join_key__\"] = mu[cand_user_cols[0]].apply(extract_username).str.lower()\n",
    "\n",
    "            # ×§×˜×’×•×¨×™×”: ×§×•×“× source_folder ×× ×§×™×™×, ××—×¨×ª keyword, ××—×¨×ª unknown\n",
    "            if \"source_folder\" in mu.columns:\n",
    "                mu[\"category\"] = mu[\"source_folder\"].astype(str).str.strip().str.lower()\n",
    "            else:\n",
    "                cand_cat = None\n",
    "                for c in [\"keyword\", \"category\", \"ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ\", \"×§×˜×’×•×¨×™×”\"]:\n",
    "                    if c in mu.columns:\n",
    "                        cand_cat = c\n",
    "                        break\n",
    "                if cand_cat:\n",
    "                    mu[\"category\"] = (mu[cand_cat].astype(str).str.strip()\n",
    "                                      .str.replace(r\"\\s+\", \"_\", regex=True).str.lower())\n",
    "                else:\n",
    "                    mu[\"category\"] = \"unknown\"\n",
    "\n",
    "            df[\"__user_join_key__\"] = df[\"username\"].astype(str).str.lower()\n",
    "            df = df.merge(mu[[\"__user_join_key__\", \"category\"]], on=\"__user_join_key__\", how=\"left\")\n",
    "            df.drop(columns=[\"__user_join_key__\"], inplace=True, errors=\"ignore\")\n",
    "            category_col = \"category\"\n",
    "\n",
    "if category_col is None:\n",
    "    df[\"category\"] = \"unknown\"\n",
    "    category_col = \"category\"\n",
    "\n",
    "# ---------- × ×™×§×•×™ ×˜×™×¤×•×¡×™ ×¢××•×“×•×ª ××¡×¤×¨×™×•×ª ----------\n",
    "for c in [\"followers_count\", \"following_count\", \"statuses_count\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# ---------- ×—×™×©×•×‘×™ ×××§×¨×• ----------\n",
    "num_pois_total = df[\"username\"].nunique()\n",
    "\n",
    "avg_followers = df[\"followers_count\"].mean(skipna=True)\n",
    "avg_following = df[\"following_count\"].mean(skipna=True)\n",
    "avg_posts     = df[\"statuses_count\"].mean(skipna=True)\n",
    "\n",
    "print(\"\\nğŸŒŸ ×¡×™×›×•× ×›×œ×œ×™:\")\n",
    "print(f\"- ××¡×¤×¨ ×”-POIs ×”×›×•×œ×œ: {num_pois_total}\")\n",
    "print(f\"- ×××•×¦×¢ followers : {avg_followers:,.0f}\" if not math.isnan(avg_followers) else \"- ××™×Ÿ × ×ª×•× ×™× ×œ×¢×•×§×‘×™×\")\n",
    "print(f\"- ×××•×¦×¢ following : {avg_following:,.0f}\" if not math.isnan(avg_following) else \"- ××™×Ÿ × ×ª×•× ×™× ×œ× ×¢×§×‘×™×\")\n",
    "print(f\"- ×××•×¦×¢ posts     : {avg_posts:,.0f}\"     if not math.isnan(avg_posts)     else \"- ××™×Ÿ × ×ª×•× ×™× ×œ×¤×•×¡×˜×™×\")\n",
    "\n",
    "# ---------- ×˜×‘×œ×” 1: ×××¤×™×™× ×™× ×›××•×ª×™×™× ×œ×¤×™ ×§×˜×’×•×¨×™×” ----------\n",
    "grp = (df.groupby(\"category\", dropna=False)\n",
    "         .agg(\n",
    "             Num_POIs=(\"username\",\"nunique\"),\n",
    "             Avg_Followers=(\"followers_count\",\"mean\"),\n",
    "             Avg_Following=(\"following_count\",\"mean\"),\n",
    "             Avg_Posts=(\"statuses_count\",\"mean\"),\n",
    "          )\n",
    "         .reset_index()\n",
    "      )\n",
    "for c in [\"Avg_Followers\",\"Avg_Following\",\"Avg_Posts\"]:\n",
    "    grp[c] = grp[c].round(0)\n",
    "\n",
    "# ---------- ×˜×‘×œ×” 2: Occupation ××ª×•×š ×”×‘×™×• (×›×•×œ×œ ×‘×¨×™××•×ª) ----------\n",
    "occ_map = {\n",
    "    r\"\\bminister|mp|parliament|politic|senator|governor|president|ambassador|diplomat|mayor|congress|knesset\\b\": \"Politician\",\n",
    "    r\"\\bjournalist|reporter|editor|news|press\\b\": \"Journalist\",\n",
    "    r\"\\bactor|actress|cinema|film|movie|director|producer|screenwriter\\b\": \"Artist\",\n",
    "    r\"\\bsinger|vocal|music|musician|composer|band|rapper|songwriter\\b\": \"Artist\",\n",
    "    r\"\\bfootball|soccer|fifa|goalkeeper|striker|midfielder|defender|athlete\\b\": \"Athlete\",\n",
    "    r\"\\bscientist|researcher|professor|phd|chemist|physicist|biologist|engineer\\b\": \"Scientist\",\n",
    "    r\"\\bwriter|author|poet|novelist\\b\": \"Writer\",\n",
    "    r\"\\beconomist|economic\\b\": \"Economist\",\n",
    "    r\"\\bayatollah|cleric|imam|rabbi|priest\\b\": \"Religious_Leader\",\n",
    "    # Healthcare ×”×¨×—×‘×•×ª:\n",
    "    r\"\\bphysician|doctor|md|surgeon|cardiolog|oncolog|neurolog|pediatric|hospital|clinic|health\\b\": \"Healthcare\",\n",
    "}\n",
    "\n",
    "def infer_occ(text: str) -> str:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"Unknown\"\n",
    "    s = text.lower()\n",
    "    for pat, label in occ_map.items():\n",
    "        if re.search(pat, s):\n",
    "            return label\n",
    "    return \"Other/Unknown\"\n",
    "\n",
    "df[\"Occupation\"] = df[\"description / bio\"].apply(infer_occ)\n",
    "\n",
    "occ = (df.groupby(\"Occupation\", dropna=False)\n",
    "         .agg(Count=(\"username\",\"nunique\"))\n",
    "         .reset_index()\n",
    "      )\n",
    "total = occ[\"Count\"].sum() if len(occ) else 0\n",
    "occ[\"Percentage (%)\"] = (occ[\"Count\"] / total * 100).round(1) if total else 0\n",
    "\n",
    "# ---------- ×©××™×¨×” ×œ×§×‘×¦×™× ----------\n",
    "stamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_MASTER = os.path.join(POIs_DIR, \"POI_twitter_users_statistics.csv\")\n",
    "OUT_CAT    = os.path.join(POIs_DIR, \"POI_stats_by_category.csv\")\n",
    "OUT_OCC    = os.path.join(POIs_DIR, \"POI_stats_by_occupation.csv\")\n",
    "\n",
    "summary_row = pd.DataFrame([{\n",
    "    \"metric\":\"overview\",\n",
    "    \"num_pois_total\": num_pois_total,\n",
    "    \"avg_followers\": round(avg_followers,0) if not math.isnan(avg_followers) else None,\n",
    "    \"avg_following\": round(avg_following,0) if not math.isnan(avg_following) else None,\n",
    "    \"avg_posts\":     round(avg_posts,0)     if not math.isnan(avg_posts)     else None,\n",
    "    \"generated_at_utc\": stamp\n",
    "}])\n",
    "\n",
    "# × ×©××•×¨ â€œ×××¡×˜×¨â€ + × ×¦×¨×£ ××œ×™×• ××ª ×©×ª×™ ×”×˜×‘×œ××•×ª\n",
    "summary_row.to_csv(OUT_MASTER, index=False, encoding=\"utf-8-sig\")\n",
    "with open(OUT_MASTER, \"a\", encoding=\"utf-8-sig\") as f:\n",
    "    f.write(\"\\n# --- by category ---\\n\")\n",
    "    grp.to_csv(f, index=False)\n",
    "    f.write(\"\\n# --- by occupation ---\\n\")\n",
    "    occ.to_csv(f, index=False)\n",
    "\n",
    "# ×‘× ×•×¡×£ × ×©××•×¨ ×›×œ ×˜×‘×œ×” ×‘× ×¤×¨×“\n",
    "grp.to_csv(OUT_CAT, index=False, encoding=\"utf-8-sig\")\n",
    "occ.to_csv(OUT_OCC, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nâœ… × ×©××¨×• ×”×§×‘×¦×™×:\")\n",
    "print(\"  â€¢\", OUT_MASTER)\n",
    "print(\"  â€¢\", OUT_CAT)\n",
    "print(\"  â€¢\", OUT_OCC)\n",
    "\n",
    "print(\"\\nğŸ” ×”×¦×¦×” ××”×™×¨×” â€” ×˜×‘×œ×” 1 (×œ×¤×™ ×§×˜×’×•×¨×™×”):\")\n",
    "display(grp.sort_values(\"Num_POIs\", ascending=False).head(10))\n",
    "\n",
    "print(\"\\nğŸ” ×”×¦×¦×” ××”×™×¨×” â€” ×˜×‘×œ×” 2 (Occupation):\")\n",
    "display(occ.sort_values(\"Count\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Syr9TrATkYuU"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 8 â€” Visuals only (no CSVs) â€” portable (UPDATED)\n",
    "# ================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "import os, pandas as pd, matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from IPython.display import display\n",
    "\n",
    "# -------- Dynamic path detection --------\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "POIs_DIR = os.path.join(IRAN_DIR, \"POIs\")\n",
    "os.makedirs(POIs_DIR, exist_ok=True)\n",
    "\n",
    "# -------- Ensure figures/stage8_visuals exists --------\n",
    "FIG_DIR = os.path.join(POIs_DIR, \"figures\")\n",
    "OUT_DIR = os.path.join(FIG_DIR, \"stage8_visuals\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(\"ğŸ“ Saving figures to:\", OUT_DIR)\n",
    "\n",
    "# -------- Helpers --------\n",
    "def safe_read_csv(path):\n",
    "    for enc in (None, \"utf-8\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.read_csv(path, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "\n",
    "def thousands(x, pos):\n",
    "    try:\n",
    "        return f\"{int(x):,}\"\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def title_case(s):\n",
    "    return str(s).replace(\"_\", \" \").title()\n",
    "\n",
    "# -------- Load summary CSVs created in step 8 --------\n",
    "cat_csv = os.path.join(POIs_DIR, \"POI_stats_by_category.csv\")     # category, Num_POIs, Avg_Followers, Avg_Following, Avg_Posts\n",
    "occ_csv = os.path.join(POIs_DIR, \"POI_stats_by_occupation.csv\")   # Occupation, Count, Percentage (%)\n",
    "\n",
    "assert os.path.exists(cat_csv), f\"âŒ ×—×¡×¨ ×§×•×‘×¥: {cat_csv} (×”×¨×¥ ××ª STEP 8 ×”××œ× ×§×•×“×)\"\n",
    "assert os.path.exists(occ_csv), f\"âŒ ×—×¡×¨ ×§×•×‘×¥: {occ_csv} (×”×¨×¥ ××ª STEP 8 ×”××œ× ×§×•×“×)\"\n",
    "\n",
    "df_cat = safe_read_csv(cat_csv)\n",
    "df_occ = safe_read_csv(occ_csv)\n",
    "\n",
    "# × ×™×§×•×™ ×©××•×ª ×¢××•×“×•×ª\n",
    "df_cat.columns = [c.strip() for c in df_cat.columns]\n",
    "df_occ.columns = [c.strip() for c in df_occ.columns]\n",
    "\n",
    "# ×•×“× ×¢××•×“×•×ª ×—×•×‘×” ×§×™×™××•×ª\n",
    "for c in [\"category\", \"Num_POIs\", \"Avg_Followers\", \"Avg_Following\", \"Avg_Posts\"]:\n",
    "    if c not in df_cat.columns:\n",
    "        raise ValueError(f\"×¢××•×“×” ×—×¡×¨×” ×‘-POI_stats_by_category.csv: {c}\")\n",
    "\n",
    "for c in [\"Occupation\", \"Count\"]:\n",
    "    if c not in df_occ.columns:\n",
    "        raise ValueError(f\"×¢××•×“×” ×—×¡×¨×” ×‘-POI_stats_by_occupation.csv: {c}\")\n",
    "if \"Percentage (%)\" not in df_occ.columns:\n",
    "    df_occ[\"Percentage (%)\"] = (df_occ[\"Count\"] / df_occ[\"Count\"].sum() * 100) if df_occ[\"Count\"].sum() else 0\n",
    "\n",
    "# ×”××¨×•×ª ×•××™×–×•×’ ×¢×¨×›×™× ×—×¨×™×’×™×\n",
    "num_cols_cat = [\"Num_POIs\", \"Avg_Followers\", \"Avg_Following\", \"Avg_Posts\"]\n",
    "df_cat[num_cols_cat] = df_cat[num_cols_cat].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "df_occ[\"Count\"] = pd.to_numeric(df_occ[\"Count\"], errors=\"coerce\").fillna(0)\n",
    "df_occ[\"Percentage (%)\"] = pd.to_numeric(df_occ[\"Percentage (%)\"], errors=\"coerce\").fillna(0).clip(lower=0, upper=100)\n",
    "\n",
    "# ×œ×× ×•×¢ ×›×¤×™×œ×•×ª Other/Unknown ×•-Unknown\n",
    "df_occ[\"Occupation\"] = df_occ[\"Occupation\"].replace({\"Other/Unknown\": \"Unknown\"})\n",
    "df_occ = (df_occ.groupby(\"Occupation\", as_index=False)\n",
    "                 .agg({\"Count\": \"sum\", \"Percentage (%)\": \"sum\"}))\n",
    "df_occ[\"Percentage (%)\"] = df_occ[\"Percentage (%)\"].clip(upper=100)\n",
    "\n",
    "# ×¡×“×¨ ×œ×”×¦×’×” ×™×¤×”\n",
    "df_cat = df_cat.sort_values(\"Num_POIs\", ascending=True)\n",
    "df_occ = df_occ.sort_values(\"Percentage (%)\", ascending=True)\n",
    "\n",
    "# -------- 1) ×‘×¨×™×: ××¡×¤×¨ ×”-POIs ×œ×›×œ ×§×˜×’×•×¨×™×” --------\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.barh([title_case(c) for c in df_cat[\"category\"]], df_cat[\"Num_POIs\"])\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(thousands))\n",
    "plt.xlabel(\"Number of POIs\")\n",
    "plt.title(\"POIs per Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pois_per_category.png\"), dpi=150)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pois_per_category.svg\"))\n",
    "plt.show()\n",
    "\n",
    "# -------- 2) ×‘×¨×™×: ×××•×¦×¢ Followers ×œ×¤×™ ×§×˜×’×•×¨×™×” --------\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.barh([title_case(c) for c in df_cat[\"category\"]], df_cat[\"Avg_Followers\"])\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(thousands))\n",
    "plt.xlabel(\"Average Followers\")\n",
    "plt.title(\"Average Followers by Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"avg_followers_by_category.png\"), dpi=150)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"avg_followers_by_category.svg\"))\n",
    "plt.show()\n",
    "\n",
    "# -------- 3) ×‘×¨×™×: ×××•×¦×¢ Following ×œ×¤×™ ×§×˜×’×•×¨×™×” --------\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.barh([title_case(c) for c in df_cat[\"category\"]], df_cat[\"Avg_Following\"])\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(thousands))\n",
    "plt.xlabel(\"Average Following\")\n",
    "plt.title(\"Average Following by Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"avg_following_by_category.png\"), dpi=150)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"avg_following_by_category.svg\"))\n",
    "plt.show()\n",
    "\n",
    "# -------- 4) ×‘×¨×™×: ×”×ª×¤×œ×’×•×ª Occupation ×‘××—×•×–×™× --------\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_occ[\"Occupation\"], df_occ[\"Percentage (%)\"])\n",
    "plt.xlabel(\"Percentage (%)\")\n",
    "plt.title(\"Occupation Distribution of POIs\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"occupation_distribution.png\"), dpi=150)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"occupation_distribution.svg\"))\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Done. Visuals saved in:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oE4jOKaKvrH3"
   },
   "source": [
    "**API Twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ti7IylSqJGr"
   },
   "outputs": [],
   "source": [
    "pip install selenium webdriver-manager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PirGEDfj-YV5"
   },
   "outputs": [],
   "source": [
    "!apt-get update -y\n",
    "!apt-get install -y chromium-browser || apt-get install -y chromium\n",
    "!apt-get install -y chromium-chromedriver || apt-get install -y chromium-driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WVkmtlwp-yA"
   },
   "outputs": [],
   "source": [
    "# === ×“×™××’× ×•×¡×˜×™×§×” + ×¨×™×¢× ×•×Ÿ ×”×ª×§× ×•×ª ===\n",
    "!apt-get update -y\n",
    "# × ×ª×§×™×Ÿ ×’× chromium ×•×’× ×©× ×™ ×•×¨×™×× ×˜×™× ×©×œ chromedriver (×œ×¤×™ ×”×“×™×¡×˜×¨×• ×©×œ Colab)\n",
    "!apt-get install -y chromium-browser || apt-get install -y chromium\n",
    "!apt-get install -y chromium-chromedriver || apt-get install -y chromium-driver\n",
    "\n",
    "# ×‘×“×™×§×ª ×’×¨×¡××•×ª ×•× ×ª×™×‘×™×\n",
    "!which -a chromium chromium-browser || true\n",
    "!which -a chromedriver || true\n",
    "!chromium --version || chromium-browser --version\n",
    "!chromedriver --version\n",
    "!ls -l /usr/bin/chromedriver || true\n",
    "!ls -l /usr/lib/chromium-browser/chromedriver || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbBG-Zk6us2l"
   },
   "outputs": [],
   "source": [
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hZYLyl-uozH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTEXcdwAtn-m"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 9 â€” Followers/Following for POIs (Colab, Selenium)\n",
    "# ============================================\n",
    "\n",
    "# ---------- ğŸ”§ CONFIG ----------\n",
    "import os, json, time, random, re, pandas as pd, shutil, stat\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Dynamic path detection\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "BASE_DRIVE_DIR = os.path.join(IRAN_DIR, \"POIs\")\n",
    "USERS_CSV      = os.path.join(BASE_DRIVE_DIR, \"POI_twitter_users_data.csv\") # ×—×™×™×‘ ×¢××•×“×” 'username'\n",
    "OUT_DIR        = os.path.join(BASE_DRIVE_DIR, \"Candidates\")\n",
    "COOKIES_JSON   = os.path.join(BASE_DRIVE_DIR, \"x_cookies.json\")            # ğŸ”§ ×§×•×§×™×– JSON ××—×©×‘×•×Ÿ X ×”××—×•×‘×¨\n",
    "\n",
    "# ××”×™×¨ ×œ×‘×“×™×§×”:\n",
    "SAMPLE_SIZE  = None        # â† ×¢×•×“×›×Ÿ ×œ-5\n",
    "DO_FOLLOWERS = True\n",
    "DO_FOLLOWING = True\n",
    "MAX_PER_LIST = 80\n",
    "SCROLL_STEPS = 15\n",
    "OPEN_PAUSE   = 1.6\n",
    "SCROLL_PAUSE = 0.7\n",
    "\n",
    "# ××¤×©×¨ ×œ×‘×—×•×¨ ×™×“× ×™×ª ×‘××§×•× ×œ×§×—×ª ××”-CSV:\n",
    "MANUAL_USERS = []  # ×œ×“×•×’××”: [\"amir_jadidi\", \"OAradwinwin\", \"Golshifteh\"]\n",
    "\n",
    "print(f\"ğŸ“ Working directory: {IRAN_DIR}\")\n",
    "\n",
    "# ---------- FOLDERS ----------\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- INIT CHROMIUM ----------\n",
    "def _which(cmds):\n",
    "    import shutil as _sh\n",
    "    for c in cmds:\n",
    "        p = _sh.which(c)\n",
    "        if p:\n",
    "            return p\n",
    "    return None\n",
    "def _chmod_x(path):\n",
    "    try:\n",
    "        mode = os.stat(path).st_mode\n",
    "        os.chmod(path, mode | stat.S_IEXEC)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "CHROME_BIN = _which([\"chromium-browser\",\"chromium\",\"google-chrome\",\"chrome\"])\n",
    "CHROMEDRIVER_CANDIDATES = [\n",
    "    _which([\"chromedriver\"]),\n",
    "    \"/usr/bin/chromedriver\",\n",
    "    \"/usr/lib/chromium-browser/chromedriver\",\n",
    "]\n",
    "assert CHROME_BIN, \"Chromium/Chrome ×œ× × ××¦×. ×•×“× ×©×”×¨×¦×ª ××ª ×ª× ×”×”×ª×§× ×”.\"\n",
    "\n",
    "opts = Options()\n",
    "opts.binary_location = CHROME_BIN\n",
    "opts.add_argument(\"--headless=new\")\n",
    "opts.add_argument(\"--no-sandbox\")\n",
    "opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "opts.add_argument(\"--disable-gpu\")\n",
    "opts.add_argument(\"--window-size=1280,2000\")\n",
    "opts.add_argument(\"--lang=en-US,en;q=0.9\")\n",
    "\n",
    "driver = None\n",
    "for drv in [p for p in CHROMEDRIVER_CANDIDATES if p and os.path.exists(p)]:\n",
    "    try:\n",
    "        _chmod_x(drv)\n",
    "        service = Service(drv)\n",
    "        driver  = webdriver.Chrome(service=service, options=opts)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"chromedriver try failed:\", e)\n",
    "        driver = None\n",
    "if driver is None:\n",
    "    driver = webdriver.Chrome(options=opts)  # Selenium Manager fallback\n",
    "print(\"âœ… WebDriver is up.\")\n",
    "\n",
    "# ---------- COOKIES ----------\n",
    "def load_cookies(cookies_path: str):\n",
    "    driver.get(\"https://x.com/\")\n",
    "    time.sleep(1.5)\n",
    "    with open(cookies_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cookies = json.load(f)\n",
    "    injected = 0\n",
    "    for c in cookies:\n",
    "        try:\n",
    "            c_fixed = {\n",
    "                \"name\": c.get(\"name\"),\n",
    "                \"value\": c.get(\"value\"),\n",
    "                \"domain\": c.get(\"domain\", \".x.com\"),\n",
    "                \"path\": c.get(\"path\", \"/\"),\n",
    "                \"secure\": bool(c.get(\"secure\", True)),\n",
    "                \"httpOnly\": bool(c.get(\"httpOnly\", False))\n",
    "            }\n",
    "            if \"expiry\" in c:           c_fixed[\"expiry\"] = int(c[\"expiry\"])\n",
    "            elif \"expirationDate\" in c: c_fixed[\"expiry\"] = int(c[\"expirationDate\"])\n",
    "            driver.add_cookie(c_fixed); injected += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    driver.refresh(); time.sleep(2.0)\n",
    "    print(f\"Injected {injected} cookies.\")\n",
    "\n",
    "# ---------- SCRAPE HELPERS ----------\n",
    "USER_HREF_RE = re.compile(r\"^/[A-Za-z0-9_]{1,15}$\")\n",
    "\n",
    "def _extract_usernames_from_page():\n",
    "    anchors = driver.find_elements(By.XPATH, \"//a[@href]\")\n",
    "    names = set()\n",
    "    for a in anchors:\n",
    "        try:\n",
    "            href = a.get_attribute(\"href\") or \"\"\n",
    "            path  = \"/\" + href.split(\"://\",1)[-1].split(\"/\",1)[-1]\n",
    "            first = \"/\" + path.strip(\"/\").split(\"/\",1)[0]\n",
    "            if USER_HREF_RE.fullmatch(first):\n",
    "                cand = first.strip(\"/\")\n",
    "                if cand.lower() in {\"home\",\"i\",\"explore\",\"notifications\",\"messages\",\"settings\"}:\n",
    "                    continue\n",
    "                names.add(cand)\n",
    "        except:\n",
    "            pass\n",
    "    return list(names)\n",
    "\n",
    "def _smooth_scroll(steps):\n",
    "    last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for _ in range(steps):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE + random.uniform(0.15, 0.35))\n",
    "        new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_h == last_h:\n",
    "            time.sleep(0.5)\n",
    "            new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_h == last_h:\n",
    "                break\n",
    "        last_h = new_h\n",
    "\n",
    "def _collect_list(user, tab):\n",
    "    url = f\"https://x.com/{user}/{tab}\"\n",
    "    driver.get(url); time.sleep(OPEN_PAUSE + random.uniform(0.1, 0.3))\n",
    "    got = set()\n",
    "    for _ in range(SCROLL_STEPS):\n",
    "        got.update(_extract_usernames_from_page())\n",
    "        if len(got) >= MAX_PER_LIST:\n",
    "            break\n",
    "        _smooth_scroll(1)\n",
    "    got.discard(user)\n",
    "    return list(got)[:MAX_PER_LIST]\n",
    "\n",
    "# ---------- Full name helper (×œ×“×¨×™×©×ª ×”-README) ----------\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def _get_full_name(user):\n",
    "    \"\"\"××—×–×™×¨ ××ª ×©× ×”×ª×¦×•×’×” (Full Name) ××¤×¨×•×¤×™×œ ×”××©×ª××©, ××• None ×× ×œ× × ××¦×.\"\"\"\n",
    "    try:\n",
    "        driver.get(f\"https://x.com/{user}\")\n",
    "        time.sleep(OPEN_PAUSE)\n",
    "        el = WebDriverWait(driver, 6).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid=\"UserName\"]'))\n",
    "        )\n",
    "        txt = el.text or \"\"\n",
    "        return txt.split(\"\\n\")[0].strip() if txt else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------- LOAD USERS ----------\n",
    "df = pd.read_csv(USERS_CSV)\n",
    "assert \"username\" in df.columns, \"×‘×§×•×‘×¥ ×”××©×ª××©×™× ×—×¡×¨×” ×¢××•×“×” 'username'\"\n",
    "users_all = [str(u).strip().lstrip(\"@\") for u in df[\"username\"].dropna()]\n",
    "users_all = list(dict.fromkeys([u for u in users_all if u]))\n",
    "\n",
    "if MANUAL_USERS:\n",
    "    users = [u.strip().lstrip(\"@\") for u in MANUAL_USERS] if SAMPLE_SIZE in (None, 0) else [u.strip().lstrip(\"@\") for u in MANUAL_USERS][:SAMPLE_SIZE]\n",
    "else:\n",
    "    users = users_all if SAMPLE_SIZE in (None, 0) else users_all[:SAMPLE_SIZE]\n",
    "\n",
    "print(f\"Processing {len(users)} users:\", users[:10], \"...\" if len(users) > 10 else \"\")\n",
    "\n",
    "\n",
    "# ---------- RUN ----------\n",
    "load_cookies(COOKIES_JSON)  # ×—×©×•×‘: ×§×•×§×™×– ×¢×“×›× ×™×™× ××”×—×©×‘×•×Ÿ ×©×œ×š\n",
    "connections, stats = [], []\n",
    "\n",
    "for uname in users:\n",
    "    print(f\"\\nâ¡ï¸ {uname}\")\n",
    "    errs = []\n",
    "    followers, following = [], []\n",
    "\n",
    "    if DO_FOLLOWERS:\n",
    "        try:\n",
    "            followers = _collect_list(uname, \"followers\")\n",
    "        except Exception as e:\n",
    "            errs.append(f\"followers_error:{e}\")\n",
    "\n",
    "    if DO_FOLLOWING:\n",
    "        try:\n",
    "            following = _collect_list(uname, \"following\")\n",
    "        except Exception as e:\n",
    "            errs.append(f\"following_error:{e}\")\n",
    "\n",
    "    for x in followers:\n",
    "        connections.append((uname, x, \"follower\"))\n",
    "    for x in following:\n",
    "        connections.append((uname, x, \"following\"))\n",
    "\n",
    "    full_name = _get_full_name(uname)  # â† ×—×“×©: ×©× ×ª×¦×•×’×” ×œ×¡×˜×˜×™×¡×˜×™×§×•×ª\n",
    "    stats.append({\n",
    "        \"Twitter_username\": uname,\n",
    "        \"Full_Name\": full_name,                       # â† ×—×“×©\n",
    "        \"Followers_Collected\": len(followers),\n",
    "        \"Following_Collected\": len(following),\n",
    "        \"Error\": \"; \".join(errs) if errs else None\n",
    "    })\n",
    "\n",
    "# ---------- SAVE (×‘×“×™×•×§ ×œ×¤×™ ×”×“×¨×™×©×•×ª ×©×œ ×¡×¢×™×£ 9) ----------\n",
    "CONN_CSV = os.path.join(OUT_DIR, \"POIs_candidate_connections.csv\")\n",
    "STAT_CSV = os.path.join(OUT_DIR, \"Candidates_statistics.csv\")\n",
    "\n",
    "# ××—×™×§×” ××•×˜×•××˜×™×ª ×›×“×™ ×©×ª×¨××” ×¢×“×›×•×Ÿ ××™×™×“×™\n",
    "for path in [CONN_CSV, STAT_CSV]:\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "        print(f\"ğŸ§¹ Deleted old file: {path}\")\n",
    "\n",
    "(pd.DataFrame(connections, columns=[\"target_username\",\"other_username\",\"type\"])\n",
    "   .drop_duplicates()\n",
    "   .to_csv(CONN_CSV, index=False, encoding=\"utf-8-sig\"))\n",
    "\n",
    "\n",
    "(pd.DataFrame(stats)[[\"Twitter_username\",\"Full_Name\",\"Followers_Collected\",\"Following_Collected\",\"Error\"]]\n",
    "\n",
    "   .to_csv(STAT_CSV, index=False, encoding=\"utf-8-sig\"))print(\"stats:\", STAT_CSV)\n",
    "\n",
    "print(\"connections:\", CONN_CSV, f\"rows={len(connections)}\")\n",
    "print(\"âœ… Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjSs7XxdSn_R"
   },
   "outputs": [],
   "source": [
    "!pip -q install playwright tqdm pandas\n",
    "!playwright install --with-deps chromium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYMVJXn7QUPq"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 10 â€” Candidates user metadata (built on your twitter_selenium scraper)\n",
    "# Output: /content/drive/MyDrive/Iran/POIs/Candidates/Candidates_user_data.csv\n",
    "# ============================================\n",
    "\n",
    "# --- ×× ×¦×¨×™×š ×”×ª×§× ×•×ª ×‘×¡×™×¡ (×‘×˜×œ ×”×¢×¨×” ×•×”×¨×™×¥ ×¤×¢× ××—×ª) ---\n",
    "# !apt-get update -y && apt-get install -y chromium-browser chromium-chromedriver\n",
    "# !pip install -U selenium tqdm pandas\n",
    "\n",
    "# ---------- CONFIG (Dynamic path detection) ----------\n",
    "import os, json, time, shutil, pandas as pd, sys, importlib.util, re\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Auto-detect project root\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "BASE_DRIVE_DIR = os.path.join(IRAN_DIR, 'POIs')\n",
    "CAND_DIR       = os.path.join(BASE_DRIVE_DIR, 'Candidates')\n",
    "\n",
    "USERS_CSV      = os.path.join(BASE_DRIVE_DIR, 'POI_twitter_users_data.csv')     # ×§×œ×˜: ×”-POIs ×”××§×•×¨×™×™× (×¢××•×“×”: username)\n",
    "CONN_CSV       = os.path.join(CAND_DIR, 'POIs_candidate_connections.csv')       # ×§×œ×˜: ×©×œ×‘ 9 (×¢××•×“×•×ª: target_username, other_username)\n",
    "OUT_CSV        = os.path.join(CAND_DIR, 'Candidates_user_data.csv')             # ×¤×œ×˜: ×©×œ×‘ 10\n",
    "\n",
    "# ğŸ”¹ ×§×•×‘×¥ ×”-scraper ×©×œ×š\n",
    "SCRAPER_PATH = os.path.join(BASE_DRIVE_DIR, 'tools', 'twitter_selenium.py')\n",
    "\n",
    "# ×©×œ×™×˜×” ×‘×”×¨×¦×”\n",
    "SAMPLE_LIMIT      = 4000     # â† ×‘×“×™×§×ª ×¢×©×Ÿ ×¢×œ 10; ×©× ×” ×œ-None ×›×“×™ ×œ×¨×•×¥ ×¢×œ ×›×•×œ×\n",
    "BATCH_SIZE        = 40      # ×›××” ××©×ª××©×™× ×‘×›×œ ×× ×”\n",
    "RETRIES_PER_USER  = 2       # × ×™×¡×™×•× ×•×ª ×œ×›×œ ××©×ª××©\n",
    "OPEN_PAUSE        = 1.4     # ×©× ×™×•×ª ×”××ª× ×” ××—×¨×™ ×¤×ª×™×—×ª ×¤×¨×•×¤×™×œ\n",
    "\n",
    "# ××™×¤×•×™ ×¢××•×“×•×ª: ××”×¡×§×¨×•×œ×¨ ×©×œ×š ××œ ×”×¡×›×™××” ×”×ª×§× ×™×ª ×©×œ ×”×¤×¨×•×™×§×˜\n",
    "COLUMNS = [\n",
    "    \"username\",\"Full_Name\",\"Bio\",\"Location\",\"External_URL\",\n",
    "    \"Followers_Count\",\"Following_Count\",\"Is_Verified\",\"Is_Protected\",\n",
    "    \"Joined_Date\",\"Profile_URL\",\"Profile_Image\"\n",
    "]\n",
    "\n",
    "Path(CAND_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"ğŸ“ Working directory: {IRAN_DIR}\")\n",
    "\n",
    "# ---------- LOAD YOUR SCRAPER MODULE ----------\n",
    "def load_scraper(scraper_path: str):\n",
    "    if not os.path.exists(scraper_path):\n",
    "        raise FileNotFoundError(f\"twitter_selenium.py not found at: {scraper_path}\")\n",
    "    spec = importlib.util.spec_from_file_location(\"twitter_selenium\", scraper_path)\n",
    "    mod  = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"twitter_selenium\"] = mod\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "ts = load_scraper(SCRAPER_PATH)  # ××›×™×œ scrape_twitter_profile(username)\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def normalize_row_from_scraper(d: dict):\n",
    "    \"\"\"\n",
    "    ×××¤×” ××ª ×¤×œ×˜ ×”×¡×§×¨×•×œ×¨ ×©×œ×š ×œ×©×“×•×ª ×©×œ ×”×¤×¨×•×™×§×˜.\n",
    "    scraper keys (your file): user_name, name, bio, location, url, joined_date,\n",
    "                              followers, following, verified, profile_image, profile_url\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"username\":       (d.get(\"user_name\") or \"\").lstrip(\"@\"),\n",
    "        \"Full_Name\":      d.get(\"name\"),\n",
    "        \"Bio\":            d.get(\"bio\"),\n",
    "        \"Location\":       d.get(\"location\"),\n",
    "        \"External_URL\":   d.get(\"url\"),\n",
    "        \"Followers_Count\":d.get(\"followers\"),\n",
    "        \"Following_Count\":d.get(\"following\"),\n",
    "        \"Is_Verified\":    bool(d.get(\"verified\")),\n",
    "        \"Is_Protected\":   None,  # ×”×¡×§×¨×•×œ×¨ ×©×œ×š ×œ× ××—×œ×¥; ××¤×©×¨ ×œ×”×©××™×¨ None (×œ× × ×“×¨×© ×‘×¡×¢×™×£)\n",
    "        \"Joined_Date\":    d.get(\"joined_date\"),\n",
    "        \"Profile_URL\":    d.get(\"profile_url\"),\n",
    "        \"Profile_Image\":  d.get(\"profile_image\")\n",
    "    }\n",
    "\n",
    "# ---------- BUILD USER SET (×©×œ×‘ 9 + ×”××§×•×¨, ×œ×œ× ×›×¤×™×œ×•×™×•×ª) ----------\n",
    "user_set = set()\n",
    "\n",
    "if os.path.exists(CONN_CSV):\n",
    "    df_conn = pd.read_csv(CONN_CSV)\n",
    "    if \"target_username\" in df_conn.columns:\n",
    "        user_set.update(df_conn[\"target_username\"].dropna().astype(str).str.replace(\"@\",\"\", regex=False).str.strip())\n",
    "    if \"other_username\" in df_conn.columns:\n",
    "        user_set.update(df_conn[\"other_username\"].dropna().astype(str).str.replace(\"@\",\"\", regex=False).str.strip())\n",
    "\n",
    "if os.path.exists(USERS_CSV):\n",
    "    df_users = pd.read_csv(USERS_CSV)\n",
    "    if \"username\" in df_users.columns:\n",
    "        user_set.update(df_users[\"username\"].dropna().astype(str).str.replace(\"@\",\"\", regex=False).str.strip())\n",
    "\n",
    "users_all = [u for u in dict.fromkeys([u for u in user_set if u])]  # ×™×™×—×•×“ + ×©××™×¨×ª ×¡×“×¨\n",
    "if SAMPLE_LIMIT:\n",
    "    users_all = users_all[:SAMPLE_LIMIT]\n",
    "\n",
    "print(f\"Users to enrich: {len(users_all)} â†’ {users_all[:5]}\")\n",
    "\n",
    "# ---------- HEADER BOOTSTRAP (××‘×˜×™×— ×›×•×ª×¨×•×ª) ----------\n",
    "if not os.path.exists(OUT_CSV):\n",
    "    pd.DataFrame(columns=COLUMNS).to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- RESUME (×“×œ×’ ×¢×œ ××™ ×©×›×‘×¨ × ×©××¨) ----------\n",
    "done = set()\n",
    "try:\n",
    "    prev = pd.read_csv(OUT_CSV, usecols=[\"username\"])\n",
    "    done = set(prev[\"username\"].dropna().astype(str))\n",
    "    print(f\"â†» Resume: skipping {len(done)} already saved.\")\n",
    "except Exception as e:\n",
    "    print(\"Resume read issue:\", e)\n",
    "\n",
    "# ---------- RUN (×‘×× ×•×ª) ----------\n",
    "from math import ceil\n",
    "total   = len(users_all)\n",
    "batches = ceil(total / BATCH_SIZE)\n",
    "idx = 0\n",
    "\n",
    "for b in range(batches):\n",
    "    chunk = [u for u in users_all[idx: idx+BATCH_SIZE] if u not in done]\n",
    "    idx += BATCH_SIZE\n",
    "    if not chunk:\n",
    "        continue\n",
    "\n",
    "    with tqdm(total=len(chunk), desc=f\"Batch {b+1}/{batches}\", unit=\"user\") as pbar:\n",
    "        for uname in chunk:\n",
    "            row = None\n",
    "            # × ×™×¡×™×•× ×•×ª ×—×•×–×¨×™× ×œ×›×œ ××©×ª××© (×”×¡×§×¨×•×œ×¨ ×™×¤×ª×—/×™×¡×’×•×¨ ×›×¨×•× ×‘×¢×¦××•)\n",
    "            for attempt in range(RETRIES_PER_USER + 1):\n",
    "                try:\n",
    "                    raw = ts.scrape_twitter_profile(uname)   # âš ï¸ ××ª×•×š twitter_selenium.py ×©×œ×š\n",
    "                    row = normalize_row_from_scraper(raw)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < RETRIES_PER_USER:\n",
    "                        time.sleep(0.8)\n",
    "                    else:\n",
    "                        # ×‘××§×¨×” ×›×©×œ â€” × ×©××•×¨ ×©×•×¨×” \"×¨×™×§×”\" ×¢× ×”×©× ×•×”-URL ×‘×œ×‘×“, ×›×“×™ ×œ× ×œ×—×¡×•× ×”×ª×§×“××•×ª\n",
    "                        row = {\n",
    "                            \"username\": uname, \"Full_Name\": None, \"Bio\": None, \"Location\": None,\n",
    "                            \"External_URL\": None, \"Followers_Count\": None, \"Following_Count\": None,\n",
    "                            \"Is_Verified\": None, \"Is_Protected\": None, \"Joined_Date\": None,\n",
    "                            \"Profile_URL\": f\"https://twitter.com/{uname}\", \"Profile_Image\": None\n",
    "                        }\n",
    "\n",
    "            pd.DataFrame([row], columns=COLUMNS).to_csv(\n",
    "                OUT_CSV, mode=\"a\", header=False, index=False, encoding=\"utf-8-sig\"\n",
    "            )\n",
    "            done.add(uname)\n",
    "            pbar.update(1)\n",
    "\n",
    "# ---------- ×“×”-×“×•×¤×œ×™×§×¦×™×” (×œ×™×ª×¨ ×‘×™×˜×—×•×Ÿ) ----------\n",
    "try:\n",
    "    df_out = pd.read_csv(OUT_CSV)\n",
    "    df_out = df_out.drop_duplicates(subset=[\"username\"], keep=\"first\")\n",
    "    df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"âœ… Saved:\", OUT_CSV)\n",
    "    print(df_out.head())\n",
    "except Exception as e:\n",
    "    print(\"Compact error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znmS1Wnnnu_l"
   },
   "outputs": [],
   "source": [
    "ls /content/drive/MyDrive/Iran/POIs/tools\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (IRAN Project)",
   "language": "python",
   "name": "iran"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
