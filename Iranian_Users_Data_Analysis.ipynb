{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w9_WRvuHxVE"
   },
   "source": [
    "Iran Twitter / X Data Analysis Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6PofLTHdHfU_"
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Iran Twitter / X Data Analysis Project\n",
    "# ====================================================\n",
    "# Course: Data Science Final Project (SCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q requests pandas urllib3 emoji transformers sentence-transformers scikit-learn matplotlib seaborn plotly numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOI74U-kVM5v"
   },
   "source": [
    "# **italicized text*POI Scraper (Wikipedia Categories â†’ CSV)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myZurgeOLVxN"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# File: step1_collect_wikipedia_categories.py (UPDATED)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#   Also produces a summary table and a ZIP with all CSVs (for Colab download).\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# ============================================\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mzipfile\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quote\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# File: step1_collect_wikipedia_categories.py (UPDATED)\n",
    "#\n",
    "# Description:\n",
    "#   Recursively collects people/pages from Wikipedia categories (including subcategories),\n",
    "#   and saves a CSV per category with columns: Name, Wikipedia_Link.\n",
    "#   Output structure (per course spec):\n",
    "#       <IRAN_DIR>/\n",
    "#         POIs/\n",
    "#           <slug>/\n",
    "#             <slug>_wikipedia.csv\n",
    "#   Also produces a summary table and a ZIP with all CSVs.\n",
    "# ============================================\n",
    "\n",
    "import requests, time, os, zipfile\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# ---------------- IRAN_DIR resolution ----------------\n",
    "# Set the Iran directory path (adjust if needed)\n",
    "IRAN_DIR = r\"C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\"\n",
    "\n",
    "if not os.path.isdir(IRAN_DIR):\n",
    "    raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran' ×‘× ×ª×™×‘: {IRAN_DIR}\")\n",
    "\n",
    "# Base POIs directory (as per course structure)\n",
    "POIS_DIR = os.path.join(IRAN_DIR, \"POIs\")\n",
    "os.makedirs(POIS_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------- API config ----------------\n",
    "BASE_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "HEADERS = {\"User-Agent\": \"SCE-DataScience-FinalProject/1.0 (contact: student@example.com)\"}\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=0.8, status_forcelist=[403,429,500,502,503,504])\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def api_get(params):\n",
    "    \"\"\"Thin wrapper around MediaWiki API GET with a fallback UA on 403.\"\"\"\n",
    "    r = session.get(BASE_API, params=params, headers=HEADERS, timeout=30)\n",
    "    if r.status_code == 403:\n",
    "        time.sleep(1)\n",
    "        r = session.get(BASE_API, params=params, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def get_category_members(category_title, cmtype=\"page\"):\n",
    "    \"\"\"\n",
    "    Fetches up to all members of a category, paging through 'continue'.\n",
    "    cmtype can be \"page\" (actual pages) or \"subcat\" (subcategories).\n",
    "    \"\"\"\n",
    "    members, params = [], {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": category_title,\n",
    "        \"cmlimit\": \"500\",\n",
    "        \"cmtype\": cmtype,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    while True:\n",
    "        data = api_get(params)\n",
    "        members.extend(data.get(\"query\", {}).get(\"categorymembers\", []))\n",
    "        if \"continue\" in data:\n",
    "            params[\"cmcontinue\"] = data[\"continue\"][\"cmcontinue\"]\n",
    "        else:\n",
    "            break\n",
    "    return members\n",
    "\n",
    "def collect_people_from_category(root_category):\n",
    "    \"\"\"\n",
    "    Breadth-first traversal from a root category:\n",
    "      - collects 'page' members as rows {Name, Wikipedia_Link}\n",
    "      - enqueues 'subcat' members for further traversal\n",
    "    Returns a deduplicated DataFrame by Name.\n",
    "    \"\"\"\n",
    "    seen, pages, queue = set(), [], [root_category]\n",
    "    while queue:\n",
    "        cat = queue.pop(0)\n",
    "        if cat in seen:\n",
    "            continue\n",
    "        seen.add(cat)\n",
    "        print(f\"ğŸ“‚ ×¡×•×¨×§: {cat}\")\n",
    "        # pages\n",
    "        for m in get_category_members(cat, \"page\"):\n",
    "            title = m[\"title\"]\n",
    "            link  = \"https://en.wikipedia.org/wiki/\" + quote(title.replace(\" \", \"_\"))\n",
    "            pages.append({\"Name\": title, \"Wikipedia_Link\": link})\n",
    "        # subcategories\n",
    "        for sc in get_category_members(cat, \"subcat\"):\n",
    "            queue.append(sc[\"title\"])\n",
    "        time.sleep(0.15)  # be polite\n",
    "    return pd.DataFrame(pages).drop_duplicates(subset=[\"Name\"]).reset_index(drop=True)\n",
    "\n",
    "def safe_slug(cat):\n",
    "    \"\"\"\n",
    "    Converts a category title to a folder/filename-safe slug.\n",
    "    Example:\n",
    "      \"Category:Iranian physicians\" -> \"iranian_physicians\"\n",
    "    \"\"\"\n",
    "    name = cat.replace(\"Category:\", \"\").strip()\n",
    "    name = name.replace(\" \", \"_\")\n",
    "    for bad in ['\"', \"'\", \"'\", \"\"\", \"\"\", \"(\", \")\", \"/\", \"\\\\\", \":\", \"*\", \"?\", \"<\", \">\", \"|\", \",\", \";\", \"â€”\", \"â€“\"]:\n",
    "        name = name.replace(bad, \"\")\n",
    "    # compress consecutive underscores\n",
    "    while \"__\" in name:\n",
    "        name = name.replace(\"__\", \"_\")\n",
    "    return name.lower()\n",
    "\n",
    "# ---------------- Categories to collect (expanded incl. healthcare) ----------------\n",
    "categories = [\n",
    "    # Existing:\n",
    "    \"Category:Government_ministers_of_Iran\",\n",
    "    \"Category:Presidents_of_Iran\",\n",
    "    \"Category:Vice_presidents_of_Iran\",\n",
    "    \"Category:Iranian_ayatollahs\",\n",
    "    \"Category:Iranian_actors\",\n",
    "    \"Category:Iranian_singers\",\n",
    "    \"Category:Iranian_scientists\",\n",
    "    \"Category:Iranian_economists\",\n",
    "    \"Category:Iranian_writers\",\n",
    "    \"Category:Iranian_football_managers\",\n",
    "\n",
    "    # NEW â€” Healthcare related:\n",
    "    \"Category:Hospitals_in_Iran\",\n",
    "    \"Category:Private_hospitals_in_Iran\",\n",
    "    \"Category:Teaching_hospitals_in_Iran\",\n",
    "    \"Category:Iranian_physicians\",\n",
    "    \"Category:Iranian_cardiologists\",\n",
    "    \"Category:Iranian_women_physicians\",\n",
    "    \"Category:21st-century_Iranian_physicians\",\n",
    "    \"Category:19th-century_Iranian_physicians\",\n",
    "    \"Category:Medical_and_health_organisations_based_in_Iran\",\n",
    "    \"Category:Healthcare_in_Iran\",\n",
    "    \"Category:Medicine_in_Iran\",\n",
    "]\n",
    "\n",
    "# ---------------- Main loop: collect, save per-spec, summarize ----------------\n",
    "csv_files = []\n",
    "summary_rows = []\n",
    "\n",
    "for cat in categories:\n",
    "    try:\n",
    "        df = collect_people_from_category(cat)\n",
    "        n = len(df)\n",
    "        slug = safe_slug(cat)\n",
    "        cat_dir = os.path.join(POIS_DIR, slug)\n",
    "        os.makedirs(cat_dir, exist_ok=True)\n",
    "\n",
    "        if n == 0:\n",
    "            print(f\"âš ï¸ ××™×Ÿ × ×ª×•× ×™× ×¢×‘×•×¨ {cat}\")\n",
    "            summary_rows.append({\"Category\": cat, \"Slug\": slug, \"SavedRows\": 0, \"CSV\": None})\n",
    "            continue\n",
    "\n",
    "        csv_name = f\"{slug}_wikipedia.csv\"\n",
    "        csv_path = os.path.join(cat_dir, csv_name)\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        csv_files.append(csv_path)\n",
    "\n",
    "        print(f\"âœ… × ×©××¨: {csv_path} | ×¨×©×•××•×ª: {n}\")\n",
    "        summary_rows.append({\"Category\": cat, \"Slug\": slug, \"SavedRows\": n, \"CSV\": csv_path})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘×§×˜×’×•×¨×™×” {cat}: {e}\")\n",
    "        summary_rows.append({\"Category\": cat, \"Slug\": safe_slug(cat), \"SavedRows\": 0, \"CSV\": None})\n",
    "\n",
    "# --- Summary table ---\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"SavedRows\", ascending=False).reset_index(drop=True)\n",
    "total_saved = summary_df[\"SavedRows\"].sum()\n",
    "\n",
    "print(\"\\n================= Summary =================\")\n",
    "for _, r in summary_df.iterrows():\n",
    "    base = os.path.basename(r['CSV']) if (pd.notna(r['CSV']) and r['CSV']) else \"-\"\n",
    "    print(f\"â€¢ {r['Category']} -> {r['Slug']}: {r['SavedRows']} ×¨×©×•××•×ª  |  ×§×•×‘×¥: {base}\")\n",
    "print(f\"\\nğŸ“Š Total across files: {total_saved} rows\")\n",
    "print(\"===========================================\\n\")\n",
    "\n",
    "# Display summary table\n",
    "display(summary_df)\n",
    "\n",
    "# ---------------- ZIP all CSVs ----------------\n",
    "if csv_files:\n",
    "    zip_path = os.path.join(IRAN_DIR, \"Iran_POIs_Wikipedia_Categories.zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file in csv_files:\n",
    "            # Save relative path in zip\n",
    "            arcname = os.path.relpath(file, start=IRAN_DIR)\n",
    "            zipf.write(file, arcname)\n",
    "    print(f\"ğŸ’¾ Created ZIP with all categories: {zip_path}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No CSV files were created â†’ no ZIP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImWc_TSkWUSQ"
   },
   "source": [
    "# **Step 3: Wikidata Enrichment for ALL POI Folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEId6T_TvQT7"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 3: Wikidata Enrichment for ALL POI Folders (UPDATED)\n",
    "# ============================================\n",
    "# For each subfolder under POIs:\n",
    "#  1) Locate *_wikipedia.csv (or the first .csv)\n",
    "#  2) Detect the Wikipedia link column (fallback to Name)\n",
    "#  3) Resolve wikidata_qid (+ wikidata_url)\n",
    "#  4) Fetch details via SPARQL in batches\n",
    "#  5) Save:\n",
    "#     a) *_with_wikidata_ids_and_links.csv\n",
    "#     b) *_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
    "# ============================================\n",
    "\n",
    "import os, re, time, glob, json, math, random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from google.colab import drive\n",
    "\n",
    "# ---------------- Drive mount & ROOT ----------------\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "def find_shared_folder(folder_name: str):\n",
    "    base_path = '/content/drive'\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for d in dirs:\n",
    "            if d.lower() == folder_name.lower():\n",
    "                return os.path.join(root, d)\n",
    "    return None\n",
    "\n",
    "BASE = find_shared_folder('Iran')\n",
    "assert BASE is not None, \"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran' ×‘×“×¨×™×™×‘. ×•×“× ×©×”×•×¡×¤×ª ××•×ª×” ×œ-MyDrive/Shared Drive.\"\n",
    "ROOT_DIR = os.path.join(BASE, \"POIs\")\n",
    "os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "print(f\"ğŸ“ ROOT_DIR: {ROOT_DIR}\")\n",
    "\n",
    "# ---------------- HTTP / Endpoints ----------------\n",
    "UA = {\"User-Agent\": \"SCE-DS-FinalProject/1.0 (contact: student@example.com)\"}\n",
    "MEDIAWIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "SPARQL = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# ---------------- Helpers: retries ----------------\n",
    "def http_get(url, params=None, headers=None, timeout=30, max_tries=4, sleep_base=0.8):\n",
    "    \"\"\"\n",
    "    Simple GET with retries/backoff. Jitters a bit to be polite.\n",
    "    \"\"\"\n",
    "    headers = headers or UA\n",
    "    for attempt in range(1, max_tries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "            if r.status_code == 403 and headers is UA:\n",
    "                # fallback UA\n",
    "                r = requests.get(url, params=params, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            if attempt == max_tries:\n",
    "                # last attempt: raise\n",
    "                raise\n",
    "            sleep_s = sleep_base * attempt + random.uniform(0, 0.3)\n",
    "            time.sleep(sleep_s)\n",
    "    # Shouldn't reach here\n",
    "    raise RuntimeError(\"GET retries exhausted\")\n",
    "\n",
    "# ---------------- Column detection ----------------\n",
    "def find_wikipedia_column(df: pd.DataFrame) -> str:\n",
    "    # First pass: headers with hints\n",
    "    for name in df.columns:\n",
    "        n = str(name).strip().lower()\n",
    "        if any(k in n for k in [\"wikipedia\", \"wiki\", \"link\", \"url\"]):\n",
    "            return name\n",
    "    # Second pass: sample content\n",
    "    for name in df.columns:\n",
    "        sample = \" \".join(map(str, df[name].dropna().astype(str).head(20).tolist())).lower()\n",
    "        if \"wikipedia.org\" in sample or sample.startswith(\"http\"):\n",
    "            return name\n",
    "    # Fallback\n",
    "    if \"Wikipedia_Link\" in df.columns:\n",
    "        return \"Wikipedia_Link\"\n",
    "    # As a last resort: if Name exists, we'll construct enwiki URLs from it\n",
    "    if \"Name\" in df.columns:\n",
    "        return None  # signal to construct from Name\n",
    "    raise ValueError(\"×œ× ××¦××ª×™ ×¢××•×“×ª ×§×™×©×•×¨ ×œ×•×•×™×§×™×¤×“×™×”, ×•××™×Ÿ ×’× ×¢××•×“×ª 'Name' ×œ×‘× ×™×™×ª ×§×™×©×•×¨×™×.\")\n",
    "\n",
    "def name_to_enwiki_url(name: str) -> str:\n",
    "    from urllib.parse import quote\n",
    "    if not isinstance(name, str) or not name.strip():\n",
    "        return None\n",
    "    title = name.strip().replace(\" \", \"_\")\n",
    "    return f\"https://en.wikipedia.org/wiki/{quote(title)}\"\n",
    "\n",
    "def wikipedia_url_to_title(url: str) -> str | None:\n",
    "    if not isinstance(url, str) or not url:\n",
    "        return None\n",
    "    try:\n",
    "        url = url.split(\"?\")[0].split(\"#\")[0]\n",
    "        title = url.rstrip(\"/\").split(\"/\")[-1]\n",
    "        return title if title else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------- QID resolution (with cache) ----------------\n",
    "qid_cache = {}\n",
    "\n",
    "def get_qid_from_wikipedia_url(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Resolve QID for a single Wikipedia page using the pageprops endpoint.\n",
    "    \"\"\"\n",
    "    title = wikipedia_url_to_title(url)\n",
    "    if not title:\n",
    "        return None\n",
    "    if title in qid_cache:\n",
    "        return qid_cache[title]\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"pageprops\",\n",
    "        \"redirects\": 1,\n",
    "        \"titles\": title,\n",
    "    }\n",
    "    try:\n",
    "        r = http_get(MEDIAWIKI_API, params=params, headers=UA, timeout=25)\n",
    "        data = r.json()\n",
    "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "        qid = page.get(\"pageprops\", {}).get(\"wikibase_item\")\n",
    "        if qid:\n",
    "            qid_cache[title] = qid\n",
    "        return qid\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------- SPARQL batch fetch ----------------\n",
    "def batch_fetch_wikidata_details(qids: list[str], batch_size: int = 50) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch properties for many QIDs via SPARQL using VALUES batching.\n",
    "    Returns dict: { QID: {fields...}, ... }\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # Normalize & unique\n",
    "    qids = [q for q in qids if isinstance(q, str) and q.strip()]\n",
    "    uniq = sorted(set(qids))\n",
    "    if not uniq:\n",
    "        return results\n",
    "\n",
    "    def run_batch(subset):\n",
    "        values = \" \".join(f\"wd:{q}\" for q in subset)\n",
    "        query = f\"\"\"\n",
    "        SELECT ?item ?genderLabel ?occupationLabel ?countryLabel ?placeOfBirthLabel ?dateOfBirth WHERE {{\n",
    "          VALUES ?item {{ {values} }}\n",
    "          OPTIONAL {{ ?item wdt:P21 ?gender. }}\n",
    "          OPTIONAL {{ ?item wdt:P106 ?occupation. }}\n",
    "          OPTIONAL {{ ?item wdt:P27 ?country. }}\n",
    "          OPTIONAL {{ ?item wdt:P19 ?placeOfBirth. }}\n",
    "          OPTIONAL {{ ?item wdt:P569 ?dateOfBirth. }}\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en,fa,en-gb\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        r = http_get(SPARQL, params={\"query\": query, \"format\": \"json\"}, headers=UA, timeout=45)\n",
    "        rows = r.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "        # collect multiple rows per item\n",
    "        tmp = {}\n",
    "        for b in rows:\n",
    "            uri = b.get(\"item\", {}).get(\"value\", \"\")\n",
    "            q = uri.rsplit(\"/\", 1)[-1] if uri else None\n",
    "            if not q:\n",
    "                continue\n",
    "            cur = tmp.setdefault(q, {\"wikidata_gender\": set(),\n",
    "                                     \"wikidata_occupation\": set(),\n",
    "                                     \"wikidata_country_of_citizenship\": set(),\n",
    "                                     \"wikidata_place_of_birth\": None,\n",
    "                                     \"wikidata_date_of_birth\": None})\n",
    "            if \"genderLabel\" in b:\n",
    "                cur[\"wikidata_gender\"].add(b[\"genderLabel\"][\"value\"])\n",
    "            if \"occupationLabel\" in b:\n",
    "                cur[\"wikidata_occupation\"].add(b[\"occupationLabel\"][\"value\"])\n",
    "            if \"countryLabel\" in b:\n",
    "                cur[\"wikidata_country_of_citizenship\"].add(b[\"countryLabel\"][\"value\"])\n",
    "            if \"placeOfBirthLabel\" in b and not cur[\"wikidata_place_of_birth\"]:\n",
    "                cur[\"wikidata_place_of_birth\"] = b[\"placeOfBirthLabel\"][\"value\"]\n",
    "            if \"dateOfBirth\" in b and not cur[\"wikidata_date_of_birth\"]:\n",
    "                cur[\"wikidata_date_of_birth\"] = b[\"dateOfBirth\"][\"value\"]\n",
    "        # flatten sets\n",
    "        for q, d in tmp.items():\n",
    "            results[q] = {\n",
    "                \"wikidata_gender\": \"; \".join(sorted(d[\"wikidata_gender\"])) or None,\n",
    "                \"wikidata_occupation\": \"; \".join(sorted(d[\"wikidata_occupation\"])) or None,\n",
    "                \"wikidata_country_of_citizenship\": \"; \".join(sorted(d[\"wikidata_country_of_citizenship\"])) or None,\n",
    "                \"wikidata_place_of_birth\": d[\"wikidata_place_of_birth\"],\n",
    "                \"wikidata_date_of_birth\": d[\"wikidata_date_of_birth\"],\n",
    "            }\n",
    "\n",
    "    for i in range(0, len(uniq), batch_size):\n",
    "        subset = uniq[i:i+batch_size]\n",
    "        # polite pacing\n",
    "        try:\n",
    "            run_batch(subset)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ SPARQL batch failed ({subset[0]}..): {e}. ×× ×¡×” ×©×•×‘ ×‘×§×‘×•×¦×•×ª ×§×˜× ×•×ª ×™×•×ª×¨...\")\n",
    "            # fallback: try half batch to circumvent transient errors\n",
    "            mid = len(subset)//2 or 1\n",
    "            for chunk in (subset[:mid], subset[mid:]):\n",
    "                try:\n",
    "                    run_batch(chunk)\n",
    "                except Exception as ee:\n",
    "                    print(f\"âŒ SPARQL sub-batch failed ({chunk[0]}..): {ee}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ---------------- Main processing ----------------\n",
    "folders = [f for f in sorted(os.listdir(ROOT_DIR)) if os.path.isdir(os.path.join(ROOT_DIR, f))]\n",
    "print(f\"ğŸ—‚ï¸ × ××¦××• {len(folders)} ×ª×™×§×™×•×ª POI ×œ×¢×™×‘×•×“.\")\n",
    "\n",
    "for folder in folders:\n",
    "    FOLDER_PATH = os.path.join(ROOT_DIR, folder)\n",
    "    print(f\"\\nğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: {folder}\")\n",
    "\n",
    "    # Locate input CSV (prefer *_wikipedia.csv)\n",
    "    candidates = sorted(glob.glob(os.path.join(FOLDER_PATH, \"*_wikipedia.csv\")))\n",
    "    if not candidates:\n",
    "        candidates = sorted(glob.glob(os.path.join(FOLDER_PATH, \"*.csv\")))\n",
    "    if not candidates:\n",
    "        print(\"âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\")\n",
    "        continue\n",
    "\n",
    "    INPUT_CSV = candidates[0]\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×œ× ×”×¦×œ×—×ª×™ ×œ×§×¨×•× ××ª ×”×§×•×‘×¥ {os.path.basename(INPUT_CSV)}: {e} â€” ××“×œ×’.\")\n",
    "        continue\n",
    "\n",
    "    # Detect / construct Wikipedia links\n",
    "    try:\n",
    "        wiki_col = find_wikipedia_column(df)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ {e} â€” ×× ×¡×” ×œ×‘× ×•×ª ×§×™×©×•×¨×™ ×•×™×§×™×¤×“×™×” ×Ö¾'Name'...\")\n",
    "        wiki_col = None\n",
    "\n",
    "    if wiki_col is None:\n",
    "        if \"Name\" in df.columns:\n",
    "            df[\"Wikipedia_Link\"] = df[\"Name\"].apply(name_to_enwiki_url)\n",
    "            wiki_col = \"Wikipedia_Link\"\n",
    "        else:\n",
    "            print(\"âŒ ××™×Ÿ ×¢××•×“×ª ×§×™×©×•×¨/Name â€” ××“×œ×’ ×¢×œ ×”×ª×™×§×™×™×”.\")\n",
    "            continue\n",
    "\n",
    "    print(\"ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”:\", wiki_col)\n",
    "\n",
    "    # ---- A) Resolve QIDs (with simple in-memory cache) ----\n",
    "    qids = []\n",
    "    for url in df[wiki_col].fillna(\"\"):\n",
    "        qids.append(get_qid_from_wikipedia_url(url))\n",
    "        time.sleep(0.08)  # polite throttle\n",
    "\n",
    "    df[\"wikidata_qid\"] = qids\n",
    "    df[\"wikidata_url\"] = df[\"wikidata_qid\"].apply(lambda q: f\"https://www.wikidata.org/wiki/{q}\" if isinstance(q, str) and q else None)\n",
    "\n",
    "    # Save mid file\n",
    "    mid_path = os.path.join(\n",
    "        FOLDER_PATH,\n",
    "        re.sub(r\"\\.csv$\", \"\", os.path.basename(INPUT_CSV)) + \"_with_wikidata_ids_and_links.csv\"\n",
    "    )\n",
    "    try:\n",
    "        df.to_csv(mid_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: {os.path.basename(mid_path)}  (×©×•×¨×•×ª: {len(df)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘×©××™×¨×ª ×§×•×‘×¥ ×‘×™× ×™×™×: {e}\")\n",
    "\n",
    "    # ---- B) Fetch details via SPARQL in batches ----\n",
    "    uniq_qids = [q for q in pd.Series(df[\"wikidata_qid\"]).dropna().astype(str).unique().tolist() if q]\n",
    "    details_map = batch_fetch_wikidata_details(uniq_qids, batch_size=50)\n",
    "\n",
    "    details_rows = []\n",
    "    for q in df[\"wikidata_qid\"]:\n",
    "        if isinstance(q, str) and q in details_map:\n",
    "            details_rows.append(details_map[q])\n",
    "        else:\n",
    "            details_rows.append({\n",
    "                \"wikidata_gender\": None,\n",
    "                \"wikidata_occupation\": None,\n",
    "                \"wikidata_country_of_citizenship\": None,\n",
    "                \"wikidata_place_of_birth\": None,\n",
    "                \"wikidata_date_of_birth\": None,\n",
    "            })\n",
    "\n",
    "    details_df = pd.DataFrame(details_rows)\n",
    "    enriched = pd.concat([df, details_df], axis=1)\n",
    "\n",
    "    out_path = os.path.join(\n",
    "        FOLDER_PATH,\n",
    "        re.sub(r\"\\.csv$\", \"\", os.path.basename(INPUT_CSV)) + \"_with_wikidata_ids_and_links_wikidata_detailed.csv\"\n",
    "    )\n",
    "    try:\n",
    "        enriched.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: {os.path.basename(out_path)}  (×©×•×¨×•×ª: {len(enriched)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘×©××™×¨×ª ×”×¤×œ×˜ ×”×¡×•×¤×™: {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ ×”×•×©×œ× ×¢×™×‘×•×“ ×œ×›×œ ×”×ª×™×§×™×•×ª!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv2SvQhGWtAI"
   },
   "source": [
    "# Step 4: Find Twitter handles for ALL POI folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsVDW3ZmG45R"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 4: Find Twitter handles for ALL POI folders\n",
    "# Adds only: Twitter_username + Twitter_url (no twitter_source)\n",
    "# ============================================\n",
    "\n",
    "from google.colab import drive\n",
    "import os, re, time, json, glob, requests, pandas as pd, random\n",
    "from urllib.parse import urlparse, unquote\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ---- DRIVE MOUNT ----\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# ---- Locate shared 'Iran' folder dynamically ----\n",
    "def find_shared_folder(folder_name='Iran'):\n",
    "    base_path = '/content/drive'\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for d in dirs:\n",
    "            if d.lower() == folder_name.lower():\n",
    "                return os.path.join(root, d)\n",
    "    return None\n",
    "\n",
    "# ××¤×©×¨×•×ª ×œ×¢×§×•×£ ×™×“× ×™×ª ×¢× ××©×ª× ×” ×¡×‘×™×‘×” (×œ××™ ×©×¨×•×¦×”):\n",
    "BASE = os.environ.get('IRAN_DIR')\n",
    "if not BASE or not os.path.isdir(BASE):\n",
    "    BASE = find_shared_folder('Iran')\n",
    "\n",
    "assert BASE and os.path.isdir(BASE), (\n",
    "    \"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. ×•×“× ×©×”×•×¡×¤×ª ××•×ª×” ×œ-MyDrive/Shared Drive ×©×œ×š. \"\n",
    "    \"× ×™×ª×Ÿ ×’× ×œ×”×’×“×™×¨ ×™×“× ×™×ª: os.environ['IRAN_DIR'] = '/content/drive/â€¦/Iran'\"\n",
    ")\n",
    "\n",
    "# ×™×¨×•×¥ ×¢×œ ×›×œ ×”×ª×™×§×™×•×ª ×ª×—×ª POIs\n",
    "REL_ROOT = \"POIs\"\n",
    "ROOT = os.path.join(BASE, REL_ROOT)\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# ---------- HTTP session with retries ----------\n",
    "UA = {\"User-Agent\": \"SCE-DS-FinalProject/1.0 (contact: student@example.com)\"}\n",
    "session = requests.Session()\n",
    "session.headers.update(UA)\n",
    "\n",
    "# urllib3 v2: allowed_methods (not method_whitelist). ×¢×“×™×£ frozenset\n",
    "retry_cfg = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=frozenset({\"GET\"}),\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retry_cfg))\n",
    "session.mount(\"http://\", HTTPAdapter(max_retries=retry_cfg))\n",
    "\n",
    "SPARQL = \"https://query.wikidata.org/sparql\"\n",
    "MEDIAWIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def pick_csv(folder):\n",
    "    for pat in [\"*_with_wikidata_ids_and_links_wikidata_detailed*.csv\",\n",
    "                \"*_with_wikidata_ids_and_links*.csv\",\n",
    "                \"*_wikipedia.csv\",\n",
    "                \"*.csv\"]:\n",
    "        cand = sorted(glob.glob(os.path.join(folder, pat)))\n",
    "        if cand: return cand[0]\n",
    "    return None\n",
    "\n",
    "def clean_handle(h):\n",
    "    if not isinstance(h, str):\n",
    "        return None\n",
    "    h = h.strip().lstrip(\"@\")\n",
    "    h = re.sub(r\"[/?#].*$\", \"\", h)\n",
    "    m = re.match(r\"^[A-Za-z0-9_]{1,15}$\", h)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def extract_handle_from_url(url):\n",
    "    \"\"\"\n",
    "    ××—×œ×¥ ×™×“×™×ª ××›×ª×•×‘×ª twitter/x ×× ×§×™×™××ª. ×œ×™× ×§×™× ××¡×•×’ /i/user/12345 ×œ× ××›×™×œ×™× ×™×“×™×ª => × ×—×–×™×¨ None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        u = url.split(\"?\")[0].split(\"#\")[0]\n",
    "        host = urlparse(u).netloc.lower()\n",
    "        if any(d in host for d in [\"twitter.com\",\"x.com\",\"mobile.twitter.com\",\"www.twitter.com\",\"www.x.com\"]):\n",
    "            parts = urlparse(u).path.strip(\"/\").split(\"/\")\n",
    "            if parts and parts[0].lower() not in {\"i\",\"intent\",\"share\",\"home\"}:\n",
    "                return clean_handle(parts[0])\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def build_twitter_url(handle):\n",
    "    if not isinstance(handle, str) or not handle.strip():\n",
    "        return None\n",
    "    return f\"https://x.com/{handle.strip().lstrip('@')}\"\n",
    "\n",
    "def find_wikipedia_column(df):\n",
    "    # ×¨××©×™×ª: ×œ×¤×™ ×©× ×¢××•×“×”\n",
    "    for c in df.columns:\n",
    "        n = str(c).lower()\n",
    "        if any(k in n for k in [\"wikipedia\", \"wiki\", \"link\", \"url\"]):\n",
    "            return c\n",
    "    # ×©× ×™×ª: ×œ×¤×™ ×ª×•×›×Ÿ\n",
    "    for c in df.columns:\n",
    "        vals = \" \".join(map(str, df[c].dropna().astype(str).head(15).tolist())).lower()\n",
    "        if \"wikipedia.org\" in vals or \"https://\" in vals or \"http://\" in vals:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def http_get(url, params=None, headers=None, timeout=30, tries=3, backoff=0.7):\n",
    "    headers = headers or UA\n",
    "    for attempt in range(1, tries+1):\n",
    "        try:\n",
    "            r = session.get(url, params=params, headers=headers, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            if attempt == tries:\n",
    "                raise\n",
    "            time.sleep(backoff * attempt + random.uniform(0, 0.2))\n",
    "\n",
    "def guess_twitter_from_wiki(title_or_url):\n",
    "    \"\"\"\n",
    "    × ×¡×” ×œ×—×œ×¥ ×™×“×™×ª ××”×“×£ ×‘×•×•×™×§×™×¤×“×™×”:\n",
    "    - External links\n",
    "    - ×˜×§×¡×˜ ×”××§×•×¨ (wikitext) ×¢× regex\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(title_or_url, str) and title_or_url.startswith((\"http://\", \"https://\")):\n",
    "            title = unquote(title_or_url.split(\"?\")[0].split(\"#\")[0].rstrip(\"/\").split(\"/\")[-1])\n",
    "            title = title.replace(\"_\", \" \")\n",
    "        else:\n",
    "            title = str(title_or_url).strip().replace(\"_\", \" \")\n",
    "        if not title:\n",
    "            return None\n",
    "\n",
    "        r = http_get(MEDIAWIKI_API, params={\n",
    "            \"action\": \"parse\",\n",
    "            \"format\": \"json\",\n",
    "            \"page\": title,\n",
    "            \"prop\": \"externallinks|wikitext\",\n",
    "            \"redirects\": 1\n",
    "        }, timeout=30, tries=3)\n",
    "        data = r.json()\n",
    "\n",
    "        links = data.get(\"parse\", {}).get(\"externallinks\", []) or []\n",
    "        for ln in links:\n",
    "            h = extract_handle_from_url(ln)\n",
    "            if h:\n",
    "                return h\n",
    "\n",
    "        wt = data.get(\"parse\", {}).get(\"wikitext\", {}).get(\"*\", \"\")\n",
    "        # ×ª×•×¤×¡ ×’× twitter ×•×’× x.com\n",
    "        for m in re.finditer(r\"(?:https?://)?(?:www\\.)?(?:twitter|x)\\.com/([A-Za-z0-9_]{1,15})\", wt, re.I):\n",
    "            h = clean_handle(m.group(1))\n",
    "            if h:\n",
    "                return h\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def batch_p2002(qids):\n",
    "    \"\"\"\n",
    "    ××—×–×™×¨ ××¤×” { QID: handle or None } ×¢× retry.\n",
    "    \"\"\"\n",
    "    qids = [q for q in qids if isinstance(q, str) and q]\n",
    "    if not qids:\n",
    "        return {}\n",
    "    values = \" \".join(f\"(wd:{q})\" for q in qids)\n",
    "    q = f\"\"\"\n",
    "    SELECT ?item ?twitter WHERE {{\n",
    "      VALUES (?item) {{ {values} }}\n",
    "      OPTIONAL {{ ?item wdt:P2002 ?twitter. }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    # retry ×“×¨×š http_get\n",
    "    r = http_get(SPARQL, params={\"query\": q, \"format\": \"json\"}, timeout=45, tries=3, backoff=0.9)\n",
    "    rows = r.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "    out = {}\n",
    "    for b in rows:\n",
    "        qid = b[\"item\"][\"value\"].rsplit(\"/\",1)[-1]\n",
    "        tw = b.get(\"twitter\", {}).get(\"value\")\n",
    "        out[qid] = clean_handle(tw) if tw else None\n",
    "    return out\n",
    "\n",
    "# cache (QID -> handle)\n",
    "cache_file = os.path.join(ROOT, \"_twitter_cache.json\")\n",
    "twitter_cache = {}\n",
    "if os.path.exists(cache_file):\n",
    "    try:\n",
    "        twitter_cache = json.load(open(cache_file, \"r\", encoding=\"utf-8\"))\n",
    "    except:\n",
    "        twitter_cache = {}\n",
    "\n",
    "# ---------- process all subfolders ----------\n",
    "folders = [os.path.join(ROOT, d) for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))]\n",
    "folders.sort()\n",
    "\n",
    "for FOLDER in folders:\n",
    "    in_csv = pick_csv(FOLDER)\n",
    "    if not in_csv:\n",
    "        print(f\"âš ï¸ ××™×Ÿ CSV ×‘×ª×™×§×™×™×” {os.path.basename(FOLDER)} â€” ×“×™×œ×•×’\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ“‚ {os.path.basename(FOLDER)}\")\n",
    "    try:\n",
    "        df = pd.read_csv(in_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×œ× ×”×¦×œ×—×ª×™ ×œ×§×¨×•× ××ª ×”×§×•×‘×¥: {e} â€” ×“×™×œ×•×’\")\n",
    "        continue\n",
    "\n",
    "    wiki_col = find_wikipedia_column(df)\n",
    "    if \"wikidata_qid\" not in df.columns:\n",
    "        df[\"wikidata_qid\"] = None\n",
    "\n",
    "    if \"Twitter_username\" not in df.columns:\n",
    "        df[\"Twitter_username\"] = None\n",
    "\n",
    "    # 1) Wikidata P2002 (with cache)\n",
    "    qids = [q for q in df[\"wikidata_qid\"].dropna().astype(str).unique() if q]\n",
    "    p2002_map = {}\n",
    "    # ×§×•×“× ××”-cache\n",
    "    for q in qids:\n",
    "        if q in twitter_cache:\n",
    "            p2002_map[q] = twitter_cache[q]\n",
    "    # ××” ×©×—×¡×¨ â€” ×œ×©××™×œ×ª× (×‘×§×‘×•×¦×•×ª)\n",
    "    to_query = [q for q in qids if q not in p2002_map]\n",
    "    for i in range(0, len(to_query), 60):\n",
    "        part = to_query[i:i+60]\n",
    "        try:\n",
    "            m = batch_p2002(part)\n",
    "            p2002_map.update(m)\n",
    "            # ×œ×¢×“×›×Ÿ cache ×¨×§ ×›×©×™×© ×™×“×™×ª (×›×“×™ ×œ× â€œ×œ×§×‘×¢â€ None)\n",
    "            twitter_cache.update({k: v for k, v in m.items() if v})\n",
    "            time.sleep(0.25)\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ SPARQL batch failed:\", e)\n",
    "\n",
    "    # ×›×ª×™×‘×” ×œ×¤×™ QID\n",
    "    for idx, row in df.iterrows():\n",
    "        qid = row.get(\"wikidata_qid\")\n",
    "        if isinstance(qid, str) and qid in p2002_map and p2002_map[qid]:\n",
    "            if not df.at[idx, \"Twitter_username\"]:\n",
    "                df.at[idx, \"Twitter_username\"] = p2002_map[qid]\n",
    "\n",
    "    # 2) Wikipedia fallback\n",
    "    if wiki_col:\n",
    "        missing = df[\"Twitter_username\"].isna()\n",
    "        for idx, row in df[missing].iterrows():\n",
    "            url = row[wiki_col]\n",
    "            if isinstance(url, str) and url.strip():\n",
    "                h = guess_twitter_from_wiki(url)\n",
    "                if h:\n",
    "                    df.at[idx, \"Twitter_username\"] = h\n",
    "                    # ×× ×™×© ×’× QID â€” ×¢×“×›×Ÿ cache (× ×—×¡×•×š ×‘×”×¤×¢×œ×•×ª ×¢×ª×™×“×™×•×ª)\n",
    "                    qid = row.get(\"wikidata_qid\")\n",
    "                    if isinstance(qid, str) and qid:\n",
    "                        twitter_cache[qid] = h\n",
    "\n",
    "    # × ×™×§×•×™ + URL\n",
    "    df[\"Twitter_username\"] = df[\"Twitter_username\"].apply(lambda h: clean_handle(h) if isinstance(h, str) else None)\n",
    "    df[\"Twitter_url\"] = df[\"Twitter_username\"].apply(build_twitter_url)\n",
    "\n",
    "    # save (UTF-8-SIG to be Excel-friendly)\n",
    "    out_path = os.path.join(FOLDER, os.path.splitext(os.path.basename(in_csv))[0] + \"_with_twitter.csv\")\n",
    "    try:\n",
    "        df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"âœ… Saved: {os.path.basename(out_path)} | found {df['Twitter_username'].notna().sum()} / {len(df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©××™×¨×” × ×›×©×œ×”: {e}\")\n",
    "\n",
    "# save cache\n",
    "try:\n",
    "    with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(twitter_cache, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\nğŸ’¾ cache saved:\", cache_file)\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ couldn't save cache:\", e)\n",
    "\n",
    "print(\"\\nğŸ‰ Done â€” Step 4 completed for all POI folders (username + url only).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLh9s1prXDtv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwD-8mnAmj_z"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Build Manual_Search_POIs.csv from all *_with_twitter.csv\n",
    "# ×××œ× ××•×˜×•××˜×™×ª ××ª ×”×§×•×‘×¥ ×”××¨×›×–×™ ××›×œ ×”×ª×™×§×™×•×ª (××”×¢××•×“×” Twitter_username)\n",
    "# ×‘×”×ª×× ×œ×”× ×—×™×•×ª ×”×§×•×¨×¡: ×©× ×”×§×•×‘×¥ Manual_Search_POIs.csv ×•×©××™×¨×” ×ª×—×ª POIs/\n",
    "# ============================================\n",
    "\n",
    "from google.colab import drive\n",
    "import os, re, glob, pandas as pd\n",
    "\n",
    "# ---- Mount + locate shared 'Iran' folder (dynamic, portable) ----\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "def find_shared_folder(folder_name='Iran'):\n",
    "    base_path = '/content/drive'\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for d in dirs:\n",
    "            if d.lower() == folder_name.lower():\n",
    "                return os.path.join(root, d)\n",
    "    return None\n",
    "\n",
    "BASE = os.environ.get('IRAN_DIR')\n",
    "if not BASE or not os.path.isdir(BASE):\n",
    "    BASE = find_shared_folder('Iran')\n",
    "\n",
    "assert BASE and os.path.isdir(BASE), \"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran' ×‘×“×¨×™×™×‘.\"\n",
    "REL_ROOT = \"POIs\"\n",
    "ROOT = os.path.join(BASE, REL_ROOT)\n",
    "assert os.path.isdir(ROOT), f\"âŒ ×œ× ×§×™×™××ª ×”×ª×™×§×™×™×”: {ROOT}\"\n",
    "print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# ×©× ×”×§×•×‘×¥ ×œ×¤×™ ×”×”× ×—×™×•×ª (×©×œ×‘ 5)\n",
    "OUTPUT_NAME = \"Manual_Search_POIs.csv\"\n",
    "OUTPUT_PATH = os.path.join(ROOT, OUTPUT_NAME)\n",
    "\n",
    "# -------- Keyword map (folder_slug -> keyword) --------\n",
    "# ×”×¢×¨×”: ×”×©××•×ª ×›××Ÿ ×‘-lowercase ×›×™ ×”×ª×™×§×™×•×ª × ×•×¦×¨×• ×¢× slug ×§×˜×Ÿ (safe_slug)\n",
    "KEYWORD_MAP = {\n",
    "    # Existing\n",
    "    \"government_ministers_of_iran\": \"Iran minister\",\n",
    "    \"presidents_of_iran\": \"Iran president\",\n",
    "    \"vice_presidents_of_iran\": \"Iran vice president\",\n",
    "    \"iranian_ayatollahs\": \"Iran ayatollah\",\n",
    "    \"iranian_actors\": \"Iran actor\",\n",
    "    \"iranian_singers\": \"Iran singer\",\n",
    "    \"iranian_scientists\": \"Iran scientist\",\n",
    "    \"iranian_economists\": \"Iran economist\",\n",
    "    \"iranian_writers\": \"Iran writer\",\n",
    "    \"iranian_football_managers\": \"Iran football manager\",\n",
    "\n",
    "    # Healthcare related\n",
    "    \"hospitals_in_iran\": \"Iran hospital\",\n",
    "    \"private_hospitals_in_iran\": \"Iran hospital\",\n",
    "    \"teaching_hospitals_in_iran\": \"Iran teaching hospital\",\n",
    "    \"iranian_physicians\": \"Iran physician\",\n",
    "    \"iranian_cardiologists\": \"Iran cardiologist\",\n",
    "    \"iranian_women_physicians\": \"Iran physician\",\n",
    "    \"21st-century_iranian_physicians\": \"Iran physician\",\n",
    "    \"19th-century_iranian_physicians\": \"Iran physician\",\n",
    "    \"medical_and_health_organisations_based_in_iran\": \"Iran health org\",\n",
    "    \"healthcare_in_iran\": \"Iran healthcare\",\n",
    "    \"medicine_in_iran\": \"Iran medicine\",\n",
    "}\n",
    "\n",
    "CENTURY_PREFIX = re.compile(r\"^\\d{1,2}(st|nd|rd|th)-century_\")\n",
    "\n",
    "def base_slug(folder_slug: str) -> str:\n",
    "    \"\"\"××¡×™×¨ ×§×™×“×•××ª ×©×œ ×××” ××”×¡×œ××’ (e.g., 19th-century_...)\"\"\"\n",
    "    return CENTURY_PREFIX.sub(\"\", folder_slug)\n",
    "\n",
    "def infer_keyword_from_folder(folder_slug: str) -> str:\n",
    "    \"\"\"××¤×™×§ ××™×œ×ª ×—×™×¤×•×© ×”×’×™×•× ×™×ª ××”×¡×œ××’, ×¢× ××¤×” ×™×“× ×™×ª ×œ×§×™×™×¡×™× ×—×©×•×‘×™×.\"\"\"\n",
    "    slug = folder_slug.lower().strip()\n",
    "    if slug in KEYWORD_MAP:\n",
    "        return KEYWORD_MAP[slug]\n",
    "    slug = base_slug(slug)\n",
    "    if slug in KEYWORD_MAP:\n",
    "        return KEYWORD_MAP[slug]\n",
    "    # ×›×œ×œ ×‘×¨×™×¨×ª ××—×“×œ: ×”×•×¨×“ \"iranian_\" ×•×”Ö¾s ×”×¡×•×¤×™×ª, ×•×”×—×œ×£ _ ×‘×¨×•×•×—\n",
    "    token = slug.replace(\"iranian_\", \"\").replace(\"_\", \" \").strip()\n",
    "    token = token[:-1] if token.endswith(\"s\") else token\n",
    "    return f\"Iran {token}\".strip()\n",
    "\n",
    "def detect_name_col(df: pd.DataFrame) -> str:\n",
    "    \"\"\"×××ª×¨ ×¢××•×“×ª ×©× ×¡×‘×™×¨×”.\"\"\"\n",
    "    for c in df.columns:\n",
    "        n = str(c).strip().lower()\n",
    "        if n in {\"name\", \"poi name\", \"poi_name\", \"full_name\", \"full name\"}:\n",
    "            return c\n",
    "    # fallback: ×”×¢××•×“×” ×”×¨××©×•× ×”\n",
    "    return df.columns[0]\n",
    "\n",
    "def detect_twitter_col(df: pd.DataFrame) -> str | None:\n",
    "    \"\"\"×××ª×¨ ×¢××•×“×ª Twitter_username (××• ×“×•××”).\"\"\"\n",
    "    for c in df.columns:\n",
    "        n = str(c).strip().lower()\n",
    "        if n in {\"twitter_username\", \"twitter\", \"handle\", \"username\"}:\n",
    "            return c\n",
    "    # ×—×¤×© ×¢××•×“×” ×©×™×© ×‘×” ×”×¨×‘×” ×¢×¨×›×™× ×©× ×¨××™× ×›××• ×™×“×™×•×ª\n",
    "    for c in df.columns:\n",
    "        vals = df[c].dropna().astype(str).head(20).tolist()\n",
    "        hits = sum(1 for v in vals if re.match(r\"^@?[A-Za-z0-9_]{1,15}$\", v))\n",
    "        if hits >= max(3, len(vals)//3):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def build_twitter_url(handle: str) -> str | None:\n",
    "    if not isinstance(handle, str) or not handle.strip():\n",
    "        return None\n",
    "    h = handle.strip()\n",
    "    h = h[1:] if h.startswith(\"@\") else h\n",
    "    return f\"https://x.com/{h}\"\n",
    "\n",
    "rows = []\n",
    "folders = [d for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))]\n",
    "folders.sort()\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(ROOT, folder)\n",
    "    # ×—×¤×© ××ª ×§×•×‘×¥ ×”×™×¢×“ ×©× ×•×¦×¨ ×‘×©×œ×‘ 4\n",
    "    cands = sorted(glob.glob(os.path.join(folder_path, \"*_with_twitter.csv\")))\n",
    "    if not cands:\n",
    "        continue\n",
    "    path = cands[0]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ can't read:\", path, e)\n",
    "        continue\n",
    "\n",
    "    twitter_col = detect_twitter_col(df)\n",
    "    if not twitter_col:\n",
    "        continue\n",
    "\n",
    "    name_col = detect_name_col(df)\n",
    "    kw = infer_keyword_from_folder(folder)\n",
    "\n",
    "    sub = df[[name_col, twitter_col]].copy()\n",
    "    sub.rename(columns={name_col: \"poi_name\", twitter_col: \"Twitter_username\"}, inplace=True)\n",
    "\n",
    "    # × ×™×§×•×™, ×¡×™× ×•×Ÿ, ×•×”×©×œ××ª URL\n",
    "    sub[\"Twitter_username\"] = sub[\"Twitter_username\"].astype(str).str.strip()\n",
    "    sub = sub[sub[\"Twitter_username\"].str.len() > 0]\n",
    "    sub[\"Twitter_username\"] = sub[\"Twitter_username\"].str.lstrip(\"@\")\n",
    "    sub[\"Twitter_url\"] = sub[\"Twitter_username\"].apply(build_twitter_url)\n",
    "    sub[\"keyword\"] = kw\n",
    "    sub[\"source_folder\"] = folder  # ×××™×–×” ×§×˜×’×•×¨×™×” ×”×’×™×¢\n",
    "\n",
    "    rows.append(sub[[\"keyword\", \"poi_name\", \"Twitter_username\", \"Twitter_url\", \"source_folder\"]])\n",
    "\n",
    "# ××™×—×•×“ ×•×©××™×¨×”\n",
    "if rows:\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    # ×¡×™× ×•×Ÿ ×›×¤×™×œ×•×™×•×ª ×¢×œ ×‘×¡×™×¡ ×©×œ×™×©×™×™×” (×©× ××©×ª××© + ××™×œ×ª ×—×™×¤×•×© + ×©× POI)\n",
    "    out.drop_duplicates(subset=[\"Twitter_username\", \"keyword\", \"poi_name\"], inplace=True, ignore_index=True)\n",
    "    # ×•×¨×™×× ×˜ × ×•×¡×£: ×× ××•×ª×• handle ×”×•×¤×™×¢ ×‘×›××” ×ª×™×§×™×•×ª â€” × ×©××•×¨ ××ª ×”×”×•×¤×¢×” ×”×¨××©×•× ×” ×‘×œ×‘×“\n",
    "    out.sort_values([\"Twitter_username\", \"keyword\", \"poi_name\"], inplace=True, ignore_index=True)\n",
    "    out.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… Saved: {OUTPUT_PATH} | rows: {len(out)}\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(out.head(20))\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"â„¹ï¸ ×œ× × ××¦××• *_with_twitter.csv ×‘×ª×™×§×™×•×ª, ××• ×©××™×Ÿ ×™×“×™×•×ª ×œ××œ×.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFtZ7fiyyIAe"
   },
   "source": [
    "#**Cleaning up suspended users**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYqDtkG_C-gz"
   },
   "outputs": [],
   "source": [
    "# # ============================================\n",
    "# # Update POIs_Search_Manual.csv:\n",
    "# # - Find the correct Twitter handle column (robust name detection)\n",
    "# # - Replace numeric-only handles with \"Suspended\"\n",
    "# # - Rebuild Twitter_url accordingly\n",
    "# # ============================================\n",
    "\n",
    "# import os, re, pandas as pd\n",
    "\n",
    "# BASES = [\n",
    "#     \"/content/drive/MyDrive/Iran\",\n",
    "#     \"/content/drive/MyDrive/data_science_semester_A/Iran\",\n",
    "# ]\n",
    "# REL_ROOT   = \"POIs\"\n",
    "# OUTPUT_NAME = \"POIs_Search_Manual.csv\"\n",
    "\n",
    "# # 1) Locate file\n",
    "# BASE = next((b for b in BASES if os.path.exists(b)), None)\n",
    "# assert BASE, \"Drive base not found. Mount Google Drive first.\"\n",
    "# ROOT = os.path.join(BASE, REL_ROOT)\n",
    "# assert os.path.isdir(ROOT), f\"Folder not found: {ROOT}\"\n",
    "# path = os.path.join(ROOT, OUTPUT_NAME)\n",
    "# assert os.path.exists(path), f\"File not found: {path}\"\n",
    "# print(\"ğŸ“„ Using file:\", path)\n",
    "\n",
    "# # 2) Load\n",
    "# df = pd.read_csv(path)\n",
    "\n",
    "# # 3) Detect the twitter username column robustly (case-insensitive, common variants)\n",
    "# cands = [c for c in df.columns if re.sub(r'\\s+', '', c.strip().lower()) in {\n",
    "#     \"twitter_username\",\"twitteruser\",\"twitter_user\",\"twitterhandle\",\"handle\",\"username\"\n",
    "# }]\n",
    "# if not cands:\n",
    "#     # fallback: try any column containing 'twitter' and 'name'\n",
    "#     cands = [c for c in df.columns if \"twitter\" in c.lower() and \"name\" in c.lower()]\n",
    "# assert cands, f\"Could not find Twitter username column in {list(df.columns)}\"\n",
    "# tw_col = cands[0]\n",
    "# print(\"âœ… Detected username column:\", tw_col)\n",
    "\n",
    "# # 4) Replace numeric-only usernames with \"Suspended\"\n",
    "# before = df[tw_col].astype(str)\n",
    "# df[tw_col] = df[tw_col].apply(lambda v: \"Suspended\" if isinstance(v, str) and re.fullmatch(r\"\\d+\", v.strip() or \"\") else v)\n",
    "# changed_usernames = (before != df[tw_col].astype(str)).sum()\n",
    "\n",
    "# # 5) Rebuild Twitter_url consistently (create if missing)\n",
    "# url_col = \"Twitter_url\" if \"Twitter_url\" in df.columns else None\n",
    "# if url_col is None:\n",
    "#     df[\"Twitter_url\"] = None\n",
    "#     url_col = \"Twitter_url\"\n",
    "\n",
    "# def url_from_handle(h):\n",
    "#     if not isinstance(h, str): return None\n",
    "#     h = h.strip().lstrip(\"@\")\n",
    "#     if h == \"\" or h.lower() == \"suspended\": return None\n",
    "#     return f\"https://x.com/{h}\"\n",
    "\n",
    "# before_urls = df[url_col].copy()\n",
    "# df[url_col] = df[tw_col].apply(url_from_handle)\n",
    "# fixed_urls = (before_urls != df[url_col]).sum()\n",
    "\n",
    "# # 6) Save\n",
    "# df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "# print(f\"âœ… Done. Replaced {changed_usernames} usernames with 'Suspended', updated {fixed_urls} URLs.\")\n",
    "\n",
    "# # Peek\n",
    "# try:\n",
    "#     from IPython.display import display\n",
    "#     display(df.head(15))\n",
    "# except:\n",
    "#     print(df.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SYH1sj55gew"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Build Manual_Search_POIs.csv from all *_with_twitter.csv\n",
    "# ×××œ× ××•×˜×•××˜×™×ª ××ª ×”×§×•×‘×¥ ×”××¨×›×–×™ ××›×œ ×”×ª×™×§×™×•×ª (Twitter_username)\n",
    "# ============================================\n",
    "\n",
    "from google.colab import drive\n",
    "from IPython.display import display\n",
    "import os, glob, re, pandas as pd\n",
    "\n",
    "# ---- Mount Drive ----\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# ---- Locate (or create if missing) shared 'Iran' folder ----\n",
    "def find_shared_folder(folder_name='Iran'):\n",
    "    base_path = '/content/drive'\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for d in dirs:\n",
    "            if d.lower() == folder_name.lower():\n",
    "                return os.path.join(root, d)\n",
    "    return None\n",
    "\n",
    "BASE = find_shared_folder('Iran')\n",
    "if not BASE:\n",
    "    BASE = '/content/drive/MyDrive/Iran'\n",
    "    os.makedirs(BASE, exist_ok=True)\n",
    "    print(\"âš ï¸ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran' ××©×•×ª×¤×ª â€” × ×•×¦×¨×” ×—×“×©×” ××§×•××™×ª:\", BASE)\n",
    "else:\n",
    "    print(\"ğŸ“ × ××¦××” ×ª×™×§×™×™×ª Iran:\", BASE)\n",
    "\n",
    "REL_ROOT    = \"POIs\"\n",
    "ROOT        = os.path.join(BASE, REL_ROOT)\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "OUTPUT_NAME = \"Manual_Search_POIs.csv\"   # ×©× ×œ×¤×™ ×”×”× ×—×™×•×ª\n",
    "OUTPUT_PATH = os.path.join(ROOT, OUTPUT_NAME)\n",
    "\n",
    "# ---- KEYWORD MAP: ×¨×§ ×”×§×˜×’×•×¨×™×•×ª ×©×‘×™×§×©×ª (×œ×¤×™ slug ×‘×ª×™×§×™×•×ª - lowercase) ----\n",
    "KEYWORD_MAP = {\n",
    "    # Existing\n",
    "    \"government_ministers_of_iran\": \"Iran minister\",\n",
    "    \"presidents_of_iran\": \"Iran president\",\n",
    "    \"vice_presidents_of_iran\": \"Iran vice president\",\n",
    "    \"iranian_ayatollahs\": \"Iran ayatollah\",\n",
    "    \"iranian_actors\": \"Iran actor\",\n",
    "    \"iranian_singers\": \"Iran singer\",\n",
    "    \"iranian_scientists\": \"Iran scientist\",\n",
    "    \"iranian_economists\": \"Iran economist\",\n",
    "    \"iranian_writers\": \"Iran writer\",\n",
    "    \"iranian_football_managers\": \"Iran football manager\",\n",
    "\n",
    "    # Healthcare\n",
    "    \"hospitals_in_iran\": \"Iran hospital\",\n",
    "    \"private_hospitals_in_iran\": \"Iran hospital\",\n",
    "    \"teaching_hospitals_in_iran\": \"Iran teaching hospital\",\n",
    "    \"iranian_physicians\": \"Iran physician\",\n",
    "    \"iranian_cardiologists\": \"Iran cardiologist\",\n",
    "    \"iranian_women_physicians\": \"Iran physician\",\n",
    "    \"21st-century_iranian_physicians\": \"Iran physician\",\n",
    "    \"19th-century_iranian_physicians\": \"Iran physician\",\n",
    "    \"medical_and_health_organisations_based_in_iran\": \"Iran health org\",\n",
    "    \"healthcare_in_iran\": \"Iran healthcare\",\n",
    "    \"medicine_in_iran\": \"Iran medicine\",\n",
    "}\n",
    "\n",
    "CENTURY_PREFIX = re.compile(r\"^\\d{1,2}(st|nd|rd|th)-century_\")\n",
    "\n",
    "def normalize_folder_slug(folder_name: str) -> str:\n",
    "    \"\"\"××•×¨×™×“ ×¨×•×•×—×™×/×¨×™×©×™×•×ª, ××¡×™×¨ ×§×™×“×•××ª ×××”.\"\"\"\n",
    "    slug = folder_name.strip().lower()\n",
    "    slug = CENTURY_PREFIX.sub(lambda m: m.group(0), slug)  # ××©××™×¨ ××ª ×§×™×“×•××ª ×”×××” ×× ×§×™×™××ª (×›×“×™ ×œ×”×ª××™× ×œ-map)\n",
    "    return slug\n",
    "\n",
    "def base_slug(folder_slug: str) -> str:\n",
    "    \"\"\"××¡×™×¨ ×§×™×“×•××ª ×××” ×œ×©×™××•×© ×‘×¨×™×¨×ª ××—×“×œ.\"\"\"\n",
    "    return CENTURY_PREFIX.sub(\"\", folder_slug)\n",
    "\n",
    "def infer_keyword_from_folder(folder: str) -> str:\n",
    "    slug = normalize_folder_slug(folder)\n",
    "    # ×§×•×“× ××™×¤×•×™ ××“×•×™×§\n",
    "    if slug in KEYWORD_MAP:\n",
    "        return KEYWORD_MAP[slug]\n",
    "    # × ×¡×” ×‘×œ×™ ×§×™×“×•××ª ×××”\n",
    "    bslug = base_slug(slug)\n",
    "    if bslug in KEYWORD_MAP:\n",
    "        return KEYWORD_MAP[bslug]\n",
    "    # ×‘×¨×™×¨×ª ××—×“×œ: ×”×¤×•×š ×œÖ¾\"Iran <token>\"\n",
    "    token = bslug.replace(\"iranian_\", \"\").replace(\"_\", \" \").strip()\n",
    "    token = token[:-1] if token.endswith(\"s\") else token\n",
    "    return f\"Iran {token}\".strip()\n",
    "\n",
    "def detect_name_col(df: pd.DataFrame) -> str:\n",
    "    for c in df.columns:\n",
    "        n = str(c).strip().lower()\n",
    "        if n in {\"name\", \"poi name\", \"poi_name\", \"full_name\", \"full name\"}:\n",
    "            return c\n",
    "    return df.columns[0]\n",
    "\n",
    "# ---- Aggregate from all *_with_twitter.csv under ROOT/* ----\n",
    "rows = []\n",
    "folders = sorted([d for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))])\n",
    "print(f\"ğŸ“‚ × ××¦××• {len(folders)} ×ª×™×§×™×•×ª ××ª×—×ª ×œ-{REL_ROOT}\")\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(ROOT, folder)\n",
    "    cands = sorted(glob.glob(os.path.join(folder_path, \"*_with_twitter.csv\")))\n",
    "    if not cands:\n",
    "        continue\n",
    "\n",
    "    csv_path = cands[0]\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ can't read:\", csv_path, e)\n",
    "        continue\n",
    "\n",
    "    if \"Twitter_username\" not in df.columns:\n",
    "        continue\n",
    "\n",
    "    name_col = detect_name_col(df)\n",
    "    kw = infer_keyword_from_folder(folder)\n",
    "\n",
    "    sub = df[[name_col, \"Twitter_username\"]].dropna(subset=[\"Twitter_username\"]).copy()\n",
    "    sub.rename(columns={name_col: \"poi_name\"}, inplace=True)\n",
    "    sub[\"Twitter_username\"] = sub[\"Twitter_username\"].astype(str).str.strip().str.lstrip(\"@\")\n",
    "    sub = sub[sub[\"Twitter_username\"].str.len() > 0]\n",
    "    sub[\"Twitter_url\"] = \"https://x.com/\" + sub[\"Twitter_username\"]\n",
    "    sub[\"keyword\"] = kw\n",
    "    sub[\"source_folder\"] = folder\n",
    "\n",
    "    rows.append(sub[[\"keyword\", \"poi_name\", \"Twitter_username\", \"Twitter_url\", \"source_folder\"]])\n",
    "\n",
    "# ---- Save merged CSV ----\n",
    "if rows:\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    out.drop_duplicates(subset=[\"Twitter_username\", \"keyword\", \"poi_name\"], inplace=True, ignore_index=True)\n",
    "    out.sort_values([\"keyword\", \"poi_name\"], inplace=True, ignore_index=True)\n",
    "    out.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… Saved: {OUTPUT_PATH} | rows: {len(out)}\")\n",
    "    display(out.head(20))\n",
    "else:\n",
    "    print(\"â„¹ï¸ ×œ× × ××¦××• *_with_twitter.csv ×‘×ª×™×§×™×•×ª, ××• ×©××™×Ÿ ×™×“×™×•×ª ×œ××œ×.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQoTr36h-5af"
   },
   "source": [
    "×”×•× ×¦×¢×“ ×¢×–×¨ ×˜×•×‘ ×œ×¤× ×™ ×©×œ×‘ 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtQf4kgx6Z5Q"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Add \"Twitter_status\" column to mark suspended/invalid/missing users\n",
    "# Works on Manual_Search_POIs.csv\n",
    "# ============================================\n",
    "\n",
    "from google.colab import drive\n",
    "import os, re, pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# ---- Mount Google Drive ----\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# ---- Locate (or create if missing) shared 'Iran' folder ----\n",
    "def find_shared_folder(folder_name='Iran'):\n",
    "    base_path = '/content/drive'\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for d in dirs:\n",
    "            if d.lower() == folder_name.lower():\n",
    "                return os.path.join(root, d)\n",
    "    return None\n",
    "\n",
    "BASE = find_shared_folder('Iran')\n",
    "if not BASE:\n",
    "    BASE = '/content/drive/MyDrive/Iran'\n",
    "    os.makedirs(BASE, exist_ok=True)\n",
    "    print(\"âš ï¸ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran' ××©×•×ª×¤×ª â€” × ×•×¦×¨×” ×—×“×©×” ××§×•××™×ª:\", BASE)\n",
    "else:\n",
    "    print(\"ğŸ“ × ××¦××” ×ª×™×§×™×™×ª Iran:\", BASE)\n",
    "\n",
    "# ---- Config ----\n",
    "REL_ROOT  = \"POIs\"\n",
    "FILE_NAME = \"Manual_Search_POIs.csv\"   # <-- ××¢×•×“×›×Ÿ ×œ×©× ×”×§×•×‘×¥ ×”×—×“×©\n",
    "\n",
    "ROOT = os.path.join(BASE, REL_ROOT)\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "path = os.path.join(ROOT, FILE_NAME)\n",
    "assert os.path.exists(path), f\"âŒ File not found: {path}\"\n",
    "\n",
    "# ---- Load ----\n",
    "try:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---- Helpers ----\n",
    "HANDLE_RE = re.compile(r\"^[A-Za-z0-9_]{1,15}$\")\n",
    "\n",
    "def classify_status(username: str) -> str:\n",
    "    if pd.isna(username):\n",
    "        return \"missing\"\n",
    "    s = str(username).strip().lstrip(\"@\")\n",
    "    if not s:\n",
    "        return \"missing\"\n",
    "    # ××¡×¤×¨ ×‘×œ×‘×“: ×œ×¢×™×ª×™× ××¦×‘×™×¢ ×¢×œ ×—×©×‘×•×Ÿ ××•×©×¢×”/××•××¨ ×œ××–×”×”\n",
    "    if s.isdigit():\n",
    "        return \"suspended\"\n",
    "    # ×™×“×™×ª ×œ× ×—×•×§×™×ª (××•×¨×š >15 ××• ×ª×•×•×™× ×œ× ×ª×§×™× ×™×/×¨×•×•×—×™×)\n",
    "    if (len(s) > 15) or (HANDLE_RE.fullmatch(s) is None):\n",
    "        return \"invalid\"\n",
    "    return \"active\"\n",
    "\n",
    "# ---- Compute & Save ----\n",
    "df[\"Twitter_status\"] = df[\"Twitter_username\"].apply(classify_status)\n",
    "\n",
    "df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… Updated file: {path} ({len(df)} rows)\")\n",
    "try:\n",
    "    display(df.head(20))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEiA_7JbyFvl"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ğŸ§® Step 6 â€“ Enhanced Statistics & Visuals (UPDATED)\n",
    "# Builds robust stats table + coverage + charts (portable)\n",
    "# ============================================\n",
    "\n",
    "from google.colab import drive\n",
    "from IPython.display import display\n",
    "import os, glob, re, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Mount & locate base (dynamic) ----------\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "def find_shared_folder(folder_name='Iran'):\n",
    "    base_path = '/content/drive'\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for d in dirs:\n",
    "            if d.lower() == folder_name.lower():\n",
    "                return os.path.join(root, d)\n",
    "    return None\n",
    "\n",
    "BASE = find_shared_folder('Iran')\n",
    "if not BASE:\n",
    "    BASE = '/content/drive/MyDrive/Iran'           # ×™×¦×™×¨×” ×œ×”×¨×¦×” ×¨××©×•× ×” ×× ××™×Ÿ ××©×•×ª×¤×ª\n",
    "    os.makedirs(BASE, exist_ok=True)\n",
    "    print(\"âš ï¸ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran' ××©×•×ª×¤×ª â€” × ×•×¦×¨×” ××§×•××™×ª:\", BASE)\n",
    "else:\n",
    "    print(\"ğŸ“ × ××¦××” ×ª×™×§×™×™×ª Iran:\", BASE)\n",
    "\n",
    "REL_ROOT     = \"POIs\"\n",
    "OUT_BASENAME = \"POI_statistics.csv\"             # basic\n",
    "OUT_ENHANCED = \"POI_statistics_enhanced.csv\"    # enhanced with coverage\n",
    "FIG_DIR      = \"figures\"\n",
    "\n",
    "ROOT = os.path.join(BASE, REL_ROOT)\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def safe_read_csv(path, nrows=None):\n",
    "    for enc in (None, \"utf-8\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, nrows=nrows, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # × ×™×¡×™×•×Ÿ ××—×¨×•×Ÿ ×¢× engine=python ×•-skip\n",
    "    try:\n",
    "        return pd.read_csv(path, nrows=nrows, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def is_wikipedia_csv(path):\n",
    "    \"\"\"Detect Wikipedia CSV by content (columns/values), not filename alone.\"\"\"\n",
    "    name = os.path.basename(path).lower()\n",
    "    if any(k in name for k in [\"wikidata\", \"with_twitter\"]):\n",
    "        return False\n",
    "    df = safe_read_csv(path, nrows=10)\n",
    "    if df is None or df.empty:\n",
    "        return False\n",
    "    cols = [str(c).strip().lower() for c in df.columns]\n",
    "    if any(any(k in c for k in [\"wikipedia\", \"wiki\", \"link\", \"url\"]) for c in cols):\n",
    "        return True\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            sample = \" \".join(map(str, df[c].dropna().astype(str).head(10).tolist())).lower()\n",
    "            if \"wikipedia.org\" in sample or sample.startswith(\"http\"):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def detect_twitter_col(df):\n",
    "    candidates = {\"twitter_username\",\"twitter user\",\"twitter_user\",\"twitter handle\",\"handle\",\"twitter\"}\n",
    "    for c in df.columns:\n",
    "        if str(c).strip().lower() in candidates:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        norm = re.sub(r'[^a-z]', '', str(c).lower())\n",
    "        if \"twitter\" in norm and (\"username\" in norm or \"handle\" in norm or \"user\" in norm or \"name\" in norm):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def detect_qid_col(df):\n",
    "    for c in df.columns:\n",
    "        if str(c).strip().lower() in {\"wikidata_qid\",\"qid\",\"wikidata id\",\"wikidataid\"}:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ---------- Build basic stats ----------\n",
    "rows = []\n",
    "folders = sorted([d for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))])\n",
    "\n",
    "for folder in folders:\n",
    "    fpath = os.path.join(ROOT, folder)\n",
    "    csvs = sorted(glob.glob(os.path.join(fpath, \"*.csv\")))\n",
    "    if not csvs:\n",
    "        continue\n",
    "\n",
    "    wiki_file = None\n",
    "    wikidata_file = None\n",
    "    twitter_file = None\n",
    "\n",
    "    # Twitter file by name\n",
    "    for p in csvs:\n",
    "        if re.search(r\"with_twitter\", os.path.basename(p), re.I):\n",
    "            twitter_file = p\n",
    "            break\n",
    "\n",
    "    # Wikidata: prefer detailed\n",
    "    for p in csvs:\n",
    "        if re.search(r\"wikidata.*detailed\", os.path.basename(p), re.I):\n",
    "            wikidata_file = p\n",
    "            break\n",
    "    if wikidata_file is None:\n",
    "        for p in csvs:\n",
    "            if re.search(r\"with_wikidata_ids_and_links\", os.path.basename(p), re.I):\n",
    "                wikidata_file = p\n",
    "                break\n",
    "\n",
    "    # Wikipedia by content (fallback to \"<folder>.csv\")\n",
    "    for p in csvs:\n",
    "        if is_wikipedia_csv(p):\n",
    "            wiki_file = p\n",
    "            break\n",
    "    if wiki_file is None:\n",
    "        fallback = os.path.join(fpath, f\"{folder}.csv\")\n",
    "        if os.path.exists(fallback):\n",
    "            wiki_file = fallback\n",
    "\n",
    "    wikipedia_count = 0\n",
    "    wikidata_count  = 0\n",
    "    twitter_count   = 0\n",
    "\n",
    "    # Wikipedia count\n",
    "    if wiki_file:\n",
    "        dfw = safe_read_csv(wiki_file)\n",
    "        if dfw is not None:\n",
    "            wikipedia_count = len(dfw)\n",
    "\n",
    "    # Wikidata count (count non-empty QIDs if column exists)\n",
    "    if wikidata_file:\n",
    "        dfd = safe_read_csv(wikidata_file)\n",
    "        if dfd is not None:\n",
    "            qcol = detect_qid_col(dfd)\n",
    "            wikidata_count = dfd[qcol].notna().sum() if qcol else len(dfd)\n",
    "\n",
    "    # Twitter count (valid handles only)\n",
    "    if twitter_file:\n",
    "        dft = safe_read_csv(twitter_file)\n",
    "        if dft is not None:\n",
    "            tw_col = detect_twitter_col(dft)\n",
    "            if tw_col:\n",
    "                s = dft[tw_col].astype(str).str.strip()\n",
    "                valid = s[(s != \"\") &\n",
    "                          (~s.str.fullmatch(r\"\\d+\")) &\n",
    "                          (~s.str.lower().isin({\"nan\",\"none\",\"null\",\"suspended\"}))]\n",
    "                twitter_count = valid.shape[0]\n",
    "\n",
    "    rows.append({\n",
    "        \"Category\": folder,\n",
    "        \"Wikipedia count\": wikipedia_count,\n",
    "        \"Wikidata count\": wikidata_count,\n",
    "        \"Twitter user count\": twitter_count\n",
    "    })\n",
    "\n",
    "basic = pd.DataFrame(rows).sort_values(\"Category\").reset_index(drop=True)\n",
    "basic_path = os.path.join(ROOT, OUT_BASENAME)\n",
    "basic.to_csv(basic_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… Saved basic table: {basic_path}\")\n",
    "display(basic)\n",
    "\n",
    "# ---------- Enhanced: coverage, gaps, totals ----------\n",
    "enh = basic.copy()\n",
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    enh[\"Wikidata coverage (%)\"] = (100 * enh[\"Wikidata count\"] / enh[\"Wikipedia count\"]).fillna(0).round(1)\n",
    "    enh[\"Twitter coverage (%)\"]  = (100 * enh[\"Twitter user count\"] / enh[\"Wikipedia count\"]).fillna(0).round(1)\n",
    "\n",
    "enh[\"Wikidata gap\"] = (enh[\"Wikipedia count\"] - enh[\"Wikidata count\"]).clip(lower=0)\n",
    "enh[\"Twitter gap\"]  = (enh[\"Wikipedia count\"] - enh[\"Twitter user count\"]).clip(lower=0)\n",
    "\n",
    "tot = pd.DataFrame([{\n",
    "    \"Category\": \"TOTAL\",\n",
    "    \"Wikipedia count\": enh[\"Wikipedia count\"].sum(),\n",
    "    \"Wikidata count\": enh[\"Wikidata count\"].sum(),\n",
    "    \"Twitter user count\": enh[\"Twitter user count\"].sum(),\n",
    "}])\n",
    "tot[\"Wikidata coverage (%)\"] = (100 * tot[\"Wikidata count\"] / tot[\"Wikipedia count\"]).round(1) if tot[\"Wikipedia count\"].iloc[0] else 0.0\n",
    "tot[\"Twitter coverage (%)\"]  = (100 * tot[\"Twitter user count\"] / tot[\"Wikipedia count\"]).round(1) if tot[\"Wikipedia count\"].iloc[0] else 0.0\n",
    "tot[\"Wikidata gap\"] = tot[\"Wikipedia count\"] - tot[\"Wikidata count\"]\n",
    "tot[\"Twitter gap\"]  = tot[\"Wikipedia count\"] - tot[\"Twitter user count\"]\n",
    "\n",
    "enhanced = pd.concat([enh, tot], ignore_index=True)\n",
    "enhanced_path = os.path.join(ROOT, OUT_ENHANCED)\n",
    "enhanced.to_csv(enhanced_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… Saved enhanced table: {enhanced_path}\")\n",
    "display(enhanced)\n",
    "\n",
    "# ---------- Charts ----------\n",
    "fig_dir = os.path.join(ROOT, FIG_DIR)\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# ××¡× × ×™× ××ª TOTAL ×¢×‘×•×¨ ×’×¨×¤×™× ×¤×¨-×§×˜×’×•×¨×™×”\n",
    "per_cat = enhanced[enhanced[\"Category\"] != \"TOTAL\"].reset_index(drop=True)\n",
    "\n",
    "# 1) Counts by category (lines) â€” ×œ×œ× TOTAL\n",
    "plt.figure(figsize=(12,5))\n",
    "for col in [\"Wikipedia count\", \"Wikidata count\", \"Twitter user count\"]:\n",
    "    plt.plot(per_cat[\"Category\"], per_cat[col], marker='o', label=col)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"POIs â€“ Counts by Source\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig1_path = os.path.join(fig_dir, \"counts_by_category.png\")\n",
    "plt.savefig(fig1_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ Saved: {fig1_path}\")\n",
    "\n",
    "# 2) Coverage by category (%) â€” ×œ×œ× TOTAL\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(per_cat[\"Category\"], per_cat[\"Wikidata coverage (%)\"], alpha=0.8, label=\"Wikidata coverage (%)\")\n",
    "plt.bar(per_cat[\"Category\"], per_cat[\"Twitter coverage (%)\"], alpha=0.6, label=\"Twitter coverage (%)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"POIs â€“ Coverage by Category (%)\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Coverage (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig2_path = os.path.join(fig_dir, \"coverage_by_category.png\")\n",
    "plt.savefig(fig2_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ Saved: {fig2_path}\")\n",
    "\n",
    "# 3) Stacked bars: gaps to Wikipedia â€” ×œ×œ× TOTAL\n",
    "plt.figure(figsize=(12,5))\n",
    "bar_x = range(len(per_cat))\n",
    "plt.bar(bar_x, per_cat[\"Wikidata count\"], label=\"Wikidata count\")\n",
    "plt.bar(bar_x, per_cat[\"Wikidata gap\"], bottom=per_cat[\"Wikidata count\"], label=\"Wikidata gap\")\n",
    "plt.xticks(bar_x, per_cat[\"Category\"], rotation=45, ha='right')\n",
    "plt.title(\"Wikidata Count + Gap to Wikipedia\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Entities\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig3_path = os.path.join(fig_dir, \"wikidata_gap_stacked.png\")\n",
    "plt.savefig(fig3_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ Saved: {fig3_path}\")\n",
    "\n",
    "print(\"\\nâœ… Done. Files written to:\")\n",
    "print(f\"- {basic_path}\")\n",
    "print(f\"- {enhanced_path}\")\n",
    "print(f\"- {fig1_path}\")\n",
    "print(f\"- {fig2_path}\")\n",
    "print(f\"- {fig3_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyxENkekGZG_"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y snscrape\n",
    "!pip install git+https://github.com/JustAnotherArchivist/snscrape.git@master\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCiCJWiLY8OM"
   },
   "source": [
    "# *Build POI_twitter_users_data.csv from POIs_Search_Manual*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HAqzyi1ZdW7"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 7 (UPDATED) â€” Build POI_twitter_users_data.csv\n",
    "# Source: Manual_Search_POIs.csv  (not POIs_Search_Manual*)\n",
    "# Portable: dynamic Drive path detection, robust CSV reading\n",
    "# ============================================================\n",
    "\n",
    "# 0) Mount Drive (safe if already mounted)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# 0.1) Locate (or create if missing) shared 'Iran' folder + POIs root\n",
    "import os, re, glob\n",
    "from pathlib import Path\n",
    "\n",
    "def find_shared_folder(folder_name='Iran'):\n",
    "    base_path = '/content/drive'\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for d in dirs:\n",
    "            if d.lower() == folder_name.lower():\n",
    "                return os.path.join(root, d)\n",
    "    return None\n",
    "\n",
    "BASE = find_shared_folder('Iran')\n",
    "if not BASE:\n",
    "    BASE = '/content/drive/MyDrive/Iran'\n",
    "    os.makedirs(BASE, exist_ok=True)\n",
    "    print(\"âš ï¸ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran' ××©×•×ª×¤×ª â€” × ×•×¦×¨×” ×—×“×©×” ××§×•××™×ª:\", BASE)\n",
    "else:\n",
    "    print(\"ğŸ“ × ××¦××” ×ª×™×§×™×™×ª Iran:\", BASE)\n",
    "\n",
    "REL = \"POIs\"\n",
    "POIs_DIR = os.path.join(BASE, REL)\n",
    "os.makedirs(POIs_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Quiet logs + progress bar\n",
    "import logging, warnings\n",
    "logging.getLogger(\"snscrape\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"snscrape.modules.twitter\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# 2) SSL & deps\n",
    "import sys, subprocess, importlib, site, pathlib, time, random\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"certifi\", \"requests\", \"urllib3\", \"idna\", \"tqdm\"], check=True)\n",
    "import certifi\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "\n",
    "# 3) Install/patch snscrape for Py3.12 (if needed)\n",
    "def ensure_snscrape():\n",
    "    try:\n",
    "        import snscrape.modules.twitter as sntwitter\n",
    "        return sntwitter\n",
    "    except Exception:\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"git+https://github.com/JustAnotherArchivist/snscrape.git@master\"], check=True)\n",
    "        # patch legacy importer if needed\n",
    "        import pkgutil\n",
    "        for sp in set(site.getsitepackages() + [site.getusersitepackages()]):\n",
    "            p = pathlib.Path(sp) / \"snscrape\" / \"modules\" / \"__init__.py\"\n",
    "            if p.exists():\n",
    "                txt = p.read_text(encoding=\"utf-8\")\n",
    "                if \"find_module(\" in txt or \"load_module(\" in txt:\n",
    "                    import re as _re\n",
    "                    patched = _re.sub(\n",
    "                        r'(\\s*)module\\s*=\\s*importer\\.find_module\\(moduleName\\)\\.load_module\\(moduleName\\)',\n",
    "                        r'\\1import importlib\\n\\1module = importlib.import_module(moduleName)',\n",
    "                        txt\n",
    "                    )\n",
    "                    if \"prefixLen\" not in patched:\n",
    "                        patched = _re.sub(\n",
    "                            r'for importer, moduleName, ispkg in pkgutil\\.iter_modules\\(__path__, prefix\\):',\n",
    "                            'prefixLen = len(prefix)\\n    for importer, moduleName, ispkg in pkgutil.iter_modules(__path__, prefix):',\n",
    "                            patched\n",
    "                        )\n",
    "                    p.write_text(patched, encoding=\"utf-8\")\n",
    "        importlib.invalidate_caches()\n",
    "        import snscrape.modules.twitter as sntwitter\n",
    "        return sntwitter\n",
    "\n",
    "sntwitter = ensure_snscrape()\n",
    "\n",
    "# 4) Locate Manual_Search_POIs.csv under POIs_DIR\n",
    "import pandas as pd\n",
    "\n",
    "def safe_read_csv(path):\n",
    "    for enc in (None, \"utf-8\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        return pd.read_csv(path, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "def find_manual_csv(root_dir: str) -> str:\n",
    "    # ×—×¤×© ×‘×“×™×•×§ Manual_Search_POIs.csv ×§×•×“×\n",
    "    exact = os.path.join(root_dir, \"Manual_Search_POIs.csv\")\n",
    "    if os.path.exists(exact):\n",
    "        return exact\n",
    "    # fallback: ×›×œ ×§×•×‘×¥ ×‘×©× Manual_Search_POIs*.csv\n",
    "    candidates = []\n",
    "    for dp, _, files in os.walk(root_dir):\n",
    "        for fn in files:\n",
    "            if re.match(r\"Manual_Search_POIs.*\\.csv$\", fn, flags=re.I):\n",
    "                full = os.path.join(dp, fn)\n",
    "                try:\n",
    "                    mtime = os.path.getmtime(full)\n",
    "                except Exception:\n",
    "                    mtime = 0\n",
    "                candidates.append((mtime, full))\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"×œ× × ××¦× ×§×•×‘×¥ ×‘×©× 'Manual_Search_POIs*.csv' ×ª×—×ª: {root_dir}\")\n",
    "    candidates.sort(reverse=True)\n",
    "    return candidates[0][1]\n",
    "\n",
    "MANUAL_CSV = find_manual_csv(POIs_DIR)\n",
    "print(\"ğŸ“„ Manual CSV:\", MANUAL_CSV)\n",
    "\n",
    "# 5) Output side-by-side to POIs root\n",
    "OUT_CSV = os.path.join(POIs_DIR, \"POI_twitter_users_data.csv\")\n",
    "print(\"ğŸ“ Output CSV :\", OUT_CSV)\n",
    "\n",
    "# 6) Load manual CSV & detect twitter/name/status columns\n",
    "df = safe_read_csv(MANUAL_CSV)\n",
    "\n",
    "def looks_like_twitter_col(name: str) -> bool:\n",
    "    n = name.lower().strip()\n",
    "    keys = [\"twitter\", \"x\", \"handle\", \"username\", \"tw\", \"×˜×•×•×™×˜×¨\"]\n",
    "    return any(k == n or k in n for k in keys)\n",
    "\n",
    "twitter_col = next((c for c in df.columns if looks_like_twitter_col(str(c))), None)\n",
    "if twitter_col is None:\n",
    "    for c in df.columns:\n",
    "        sample = \" \".join(map(str, df[c].dropna().astype(str).head(40).tolist())).lower()\n",
    "        if \"twitter.com\" in sample or \"x.com/\" in sample or \"@\" in sample:\n",
    "            twitter_col = c\n",
    "            break\n",
    "assert twitter_col, \"×œ× ××¦××ª×™ ×¢××•×“×ª ×˜×•×•×™×˜×¨.\"\n",
    "\n",
    "status_col = next((c for c in df.columns if str(c).lower().strip() in [\"twitter_status\", \"status\", \"tw_status\"]), None)\n",
    "name_col   = next((c for c in df.columns if str(c).lower().strip() in [\"poi_name\", \"name\", \"full name\", \"fullname\"]), None)\n",
    "\n",
    "# 7) Extract usernames (max 15 chars per X rules)\n",
    "import re as _re\n",
    "HANDLE_RE = _re.compile(r\"^[A-Za-z0-9_]{1,15}$\")\n",
    "\n",
    "def extract_username(val) -> str | None:\n",
    "    if pd.isna(val): return None\n",
    "    s = str(val).strip()\n",
    "    if not s: return None\n",
    "    s = s.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"www.\", \"\")\n",
    "    m = _re.search(r\"(?:twitter\\.com|x\\.com)/([A-Za-z0-9_]{1,15})\", s, flags=_re.I)\n",
    "    if m: return m.group(1)\n",
    "    m = _re.search(r\"@([A-Za-z0-9_]{1,15})\", s)\n",
    "    if m: return m.group(1)\n",
    "    if HANDLE_RE.fullmatch(s): return s\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "for _, row in df.iterrows():\n",
    "    u = extract_username(row.get(twitter_col))\n",
    "    if not u:\n",
    "        continue\n",
    "    rows.append({\n",
    "        \"username\": u,\n",
    "        \"poi_name\": row.get(name_col),\n",
    "        \"twitter_status\": (str(row.get(status_col)).lower().strip() if status_col else None)\n",
    "    })\n",
    "\n",
    "assert rows, \"×œ× × ××¦××• ×©××•×ª ××©×ª××© ×ª×§×™× ×™×.\"\n",
    "\n",
    "# 8) Helpers: scraping / stub rows\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def g(obj, name, default=None):\n",
    "    try: return getattr(obj, name, default)\n",
    "    except Exception: return default\n",
    "\n",
    "def safe_attr(obj, path, default=None):\n",
    "    cur = obj\n",
    "    try:\n",
    "        for part in path.split(\".\"):\n",
    "            cur = getattr(cur, part, None)\n",
    "            if cur is None: return default\n",
    "        return cur\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def build_stub(u, poi, status, note):\n",
    "    return {\n",
    "        \"full name\": None,\n",
    "        \"description / bio\": None,\n",
    "        \"followers_count\": None,\n",
    "        \"following_count\": None,\n",
    "        \"statuses_count\": None,\n",
    "        \"created_at\": None,\n",
    "        \"profile_image_url\": None,\n",
    "        \"banner_url\": None,\n",
    "        \"username\": u,\n",
    "        \"external_url\": None,\n",
    "        \"location\": None,\n",
    "        \"verified\": None,\n",
    "        \"protected\": None,\n",
    "        \"language\": None,\n",
    "        \"scraped_at\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"poi_name\": poi,\n",
    "        \"twitter_status\": status,\n",
    "        \"source\": \"csv_status_only\",\n",
    "        \"error\": note,\n",
    "    }\n",
    "\n",
    "def fetch_user_metadata(u: str):\n",
    "    user_obj, last_err = None, None\n",
    "    for ctor in [\n",
    "        lambda x: sntwitter.TwitterUserScraper(x),           # positional\n",
    "        lambda x: sntwitter.TwitterUserScraper(user=x),      # keyword (some builds)\n",
    "        lambda x: sntwitter.XUserScraper(x) if hasattr(sntwitter, \"XUserScraper\") else (_ for _ in ()).throw(AttributeError(\"XUserScraper missing\")),\n",
    "    ]:\n",
    "        try:\n",
    "            user_obj = ctor(u).entity\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if user_obj is None:\n",
    "        raise RuntimeError(repr(last_err))\n",
    "\n",
    "    return {\n",
    "        \"full name\": g(user_obj, \"displayname\"),\n",
    "        \"description / bio\": g(user_obj, \"renderedDescription\"),\n",
    "        \"followers_count\": g(user_obj, \"followersCount\"),\n",
    "        \"following_count\": g(user_obj, \"friendsCount\"),\n",
    "        \"statuses_count\": g(user_obj, \"statusesCount\"),\n",
    "        \"created_at\": g(user_obj, \"created\"),\n",
    "        \"profile_image_url\": g(user_obj, \"profileImageUrl\"),\n",
    "        \"banner_url\": g(user_obj, \"bannerImageUrl\"),\n",
    "        \"username\": g(user_obj, \"username\"),\n",
    "        \"external_url\": safe_attr(user_obj, \"link.url\"),\n",
    "        \"location\": g(user_obj, \"location\"),\n",
    "        \"verified\": g(user_obj, \"verified\"),\n",
    "        \"protected\": g(user_obj, \"protected\"),\n",
    "        \"language\": g(user_obj, \"lang\"),\n",
    "        \"scraped_at\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"poi_name\": None,  # × ××œ× ×‘×”××©×š\n",
    "        \"twitter_status\": None,  # × ××œ× ×‘×”××©×š\n",
    "        \"source\": \"snscrape\",\n",
    "        \"error\": None,\n",
    "    }\n",
    "\n",
    "# 9) Run with progress, retries, periodic saves\n",
    "MAX_USERS  = None     # ×œ×“×•×’××” 100 ×œ×‘×“×™×§×”; None = ×”×›×œ\n",
    "RETRIES    = 3\n",
    "BASE_SLEEP = 1.0\n",
    "SAVE_EVERY = 25       # ×©××™×¨×ª ×‘×™× ×™×™× ×›×œ N ××©×ª××©×™×\n",
    "\n",
    "to_process = rows[:MAX_USERS] if MAX_USERS else rows\n",
    "records = []\n",
    "ok = suspended_cnt = failed = 0\n",
    "\n",
    "cols_order = [\n",
    "    \"full name\", \"description / bio\", \"followers_count\", \"following_count\",\n",
    "    \"statuses_count\", \"created_at\", \"profile_image_url\", \"banner_url\",\n",
    "    \"username\", \"external_url\", \"location\", \"verified\", \"protected\",\n",
    "    \"language\", \"scraped_at\", \"poi_name\", \"twitter_status\", \"source\", \"error\"\n",
    "]\n",
    "\n",
    "pbar = tqdm(to_process, desc=\"Scraping users\", unit=\"user\")\n",
    "for idx, r in enumerate(pbar, start=1):\n",
    "    u = r[\"username\"]; poi = r[\"poi_name\"]; st = r[\"twitter_status\"]\n",
    "\n",
    "    # Respect suspended/invalid/missing from Step 5/6\n",
    "    if st and any(k in st for k in (\"suspend\", \"invalid\", \"missing\")):\n",
    "        records.append(build_stub(u, poi, st, note=f\"status_from_manual_csv:{st}\"))\n",
    "        suspended_cnt += 1 if \"suspend\" in (st or \"\") else suspended_cnt\n",
    "        pbar.set_postfix(ok=ok, suspended=suspended_cnt, failed=failed)\n",
    "        continue\n",
    "\n",
    "    for a in range(RETRIES):\n",
    "        try:\n",
    "            rec = fetch_user_metadata(u)\n",
    "            rec[\"poi_name\"] = poi\n",
    "            rec[\"twitter_status\"] = st\n",
    "            records.append(rec)\n",
    "            ok += 1\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if a == RETRIES - 1:\n",
    "                records.append(build_stub(u, poi, st, note=f\"scrape_failed: {e}\"))\n",
    "                failed += 1\n",
    "            time.sleep(BASE_SLEEP + 0.6 * a + random.random() * 0.4)\n",
    "\n",
    "    pbar.set_postfix(ok=ok, suspended=suspended_cnt, failed=failed)\n",
    "\n",
    "    # periodic save\n",
    "    if idx % SAVE_EVERY == 0:\n",
    "        tmp_df = pd.DataFrame.from_records(records)\n",
    "        tmp_df = tmp_df[[c for c in cols_order if c in tmp_df.columns]]\n",
    "        if os.path.exists(OUT_CSV):\n",
    "            old = safe_read_csv(OUT_CSV)\n",
    "            merged = (pd.concat([old, tmp_df], ignore_index=True)\n",
    "                      .drop_duplicates(subset=[\"username\"], keep=\"last\"))\n",
    "        else:\n",
    "            merged = tmp_df\n",
    "        merged.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 10) Final save/merge\n",
    "out_df = pd.DataFrame.from_records(records)\n",
    "out_df = out_df[[c for c in cols_order if c in out_df.columns]]\n",
    "\n",
    "if os.path.exists(OUT_CSV):\n",
    "    old = safe_read_csv(OUT_CSV)\n",
    "    final = (pd.concat([old, out_df], ignore_index=True)\n",
    "             .drop_duplicates(subset=[\"username\"], keep=\"last\"))\n",
    "else:\n",
    "    final = out_df\n",
    "\n",
    "final.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nâœ… DONE. Saved -> {OUT_CSV}\")\n",
    "print(f\"   Totals: scraped={ok}, suspended/invalid/missing(stub)={suspended_cnt}, failed(stub)={failed}, total_written={len(final)}\")\n",
    "\n",
    "# Preview\n",
    "try:\n",
    "    display(final.tail(5))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndMxspAAeD7r"
   },
   "source": [
    "# fill LANGUAGE with full names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQZ0o6FXZvJ-"
   },
   "outputs": [],
   "source": [
    "# ========= Post-process: fill LANGUAGE with full names (no abbreviations) =========\n",
    "# ×××œ× ×©×¤×ª ××©×ª××©×™× ××©×“×” ×”-bio ×”×™×›×Ÿ ×©×—×¡×¨, ×•×××™×¨ ×§×•×“×™× (en/fa/â€¦) ×œ×©××•×ª ××œ××™×\n",
    "\n",
    "# ×”×ª×§× ×•×ª ×¨×§ ×× ×¦×¨×™×š\n",
    "try:\n",
    "    from langdetect import detect, DetectorFactory\n",
    "except Exception:\n",
    "    !pip -q install langdetect\n",
    "    from langdetect import detect, DetectorFactory\n",
    "\n",
    "try:\n",
    "    import pycountry\n",
    "except Exception:\n",
    "    !pip -q install pycountry\n",
    "    import pycountry\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "import os, re, glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "DetectorFactory.seed = 0  # ×™×¦×™×‘×•×ª ×‘×–×™×”×•×™\n",
    "\n",
    "# ---- ××™×ª×•×¨ ×“×™× ××™ ×©×œ ×ª×™×§×™×™×ª 'Iran' ×•×©×œ ×§×•×‘×¥ ×”-CSV ×ª×—×ª POIs ----\n",
    "def find_shared_folder(folder_name='Iran'):\n",
    "    base_path = '/content/drive'\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for d in dirs:\n",
    "            if d.lower() == folder_name.lower():\n",
    "                return os.path.join(root, d)\n",
    "    return None\n",
    "\n",
    "BASE = find_shared_folder('Iran') or '/content/drive/MyDrive/Iran'\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "POIs_DIR = os.path.join(BASE, \"POIs\")\n",
    "os.makedirs(POIs_DIR, exist_ok=True)\n",
    "\n",
    "# ×”×§×•×‘×¥ ×”××¨×›×–×™ ×©× ×•×¦×¨ ×‘×©×œ×‘ 7\n",
    "CSV = Path(os.path.join(POIs_DIR, \"POI_twitter_users_data.csv\"))\n",
    "if not CSV.exists():\n",
    "    raise FileNotFoundError(f\"âŒ ×œ× × ××¦× ×”×§×•×‘×¥: {CSV}\")\n",
    "\n",
    "# ×§×¨×™××” ×¢××™×“×”\n",
    "try:\n",
    "    df = pd.read_csv(CSV, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(CSV, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ×•×“× ×©×§×™×™××ª ×¢××•×“×ª language ×›××—×¨×•×–×ª\n",
    "if \"language\" not in df.columns:\n",
    "    df[\"language\"] = \"\"\n",
    "df[\"language\"] = df[\"language\"].astype(\"string\")\n",
    "\n",
    "# ---- ×–×™×”×•×™ ×©×¤×” ××”-bio ×¨×§ ×”×™×›×Ÿ ×©×—×¡×¨/×¨×™×§ ----\n",
    "BIO_COL = \"description / bio\" if \"description / bio\" in df.columns else None\n",
    "\n",
    "def detect_lang_from_bio(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    s = str(text).strip()\n",
    "    if len(s) < 6:  # ×§×¦×¨ ××“×™, ×œ× × ×–×”×” ×›×“×™ ×œ×”×¤×—×™×ª false positives\n",
    "        return None\n",
    "    try:\n",
    "        return detect(s)  # ××—×–×™×¨ ×§×•×“ ×›××• 'en','fa','ar',...\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "if BIO_COL:\n",
    "    mask_missing = df[\"language\"].isna() | (df[\"language\"].str.strip().fillna(\"\") == \"\")\n",
    "    df.loc[mask_missing, \"language\"] = df.loc[mask_missing, BIO_COL].apply(detect_lang_from_bio)\n",
    "\n",
    "# ---- ×”××¨×” ×œ×©××•×ª ××œ××™× (×œ× ××§×¦×¨ ×©××•×ª ××œ××™× ×§×™×™××™×) ----\n",
    "# ×ª×•××š alpha-2/alpha-3 + × ×¤×•×¦×™×\n",
    "MANUAL_MAP = {\n",
    "    \"en\": \"English\",\n",
    "    \"fa\": \"Persian (Farsi)\",\n",
    "    \"ar\": \"Arabic\",\n",
    "    \"fr\": \"French\",\n",
    "    \"es\": \"Spanish\",\n",
    "    \"de\": \"German\",\n",
    "    \"it\": \"Italian\",\n",
    "    \"ru\": \"Russian\",\n",
    "    \"tr\": \"Turkish\",\n",
    "    \"ur\": \"Urdu\",\n",
    "    \"pt\": \"Portuguese\",\n",
    "    \"ja\": \"Japanese\",\n",
    "    \"bg\": \"Bulgarian\",\n",
    "    \"ro\": \"Romanian\",\n",
    "    \"et\": \"Estonian\",\n",
    "    \"sq\": \"Albanian\",\n",
    "    \"so\": \"Somali\",\n",
    "    \"id\": \"Indonesian\",\n",
    "    \"da\": \"Danish\",\n",
    "    \"sl\": \"Slovenian\",\n",
    "    \"ca\": \"Catalan\",\n",
    "    \"af\": \"Afrikaans\",\n",
    "    # alpha-3 × ×¤×•×¦×™×\n",
    "    \"fas\": \"Persian (Farsi)\",\n",
    "    \"ara\": \"Arabic\",\n",
    "    \"eng\": \"English\",\n",
    "    \"spa\": \"Spanish\",\n",
    "    \"deu\": \"German\",\n",
    "    \"ger\": \"German\",   # old biblio code\n",
    "    \"fra\": \"French\",\n",
    "    \"fre\": \"French\",   # old biblio code\n",
    "    \"rus\": \"Russian\",\n",
    "    \"tur\": \"Turkish\",\n",
    "    \"urd\": \"Urdu\",\n",
    "}\n",
    "\n",
    "def is_code_like(v: str) -> bool:\n",
    "    if not isinstance(v, str):\n",
    "        return False\n",
    "    s = v.strip()\n",
    "    # ×§×•×“ × ×¨××” ×›××• 2â€“3 ××•×ª×™×•×ª ×œ×˜×™× ×™×•×ª; ×œ× ×›×•×œ×œ×™× ×©××•×ª ××œ××™×/××™×œ×™× ×¢× ×¨×•×•×—\n",
    "    return bool(re.fullmatch(r\"[A-Za-z]{2,3}\", s))\n",
    "\n",
    "def to_full_language_name(val):\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    s = str(val).strip()\n",
    "    if s == \"\" or s.lower() == \"none\":\n",
    "        return \"\"\n",
    "    # ×× ×–×” ×œ× × ×¨××” ×›××• ×§×•×“ (×›×‘×¨ ×©× ××œ×?) â€” × ×©××™×¨ ×›××• ×©×”×•×\n",
    "    if not is_code_like(s):\n",
    "        return s\n",
    "    c = s.lower()\n",
    "    # ××¤×” ×™×“× ×™×ª ×§×•×“×\n",
    "    if c in MANUAL_MAP:\n",
    "        return MANUAL_MAP[c]\n",
    "    # pycountry: alpha-2\n",
    "    try:\n",
    "        lang = pycountry.languages.get(alpha_2=c)\n",
    "        if lang and getattr(lang, \"name\", None):\n",
    "            return lang.name\n",
    "    except Exception:\n",
    "        pass\n",
    "    # pycountry: alpha-3\n",
    "    try:\n",
    "        lang = pycountry.languages.get(alpha_3=c)\n",
    "        if lang and getattr(lang, \"name\", None):\n",
    "            return lang.name\n",
    "    except Exception:\n",
    "        pass\n",
    "    # × ×¤×™×œ×” ×—×–×¨×” â€” × ×©××™×¨ ×›××• ×©×”×•× (×œ× × ×›×ª×•×‘ ×§×•×“ ×¨×™×§)\n",
    "    return s\n",
    "\n",
    "# ×”××¨ ×¨×§ ×¢×¨×›×™× ×©×××© ×§×•×“/×§×•×“ ××–×•×”×”\n",
    "df[\"language\"] = df[\"language\"].apply(to_full_language_name).fillna(\"\").astype(\"string\")\n",
    "\n",
    "# ğŸ”’ ×’×™×‘×•×™ ×œ×¤× ×™ ×›×ª×™×‘×”\n",
    "backup = CSV.with_suffix(\".backup.csv\")\n",
    "df.to_csv(backup, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"ğŸ›Ÿ Backup written: {backup}\")\n",
    "\n",
    "# ×›×ª×™×‘×” ×—×–×¨×”\n",
    "df.to_csv(CSV, index=False, encoding=\"utf-8-sig\")\n",
    "filled = (df[\"language\"].str.len() > 0).sum()\n",
    "print(f\"âœ… language ×”×•××¨ ×œ×©××•×ª ××œ××™× ×•× ×©××¨. ×©×•×¨×•×ª ×¢× ×©×¤×” ×œ×-×¨×™×§×”: {filled} ××ª×•×š {len(df)}\")\n",
    "\n",
    "try:\n",
    "    display(df.head(10)[[\"username\",\"poi_name\",\"language\"]])\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eI4cpube54x"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 8 â€” POI Twitter statistics & summaries (portable, updated)\n",
    "# ============================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "import os, re, glob, math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# ---------- ××™×ª×•×¨ ×“×™× ××™ ×©×œ ×ª×™×§×™×™×ª Iran ×•-POIs ----------\n",
    "def find_shared_folder(folder_name='Iran'):\n",
    "    base_path = '/content/drive'\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for d in dirs:\n",
    "            if d.lower() == folder_name.lower():\n",
    "                return os.path.join(root, d)\n",
    "    return None\n",
    "\n",
    "BASE = find_shared_folder('Iran')\n",
    "if not BASE:\n",
    "    BASE = '/content/drive/MyDrive/Iran'\n",
    "    os.makedirs(BASE, exist_ok=True)\n",
    "    print(\"âš ï¸ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran' ××©×•×ª×¤×ª â€” × ×•×¦×¨×” ×—×“×©×” ××§×•××™×ª:\", BASE)\n",
    "else:\n",
    "    print(\"ğŸ“ × ××¦××” ×ª×™×§×™×™×ª Iran:\", BASE)\n",
    "\n",
    "REL = \"POIs\"\n",
    "POIs_DIR = os.path.join(BASE, REL)\n",
    "os.makedirs(POIs_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- ×¢×–×¨×”: ××¦×™××ª ×§×‘×¦×™× + ×§×¨×™××” ×¢××™×“×” ----------\n",
    "def newest(pattern: str):\n",
    "    files = glob.glob(os.path.join(POIs_DIR, pattern))\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda f: os.path.getmtime(f), reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "def safe_read_csv(path, nrows=None):\n",
    "    for enc in (None, \"utf-8\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, nrows=nrows, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        return pd.read_csv(path, nrows=nrows, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "DATA_CSV   = newest(\"POI_twitter_users_data*.csv\")\n",
    "# ×©× ×—×“×© ×‘×”×ª×× ×œ×©×™× ×•×™×™× ×”×§×•×“××™×\n",
    "MANUAL_CSV = newest(\"Manual_Search_POIs*.csv\")  # ××•×¤×¦×™×•× ×œ×™, ×œ×”×¤×§×ª ×§×˜×’×•×¨×™×”\n",
    "\n",
    "assert DATA_CSV is not None, \"×œ× × ××¦× ×§×•×‘×¥ POI_twitter_users_data*.csv â€“ ×™×© ×œ×”×¨×™×¥ ××ª ×©×œ×‘ 7 ×§×•×“×.\"\n",
    "\n",
    "print(\"ğŸ“„ DATA :\", DATA_CSV)\n",
    "print(\"ğŸ“„ MANUAL (optional):\", MANUAL_CSV)\n",
    "\n",
    "# ---------- ×˜×¢×™× ×ª ×”× ×ª×•× ×™× ----------\n",
    "df = safe_read_csv(DATA_CSV)\n",
    "if df is None:\n",
    "    raise RuntimeError(f\"×œ× ×”×¦×œ×—×ª×™ ×œ×§×¨×•× ××ª ×”×§×•×‘×¥: {DATA_CSV}\")\n",
    "\n",
    "# × ×•×•×“× ×¢××•×“×•×ª ×™×¡×•×“ â€“ × ×•×¦×¨×• ×‘×©×œ×‘ 7\n",
    "for col in [\"username\", \"followers_count\", \"following_count\", \"statuses_count\", \"description / bio\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = None\n",
    "\n",
    "# ---------- ×”×•×¡×¤×ª ×§×˜×’×•×¨×™×” (×× ×™×© Manual) ----------\n",
    "HANDLE_RE = re.compile(r\"^[A-Za-z0-9_]{1,15}$\")\n",
    "\n",
    "def extract_username(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    s = str(val).strip()\n",
    "    s = s.replace(\"https://\",\"\").replace(\"http://\",\"\").replace(\"www.\",\"\")\n",
    "    m = re.search(r\"(?:twitter\\.com|x\\.com)/([A-Za-z0-9_]{1,15})\", s, re.I)\n",
    "    if m: return m.group(1)\n",
    "    m = re.search(r\"@([A-Za-z0-9_]{1,15})\", s)\n",
    "    if m: return m.group(1)\n",
    "    if HANDLE_RE.fullmatch(s): return s\n",
    "    return None\n",
    "\n",
    "category_col = None\n",
    "if MANUAL_CSV:\n",
    "    manual = safe_read_csv(MANUAL_CSV)\n",
    "    if manual is not None and not manual.empty:\n",
    "        # ×¢××•×“×ª ×™×•×–×¨ ××¤×©×¨×™×ª: Twitter_username / username / ...\n",
    "        cand_user_cols = [c for c in manual.columns if str(c).lower().strip() in\n",
    "                          {\"twitter_username\",\"username\",\"tw_username\",\"twitter user\",\"user\"}]\n",
    "        if not cand_user_cols:\n",
    "            # × ×™×¡×™×•×Ÿ ×—×™×œ×•×¥ ××”×¢××•×“×” ×¢× ×§×™×©×•×¨\n",
    "            url_like = []\n",
    "            for c in manual.columns:\n",
    "                try:\n",
    "                    sample = \" \".join(map(str, manual[c].dropna().astype(str).head(100).tolist())).lower()\n",
    "                except Exception:\n",
    "                    sample = \"\"\n",
    "                if \"twitter\" in str(c).lower() or \"x.com/\" in sample or \"twitter.com/\" in sample:\n",
    "                    url_like.append(c)\n",
    "            cand_user_cols = url_like[:1]\n",
    "\n",
    "        if cand_user_cols:\n",
    "            mu = manual.copy()\n",
    "            mu[\"__user_join_key__\"] = mu[cand_user_cols[0]].apply(extract_username).str.lower()\n",
    "\n",
    "            # ×§×˜×’×•×¨×™×”: ×§×•×“× source_folder ×× ×§×™×™×, ××—×¨×ª keyword, ××—×¨×ª unknown\n",
    "            if \"source_folder\" in mu.columns:\n",
    "                mu[\"category\"] = mu[\"source_folder\"].astype(str).str.strip().str.lower()\n",
    "            else:\n",
    "                cand_cat = None\n",
    "                for c in [\"keyword\", \"category\", \"ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ\", \"×§×˜×’×•×¨×™×”\"]:\n",
    "                    if c in mu.columns:\n",
    "                        cand_cat = c\n",
    "                        break\n",
    "                if cand_cat:\n",
    "                    mu[\"category\"] = (mu[cand_cat].astype(str).str.strip()\n",
    "                                      .str.replace(r\"\\s+\", \"_\", regex=True).str.lower())\n",
    "                else:\n",
    "                    mu[\"category\"] = \"unknown\"\n",
    "\n",
    "            df[\"__user_join_key__\"] = df[\"username\"].astype(str).str.lower()\n",
    "            df = df.merge(mu[[\"__user_join_key__\", \"category\"]], on=\"__user_join_key__\", how=\"left\")\n",
    "            df.drop(columns=[\"__user_join_key__\"], inplace=True, errors=\"ignore\")\n",
    "            category_col = \"category\"\n",
    "\n",
    "if category_col is None:\n",
    "    df[\"category\"] = \"unknown\"\n",
    "    category_col = \"category\"\n",
    "\n",
    "# ---------- × ×™×§×•×™ ×˜×™×¤×•×¡×™ ×¢××•×“×•×ª ××¡×¤×¨×™×•×ª ----------\n",
    "for c in [\"followers_count\", \"following_count\", \"statuses_count\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# ---------- ×—×™×©×•×‘×™ ×××§×¨×• ----------\n",
    "num_pois_total = df[\"username\"].nunique()\n",
    "\n",
    "avg_followers = df[\"followers_count\"].mean(skipna=True)\n",
    "avg_following = df[\"following_count\"].mean(skipna=True)\n",
    "avg_posts     = df[\"statuses_count\"].mean(skipna=True)\n",
    "\n",
    "print(\"\\nğŸŒŸ ×¡×™×›×•× ×›×œ×œ×™:\")\n",
    "print(f\"- ××¡×¤×¨ ×”-POIs ×”×›×•×œ×œ: {num_pois_total}\")\n",
    "print(f\"- ×××•×¦×¢ followers : {avg_followers:,.0f}\" if not math.isnan(avg_followers) else \"- ××™×Ÿ × ×ª×•× ×™× ×œ×¢×•×§×‘×™×\")\n",
    "print(f\"- ×××•×¦×¢ following : {avg_following:,.0f}\" if not math.isnan(avg_following) else \"- ××™×Ÿ × ×ª×•× ×™× ×œ× ×¢×§×‘×™×\")\n",
    "print(f\"- ×××•×¦×¢ posts     : {avg_posts:,.0f}\"     if not math.isnan(avg_posts)     else \"- ××™×Ÿ × ×ª×•× ×™× ×œ×¤×•×¡×˜×™×\")\n",
    "\n",
    "# ---------- ×˜×‘×œ×” 1: ×××¤×™×™× ×™× ×›××•×ª×™×™× ×œ×¤×™ ×§×˜×’×•×¨×™×” ----------\n",
    "grp = (df.groupby(\"category\", dropna=False)\n",
    "         .agg(\n",
    "             Num_POIs=(\"username\",\"nunique\"),\n",
    "             Avg_Followers=(\"followers_count\",\"mean\"),\n",
    "             Avg_Following=(\"following_count\",\"mean\"),\n",
    "             Avg_Posts=(\"statuses_count\",\"mean\"),\n",
    "          )\n",
    "         .reset_index()\n",
    "      )\n",
    "for c in [\"Avg_Followers\",\"Avg_Following\",\"Avg_Posts\"]:\n",
    "    grp[c] = grp[c].round(0)\n",
    "\n",
    "# ---------- ×˜×‘×œ×” 2: Occupation ××ª×•×š ×”×‘×™×• (×›×•×œ×œ ×‘×¨×™××•×ª) ----------\n",
    "occ_map = {\n",
    "    r\"\\bminister|mp|parliament|politic|senator|governor|president|ambassador|diplomat|mayor|congress|knesset\\b\": \"Politician\",\n",
    "    r\"\\bjournalist|reporter|editor|news|press\\b\": \"Journalist\",\n",
    "    r\"\\bactor|actress|cinema|film|movie|director|producer|screenwriter\\b\": \"Artist\",\n",
    "    r\"\\bsinger|vocal|music|musician|composer|band|rapper|songwriter\\b\": \"Artist\",\n",
    "    r\"\\bfootball|soccer|fifa|goalkeeper|striker|midfielder|defender|athlete\\b\": \"Athlete\",\n",
    "    r\"\\bscientist|researcher|professor|phd|chemist|physicist|biologist|engineer\\b\": \"Scientist\",\n",
    "    r\"\\bwriter|author|poet|novelist\\b\": \"Writer\",\n",
    "    r\"\\beconomist|economic\\b\": \"Economist\",\n",
    "    r\"\\bayatollah|cleric|imam|rabbi|priest\\b\": \"Religious_Leader\",\n",
    "    # Healthcare ×”×¨×—×‘×•×ª:\n",
    "    r\"\\bphysician|doctor|md|surgeon|cardiolog|oncolog|neurolog|pediatric|hospital|clinic|health\\b\": \"Healthcare\",\n",
    "}\n",
    "\n",
    "def infer_occ(text: str) -> str:\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"Unknown\"\n",
    "    s = text.lower()\n",
    "    for pat, label in occ_map.items():\n",
    "        if re.search(pat, s):\n",
    "            return label\n",
    "    return \"Other/Unknown\"\n",
    "\n",
    "df[\"Occupation\"] = df[\"description / bio\"].apply(infer_occ)\n",
    "\n",
    "occ = (df.groupby(\"Occupation\", dropna=False)\n",
    "         .agg(Count=(\"username\",\"nunique\"))\n",
    "         .reset_index()\n",
    "      )\n",
    "total = occ[\"Count\"].sum() if len(occ) else 0\n",
    "occ[\"Percentage (%)\"] = (occ[\"Count\"] / total * 100).round(1) if total else 0\n",
    "\n",
    "# ---------- ×©××™×¨×” ×œ×§×‘×¦×™× ----------\n",
    "stamp = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUT_MASTER = os.path.join(POIs_DIR, \"POI_twitter_users_statistics.csv\")\n",
    "OUT_CAT    = os.path.join(POIs_DIR, \"POI_stats_by_category.csv\")\n",
    "OUT_OCC    = os.path.join(POIs_DIR, \"POI_stats_by_occupation.csv\")\n",
    "\n",
    "summary_row = pd.DataFrame([{\n",
    "    \"metric\":\"overview\",\n",
    "    \"num_pois_total\": num_pois_total,\n",
    "    \"avg_followers\": round(avg_followers,0) if not math.isnan(avg_followers) else None,\n",
    "    \"avg_following\": round(avg_following,0) if not math.isnan(avg_following) else None,\n",
    "    \"avg_posts\":     round(avg_posts,0)     if not math.isnan(avg_posts)     else None,\n",
    "    \"generated_at_utc\": stamp\n",
    "}])\n",
    "\n",
    "# × ×©××•×¨ â€œ×××¡×˜×¨â€ + × ×¦×¨×£ ××œ×™×• ××ª ×©×ª×™ ×”×˜×‘×œ××•×ª\n",
    "summary_row.to_csv(OUT_MASTER, index=False, encoding=\"utf-8-sig\")\n",
    "with open(OUT_MASTER, \"a\", encoding=\"utf-8-sig\") as f:\n",
    "    f.write(\"\\n# --- by category ---\\n\")\n",
    "    grp.to_csv(f, index=False)\n",
    "    f.write(\"\\n# --- by occupation ---\\n\")\n",
    "    occ.to_csv(f, index=False)\n",
    "\n",
    "# ×‘× ×•×¡×£ × ×©××•×¨ ×›×œ ×˜×‘×œ×” ×‘× ×¤×¨×“\n",
    "grp.to_csv(OUT_CAT, index=False, encoding=\"utf-8-sig\")\n",
    "occ.to_csv(OUT_OCC, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"\\nâœ… × ×©××¨×• ×”×§×‘×¦×™×:\")\n",
    "print(\"  â€¢\", OUT_MASTER)\n",
    "print(\"  â€¢\", OUT_CAT)\n",
    "print(\"  â€¢\", OUT_OCC)\n",
    "\n",
    "print(\"\\nğŸ” ×”×¦×¦×” ××”×™×¨×” â€” ×˜×‘×œ×” 1 (×œ×¤×™ ×§×˜×’×•×¨×™×”):\")\n",
    "display(grp.sort_values(\"Num_POIs\", ascending=False).head(10))\n",
    "\n",
    "print(\"\\nğŸ” ×”×¦×¦×” ××”×™×¨×” â€” ×˜×‘×œ×” 2 (Occupation):\")\n",
    "display(occ.sort_values(\"Count\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Syr9TrATkYuU"
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# STEP 8 â€” Visuals only (no CSVs) â€” portable (UPDATED)\n",
    "# ================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "import os, pandas as pd, matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from IPython.display import display\n",
    "\n",
    "# -------- Locate 'Iran/POIs' dynamically --------\n",
    "def find_shared_folder(folder_name='Iran'):\n",
    "    base_path = '/content/drive'\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for d in dirs:\n",
    "            if d.lower() == folder_name.lower():\n",
    "                return os.path.join(root, d)\n",
    "    return None\n",
    "\n",
    "BASE = find_shared_folder('Iran') or '/content/drive/MyDrive/Iran'\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "POIs_DIR = os.path.join(BASE, \"POIs\")\n",
    "os.makedirs(POIs_DIR, exist_ok=True)\n",
    "\n",
    "# -------- Ensure figures/stage8_visuals exists --------\n",
    "FIG_DIR = os.path.join(POIs_DIR, \"figures\")\n",
    "OUT_DIR = os.path.join(FIG_DIR, \"stage8_visuals\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(\"ğŸ“ Saving figures to:\", OUT_DIR)\n",
    "\n",
    "# -------- Helpers --------\n",
    "def safe_read_csv(path):\n",
    "    for enc in (None, \"utf-8\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.read_csv(path, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "\n",
    "def thousands(x, pos):\n",
    "    try:\n",
    "        return f\"{int(x):,}\"\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def title_case(s):\n",
    "    return str(s).replace(\"_\", \" \").title()\n",
    "\n",
    "# -------- Load summary CSVs created in step 8 --------\n",
    "cat_csv = os.path.join(POIs_DIR, \"POI_stats_by_category.csv\")     # category, Num_POIs, Avg_Followers, Avg_Following, Avg_Posts\n",
    "occ_csv = os.path.join(POIs_DIR, \"POI_stats_by_occupation.csv\")   # Occupation, Count, Percentage (%)\n",
    "\n",
    "assert os.path.exists(cat_csv), f\"âŒ ×—×¡×¨ ×§×•×‘×¥: {cat_csv} (×”×¨×¥ ××ª STEP 8 ×”××œ× ×§×•×“×)\"\n",
    "assert os.path.exists(occ_csv), f\"âŒ ×—×¡×¨ ×§×•×‘×¥: {occ_csv} (×”×¨×¥ ××ª STEP 8 ×”××œ× ×§×•×“×)\"\n",
    "\n",
    "df_cat = safe_read_csv(cat_csv)\n",
    "df_occ = safe_read_csv(occ_csv)\n",
    "\n",
    "# × ×™×§×•×™ ×©××•×ª ×¢××•×“×•×ª\n",
    "df_cat.columns = [c.strip() for c in df_cat.columns]\n",
    "df_occ.columns = [c.strip() for c in df_occ.columns]\n",
    "\n",
    "# ×•×“× ×¢××•×“×•×ª ×—×•×‘×” ×§×™×™××•×ª\n",
    "for c in [\"category\", \"Num_POIs\", \"Avg_Followers\", \"Avg_Following\", \"Avg_Posts\"]:\n",
    "    if c not in df_cat.columns:\n",
    "        raise ValueError(f\"×¢××•×“×” ×—×¡×¨×” ×‘-POI_stats_by_category.csv: {c}\")\n",
    "\n",
    "for c in [\"Occupation\", \"Count\"]:\n",
    "    if c not in df_occ.columns:\n",
    "        raise ValueError(f\"×¢××•×“×” ×—×¡×¨×” ×‘-POI_stats_by_occupation.csv: {c}\")\n",
    "if \"Percentage (%)\" not in df_occ.columns:\n",
    "    df_occ[\"Percentage (%)\"] = (df_occ[\"Count\"] / df_occ[\"Count\"].sum() * 100) if df_occ[\"Count\"].sum() else 0\n",
    "\n",
    "# ×”××¨×•×ª ×•××™×–×•×’ ×¢×¨×›×™× ×—×¨×™×’×™×\n",
    "num_cols_cat = [\"Num_POIs\", \"Avg_Followers\", \"Avg_Following\", \"Avg_Posts\"]\n",
    "df_cat[num_cols_cat] = df_cat[num_cols_cat].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "df_occ[\"Count\"] = pd.to_numeric(df_occ[\"Count\"], errors=\"coerce\").fillna(0)\n",
    "df_occ[\"Percentage (%)\"] = pd.to_numeric(df_occ[\"Percentage (%)\"], errors=\"coerce\").fillna(0).clip(lower=0, upper=100)\n",
    "\n",
    "# ×œ×× ×•×¢ ×›×¤×™×œ×•×ª Other/Unknown ×•-Unknown\n",
    "df_occ[\"Occupation\"] = df_occ[\"Occupation\"].replace({\"Other/Unknown\": \"Unknown\"})\n",
    "df_occ = (df_occ.groupby(\"Occupation\", as_index=False)\n",
    "                 .agg({\"Count\": \"sum\", \"Percentage (%)\": \"sum\"}))\n",
    "df_occ[\"Percentage (%)\"] = df_occ[\"Percentage (%)\"].clip(upper=100)\n",
    "\n",
    "# ×¡×“×¨ ×œ×”×¦×’×” ×™×¤×”\n",
    "df_cat = df_cat.sort_values(\"Num_POIs\", ascending=True)\n",
    "df_occ = df_occ.sort_values(\"Percentage (%)\", ascending=True)\n",
    "\n",
    "# -------- 1) ×‘×¨×™×: ××¡×¤×¨ ×”-POIs ×œ×›×œ ×§×˜×’×•×¨×™×” --------\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.barh([title_case(c) for c in df_cat[\"category\"]], df_cat[\"Num_POIs\"])\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(thousands))\n",
    "plt.xlabel(\"Number of POIs\")\n",
    "plt.title(\"POIs per Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pois_per_category.png\"), dpi=150)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"pois_per_category.svg\"))\n",
    "plt.show()\n",
    "\n",
    "# -------- 2) ×‘×¨×™×: ×××•×¦×¢ Followers ×œ×¤×™ ×§×˜×’×•×¨×™×” --------\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.barh([title_case(c) for c in df_cat[\"category\"]], df_cat[\"Avg_Followers\"])\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(thousands))\n",
    "plt.xlabel(\"Average Followers\")\n",
    "plt.title(\"Average Followers by Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"avg_followers_by_category.png\"), dpi=150)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"avg_followers_by_category.svg\"))\n",
    "plt.show()\n",
    "\n",
    "# -------- 3) ×‘×¨×™×: ×××•×¦×¢ Following ×œ×¤×™ ×§×˜×’×•×¨×™×” --------\n",
    "plt.figure(figsize=(11, 6))\n",
    "plt.barh([title_case(c) for c in df_cat[\"category\"]], df_cat[\"Avg_Following\"])\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(thousands))\n",
    "plt.xlabel(\"Average Following\")\n",
    "plt.title(\"Average Following by Category\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"avg_following_by_category.png\"), dpi=150)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"avg_following_by_category.svg\"))\n",
    "plt.show()\n",
    "\n",
    "# -------- 4) ×‘×¨×™×: ×”×ª×¤×œ×’×•×ª Occupation ×‘××—×•×–×™× --------\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_occ[\"Occupation\"], df_occ[\"Percentage (%)\"])\n",
    "plt.xlabel(\"Percentage (%)\")\n",
    "plt.title(\"Occupation Distribution of POIs\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"occupation_distribution.png\"), dpi=150)\n",
    "plt.savefig(os.path.join(OUT_DIR, \"occupation_distribution.svg\"))\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Done. Visuals saved in:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oE4jOKaKvrH3"
   },
   "source": [
    "**API Twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ti7IylSqJGr"
   },
   "outputs": [],
   "source": [
    "pip install selenium webdriver-manager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PirGEDfj-YV5"
   },
   "outputs": [],
   "source": [
    "!apt-get update -y\n",
    "!apt-get install -y chromium-browser || apt-get install -y chromium\n",
    "!apt-get install -y chromium-chromedriver || apt-get install -y chromium-driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WVkmtlwp-yA"
   },
   "outputs": [],
   "source": [
    "# === ×“×™××’× ×•×¡×˜×™×§×” + ×¨×™×¢× ×•×Ÿ ×”×ª×§× ×•×ª ===\n",
    "!apt-get update -y\n",
    "# × ×ª×§×™×Ÿ ×’× chromium ×•×’× ×©× ×™ ×•×¨×™×× ×˜×™× ×©×œ chromedriver (×œ×¤×™ ×”×“×™×¡×˜×¨×• ×©×œ Colab)\n",
    "!apt-get install -y chromium-browser || apt-get install -y chromium\n",
    "!apt-get install -y chromium-chromedriver || apt-get install -y chromium-driver\n",
    "\n",
    "# ×‘×“×™×§×ª ×’×¨×¡××•×ª ×•× ×ª×™×‘×™×\n",
    "!which -a chromium chromium-browser || true\n",
    "!which -a chromedriver || true\n",
    "!chromium --version || chromium-browser --version\n",
    "!chromedriver --version\n",
    "!ls -l /usr/bin/chromedriver || true\n",
    "!ls -l /usr/lib/chromium-browser/chromedriver || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbBG-Zk6us2l"
   },
   "outputs": [],
   "source": [
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hZYLyl-uozH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTEXcdwAtn-m"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 9 â€” Followers/Following for POIs (Colab, Selenium)\n",
    "# ============================================\n",
    "\n",
    "# ---------- ğŸ”§ CONFIG ----------\n",
    "BASE_DRIVE_DIR = \"/content/drive/MyDrive/Iran/POIs\"            # ğŸ”§ ×©×•×¨×© ×”×¤×¨×•×™×§×˜ ×‘×“×¨×™×™×‘\n",
    "USERS_CSV      = f\"{BASE_DRIVE_DIR}/POI_twitter_users_data.csv\" # ×—×™×™×‘ ×¢××•×“×” 'username'\n",
    "OUT_DIR        = f\"{BASE_DRIVE_DIR}/Candidates\"\n",
    "COOKIES_JSON   = f\"{BASE_DRIVE_DIR}/x_cookies.json\"            # ğŸ”§ ×§×•×§×™×– JSON ××—×©×‘×•×Ÿ X ×”××—×•×‘×¨\n",
    "\n",
    "# ××”×™×¨ ×œ×‘×“×™×§×”:\n",
    "SAMPLE_SIZE  = None        # â† ×¢×•×“×›×Ÿ ×œ-5\n",
    "DO_FOLLOWERS = True\n",
    "DO_FOLLOWING = True\n",
    "MAX_PER_LIST = 80\n",
    "SCROLL_STEPS = 15\n",
    "OPEN_PAUSE   = 1.6\n",
    "SCROLL_PAUSE = 0.7\n",
    "\n",
    "# ××¤×©×¨ ×œ×‘×—×•×¨ ×™×“× ×™×ª ×‘××§×•× ×œ×§×—×ª ××”-CSV:\n",
    "MANUAL_USERS = []  # ×œ×“×•×’××”: [\"amir_jadidi\", \"OAradwinwin\", \"Golshifteh\"]\n",
    "\n",
    "# ---------- MOUNT ----------\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# ---------- IMPORTS ----------\n",
    "import os, json, time, random, re, pandas as pd, shutil, stat\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# ---------- FOLDERS ----------\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- INIT CHROMIUM ----------\n",
    "def _which(cmds):\n",
    "    import shutil as _sh\n",
    "    for c in cmds:\n",
    "        p = _sh.which(c)\n",
    "        if p:\n",
    "            return p\n",
    "    return None\n",
    "def _chmod_x(path):\n",
    "    try:\n",
    "        mode = os.stat(path).st_mode\n",
    "        os.chmod(path, mode | stat.S_IEXEC)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "CHROME_BIN = _which([\"chromium-browser\",\"chromium\",\"google-chrome\",\"chrome\"])\n",
    "CHROMEDRIVER_CANDIDATES = [\n",
    "    _which([\"chromedriver\"]),\n",
    "    \"/usr/bin/chromedriver\",\n",
    "    \"/usr/lib/chromium-browser/chromedriver\",\n",
    "]\n",
    "assert CHROME_BIN, \"Chromium/Chrome ×œ× × ××¦×. ×•×“× ×©×”×¨×¦×ª ××ª ×ª× ×”×”×ª×§× ×”.\"\n",
    "\n",
    "opts = Options()\n",
    "opts.binary_location = CHROME_BIN\n",
    "opts.add_argument(\"--headless=new\")\n",
    "opts.add_argument(\"--no-sandbox\")\n",
    "opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "opts.add_argument(\"--disable-gpu\")\n",
    "opts.add_argument(\"--window-size=1280,2000\")\n",
    "opts.add_argument(\"--lang=en-US,en;q=0.9\")\n",
    "\n",
    "driver = None\n",
    "for drv in [p for p in CHROMEDRIVER_CANDIDATES if p and os.path.exists(p)]:\n",
    "    try:\n",
    "        _chmod_x(drv)\n",
    "        service = Service(drv)\n",
    "        driver  = webdriver.Chrome(service=service, options=opts)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"chromedriver try failed:\", e)\n",
    "        driver = None\n",
    "if driver is None:\n",
    "    driver = webdriver.Chrome(options=opts)  # Selenium Manager fallback\n",
    "print(\"âœ… WebDriver is up.\")\n",
    "\n",
    "# ---------- COOKIES ----------\n",
    "def load_cookies(cookies_path: str):\n",
    "    driver.get(\"https://x.com/\")\n",
    "    time.sleep(1.5)\n",
    "    with open(cookies_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cookies = json.load(f)\n",
    "    injected = 0\n",
    "    for c in cookies:\n",
    "        try:\n",
    "            c_fixed = {\n",
    "                \"name\": c.get(\"name\"),\n",
    "                \"value\": c.get(\"value\"),\n",
    "                \"domain\": c.get(\"domain\", \".x.com\"),\n",
    "                \"path\": c.get(\"path\", \"/\"),\n",
    "                \"secure\": bool(c.get(\"secure\", True)),\n",
    "                \"httpOnly\": bool(c.get(\"httpOnly\", False))\n",
    "            }\n",
    "            if \"expiry\" in c:           c_fixed[\"expiry\"] = int(c[\"expiry\"])\n",
    "            elif \"expirationDate\" in c: c_fixed[\"expiry\"] = int(c[\"expirationDate\"])\n",
    "            driver.add_cookie(c_fixed); injected += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    driver.refresh(); time.sleep(2.0)\n",
    "    print(f\"Injected {injected} cookies.\")\n",
    "\n",
    "# ---------- SCRAPE HELPERS ----------\n",
    "USER_HREF_RE = re.compile(r\"^/[A-Za-z0-9_]{1,15}$\")\n",
    "\n",
    "def _extract_usernames_from_page():\n",
    "    anchors = driver.find_elements(By.XPATH, \"//a[@href]\")\n",
    "    names = set()\n",
    "    for a in anchors:\n",
    "        try:\n",
    "            href = a.get_attribute(\"href\") or \"\"\n",
    "            path  = \"/\" + href.split(\"://\",1)[-1].split(\"/\",1)[-1]\n",
    "            first = \"/\" + path.strip(\"/\").split(\"/\",1)[0]\n",
    "            if USER_HREF_RE.fullmatch(first):\n",
    "                cand = first.strip(\"/\")\n",
    "                if cand.lower() in {\"home\",\"i\",\"explore\",\"notifications\",\"messages\",\"settings\"}:\n",
    "                    continue\n",
    "                names.add(cand)\n",
    "        except:\n",
    "            pass\n",
    "    return list(names)\n",
    "\n",
    "def _smooth_scroll(steps):\n",
    "    last_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for _ in range(steps):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE + random.uniform(0.15, 0.35))\n",
    "        new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_h == last_h:\n",
    "            time.sleep(0.5)\n",
    "            new_h = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_h == last_h:\n",
    "                break\n",
    "        last_h = new_h\n",
    "\n",
    "def _collect_list(user, tab):\n",
    "    url = f\"https://x.com/{user}/{tab}\"\n",
    "    driver.get(url); time.sleep(OPEN_PAUSE + random.uniform(0.1, 0.3))\n",
    "    got = set()\n",
    "    for _ in range(SCROLL_STEPS):\n",
    "        got.update(_extract_usernames_from_page())\n",
    "        if len(got) >= MAX_PER_LIST:\n",
    "            break\n",
    "        _smooth_scroll(1)\n",
    "    got.discard(user)\n",
    "    return list(got)[:MAX_PER_LIST]\n",
    "\n",
    "# ---------- Full name helper (×œ×“×¨×™×©×ª ×”-README) ----------\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def _get_full_name(user):\n",
    "    \"\"\"××—×–×™×¨ ××ª ×©× ×”×ª×¦×•×’×” (Full Name) ××¤×¨×•×¤×™×œ ×”××©×ª××©, ××• None ×× ×œ× × ××¦×.\"\"\"\n",
    "    try:\n",
    "        driver.get(f\"https://x.com/{user}\")\n",
    "        time.sleep(OPEN_PAUSE)\n",
    "        el = WebDriverWait(driver, 6).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid=\"UserName\"]'))\n",
    "        )\n",
    "        txt = el.text or \"\"\n",
    "        return txt.split(\"\\n\")[0].strip() if txt else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------- LOAD USERS ----------\n",
    "df = pd.read_csv(USERS_CSV)\n",
    "assert \"username\" in df.columns, \"×‘×§×•×‘×¥ ×”××©×ª××©×™× ×—×¡×¨×” ×¢××•×“×” 'username'\"\n",
    "users_all = [str(u).strip().lstrip(\"@\") for u in df[\"username\"].dropna()]\n",
    "users_all = list(dict.fromkeys([u for u in users_all if u]))\n",
    "\n",
    "if MANUAL_USERS:\n",
    "    users = [u.strip().lstrip(\"@\") for u in MANUAL_USERS] if SAMPLE_SIZE in (None, 0) else [u.strip().lstrip(\"@\") for u in MANUAL_USERS][:SAMPLE_SIZE]\n",
    "else:\n",
    "    users = users_all if SAMPLE_SIZE in (None, 0) else users_all[:SAMPLE_SIZE]\n",
    "\n",
    "print(f\"Processing {len(users)} users:\", users[:10], \"...\" if len(users) > 10 else \"\")\n",
    "\n",
    "\n",
    "# ---------- RUN ----------\n",
    "load_cookies(COOKIES_JSON)  # ×—×©×•×‘: ×§×•×§×™×– ×¢×“×›× ×™×™× ××”×—×©×‘×•×Ÿ ×©×œ×š\n",
    "connections, stats = [], []\n",
    "\n",
    "for uname in users:\n",
    "    print(f\"\\nâ¡ï¸ {uname}\")\n",
    "    errs = []\n",
    "    followers, following = [], []\n",
    "\n",
    "    if DO_FOLLOWERS:\n",
    "        try:\n",
    "            followers = _collect_list(uname, \"followers\")\n",
    "        except Exception as e:\n",
    "            errs.append(f\"followers_error:{e}\")\n",
    "\n",
    "    if DO_FOLLOWING:\n",
    "        try:\n",
    "            following = _collect_list(uname, \"following\")\n",
    "        except Exception as e:\n",
    "            errs.append(f\"following_error:{e}\")\n",
    "\n",
    "    for x in followers:\n",
    "        connections.append((uname, x, \"follower\"))\n",
    "    for x in following:\n",
    "        connections.append((uname, x, \"following\"))\n",
    "\n",
    "    full_name = _get_full_name(uname)  # â† ×—×“×©: ×©× ×ª×¦×•×’×” ×œ×¡×˜×˜×™×¡×˜×™×§×•×ª\n",
    "    stats.append({\n",
    "        \"Twitter_username\": uname,\n",
    "        \"Full_Name\": full_name,                       # â† ×—×“×©\n",
    "        \"Followers_Collected\": len(followers),\n",
    "        \"Following_Collected\": len(following),\n",
    "        \"Error\": \"; \".join(errs) if errs else None\n",
    "    })\n",
    "\n",
    "# ---------- SAVE (×‘×“×™×•×§ ×œ×¤×™ ×”×“×¨×™×©×•×ª ×©×œ ×¡×¢×™×£ 9) ----------\n",
    "CONN_CSV = os.path.join(OUT_DIR, \"POIs_candidate_connections.csv\")\n",
    "STAT_CSV = os.path.join(OUT_DIR, \"Candidates_statistics.csv\")\n",
    "\n",
    "# ××—×™×§×” ××•×˜×•××˜×™×ª ×›×“×™ ×©×ª×¨××” ×¢×“×›×•×Ÿ ××™×™×“×™\n",
    "for path in [CONN_CSV, STAT_CSV]:\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "        print(f\"ğŸ§¹ Deleted old file: {path}\")\n",
    "\n",
    "(pd.DataFrame(connections, columns=[\"target_username\",\"other_username\",\"type\"])\n",
    "   .drop_duplicates()\n",
    "   .to_csv(CONN_CSV, index=False, encoding=\"utf-8-sig\"))\n",
    "\n",
    "(pd.DataFrame(stats)[[\"Twitter_username\",\"Full_Name\",\"Followers_Collected\",\"Following_Collected\",\"Error\"]]\n",
    "   .to_csv(STAT_CSV, index=False, encoding=\"utf-8-sig\"))\n",
    "\n",
    "print(\"âœ… Done\")\n",
    "print(\"connections:\", CONN_CSV, f\"rows={len(connections)}\")\n",
    "print(\"stats:\", STAT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjSs7XxdSn_R"
   },
   "outputs": [],
   "source": [
    "!pip -q install playwright tqdm pandas\n",
    "!playwright install --with-deps chromium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYMVJXn7QUPq"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 10 â€” Candidates user metadata (built on your twitter_selenium scraper)\n",
    "# Output: /content/drive/MyDrive/Iran/POIs/Candidates/Candidates_user_data.csv\n",
    "# ============================================\n",
    "\n",
    "# --- ×× ×¦×¨×™×š ×”×ª×§× ×•×ª ×‘×¡×™×¡ (×‘×˜×œ ×”×¢×¨×” ×•×”×¨×™×¥ ×¤×¢× ××—×ª) ---\n",
    "# !apt-get update -y && apt-get install -y chromium-browser chromium-chromedriver\n",
    "# !pip install -U selenium tqdm pandas\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "BASE_DRIVE_DIR = \"/content/drive/MyDrive/Iran/POIs\"\n",
    "CAND_DIR       = f\"{BASE_DRIVE_DIR}/Candidates\"\n",
    "\n",
    "USERS_CSV      = f\"{BASE_DRIVE_DIR}/POI_twitter_users_data.csv\"     # ×§×œ×˜: ×”-POIs ×”××§×•×¨×™×™× (×¢××•×“×”: username)\n",
    "CONN_CSV       = f\"{CAND_DIR}/POIs_candidate_connections.csv\"       # ×§×œ×˜: ×©×œ×‘ 9 (×¢××•×“×•×ª: target_username, other_username)\n",
    "OUT_CSV        = f\"{CAND_DIR}/Candidates_user_data.csv\"             # ×¤×œ×˜: ×©×œ×‘ 10\n",
    "\n",
    "# ğŸ”¹ ×§×•×‘×¥ ×”-scraper ×©×œ×š; ×× ×©××•×¨ ×œ×™×“ ×”× ×•×˜×‘×•×§, ×¢×“×›×Ÿ PATH ××ª××™×:\n",
    "SCRAPER_PATH = \"/content/drive/MyDrive/Iran/POIs/tools/twitter_selenium.py\"\n",
    "\n",
    "\n",
    "# ×©×œ×™×˜×” ×‘×”×¨×¦×”\n",
    "SAMPLE_LIMIT      = 4000     # â† ×‘×“×™×§×ª ×¢×©×Ÿ ×¢×œ 10; ×©× ×” ×œ-None ×›×“×™ ×œ×¨×•×¥ ×¢×œ ×›×•×œ×\n",
    "BATCH_SIZE        = 40      # ×›××” ××©×ª××©×™× ×‘×›×œ ×× ×”\n",
    "RETRIES_PER_USER  = 2       # × ×™×¡×™×•× ×•×ª ×œ×›×œ ××©×ª××©\n",
    "OPEN_PAUSE        = 1.4     # ×©× ×™×•×ª ×”××ª× ×” ××—×¨×™ ×¤×ª×™×—×ª ×¤×¨×•×¤×™×œ\n",
    "\n",
    "# ××™×¤×•×™ ×¢××•×“×•×ª: ××”×¡×§×¨×•×œ×¨ ×©×œ×š ××œ ×”×¡×›×™××” ×”×ª×§× ×™×ª ×©×œ ×”×¤×¨×•×™×§×˜\n",
    "COLUMNS = [\n",
    "    \"username\",\"Full_Name\",\"Bio\",\"Location\",\"External_URL\",\n",
    "    \"Followers_Count\",\"Following_Count\",\"Is_Verified\",\"Is_Protected\",\n",
    "    \"Joined_Date\",\"Profile_URL\",\"Profile_Image\"\n",
    "]\n",
    "\n",
    "# ---------- MOUNT ----------\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# ---------- IMPORTS ----------\n",
    "import os, json, time, shutil, pandas as pd, sys, importlib.util, re\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "Path(CAND_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- LOAD YOUR SCRAPER MODULE ----------\n",
    "def load_scraper(scraper_path: str):\n",
    "    if not os.path.exists(scraper_path):\n",
    "        raise FileNotFoundError(f\"twitter_selenium.py not found at: {scraper_path}\")\n",
    "    spec = importlib.util.spec_from_file_location(\"twitter_selenium\", scraper_path)\n",
    "    mod  = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"twitter_selenium\"] = mod\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "ts = load_scraper(SCRAPER_PATH)  # ××›×™×œ scrape_twitter_profile(username)\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def normalize_row_from_scraper(d: dict):\n",
    "    \"\"\"\n",
    "    ×××¤×” ××ª ×¤×œ×˜ ×”×¡×§×¨×•×œ×¨ ×©×œ×š ×œ×©×“×•×ª ×©×œ ×”×¤×¨×•×™×§×˜.\n",
    "    scraper keys (your file): user_name, name, bio, location, url, joined_date,\n",
    "                              followers, following, verified, profile_image, profile_url\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"username\":       (d.get(\"user_name\") or \"\").lstrip(\"@\"),\n",
    "        \"Full_Name\":      d.get(\"name\"),\n",
    "        \"Bio\":            d.get(\"bio\"),\n",
    "        \"Location\":       d.get(\"location\"),\n",
    "        \"External_URL\":   d.get(\"url\"),\n",
    "        \"Followers_Count\":d.get(\"followers\"),\n",
    "        \"Following_Count\":d.get(\"following\"),\n",
    "        \"Is_Verified\":    bool(d.get(\"verified\")),\n",
    "        \"Is_Protected\":   None,  # ×”×¡×§×¨×•×œ×¨ ×©×œ×š ×œ× ××—×œ×¥; ××¤×©×¨ ×œ×”×©××™×¨ None (×œ× × ×“×¨×© ×‘×¡×¢×™×£)\n",
    "        \"Joined_Date\":    d.get(\"joined_date\"),\n",
    "        \"Profile_URL\":    d.get(\"profile_url\"),\n",
    "        \"Profile_Image\":  d.get(\"profile_image\")\n",
    "    }\n",
    "\n",
    "# ---------- BUILD USER SET (×©×œ×‘ 9 + ×”××§×•×¨, ×œ×œ× ×›×¤×™×œ×•×™×•×ª) ----------\n",
    "user_set = set()\n",
    "\n",
    "if os.path.exists(CONN_CSV):\n",
    "    df_conn = pd.read_csv(CONN_CSV)\n",
    "    if \"target_username\" in df_conn.columns:\n",
    "        user_set.update(df_conn[\"target_username\"].dropna().astype(str).str.replace(\"@\",\"\", regex=False).str.strip())\n",
    "    if \"other_username\" in df_conn.columns:\n",
    "        user_set.update(df_conn[\"other_username\"].dropna().astype(str).str.replace(\"@\",\"\", regex=False).str.strip())\n",
    "\n",
    "if os.path.exists(USERS_CSV):\n",
    "    df_users = pd.read_csv(USERS_CSV)\n",
    "    if \"username\" in df_users.columns:\n",
    "        user_set.update(df_users[\"username\"].dropna().astype(str).str.replace(\"@\",\"\", regex=False).str.strip())\n",
    "\n",
    "users_all = [u for u in dict.fromkeys([u for u in user_set if u])]  # ×™×™×—×•×“ + ×©××™×¨×ª ×¡×“×¨\n",
    "if SAMPLE_LIMIT:\n",
    "    users_all = users_all[:SAMPLE_LIMIT]\n",
    "\n",
    "print(f\"Users to enrich: {len(users_all)} â†’ {users_all[:5]}\")\n",
    "\n",
    "# ---------- HEADER BOOTSTRAP (××‘×˜×™×— ×›×•×ª×¨×•×ª) ----------\n",
    "if not os.path.exists(OUT_CSV):\n",
    "    pd.DataFrame(columns=COLUMNS).to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- RESUME (×“×œ×’ ×¢×œ ××™ ×©×›×‘×¨ × ×©××¨) ----------\n",
    "done = set()\n",
    "try:\n",
    "    prev = pd.read_csv(OUT_CSV, usecols=[\"username\"])\n",
    "    done = set(prev[\"username\"].dropna().astype(str))\n",
    "    print(f\"â†» Resume: skipping {len(done)} already saved.\")\n",
    "except Exception as e:\n",
    "    print(\"Resume read issue:\", e)\n",
    "\n",
    "# ---------- RUN (×‘×× ×•×ª) ----------\n",
    "from math import ceil\n",
    "total   = len(users_all)\n",
    "batches = ceil(total / BATCH_SIZE)\n",
    "idx = 0\n",
    "\n",
    "for b in range(batches):\n",
    "    chunk = [u for u in users_all[idx: idx+BATCH_SIZE] if u not in done]\n",
    "    idx += BATCH_SIZE\n",
    "    if not chunk:\n",
    "        continue\n",
    "\n",
    "    with tqdm(total=len(chunk), desc=f\"Batch {b+1}/{batches}\", unit=\"user\") as pbar:\n",
    "        for uname in chunk:\n",
    "            row = None\n",
    "            # × ×™×¡×™×•× ×•×ª ×—×•×–×¨×™× ×œ×›×œ ××©×ª××© (×”×¡×§×¨×•×œ×¨ ×™×¤×ª×—/×™×¡×’×•×¨ ×›×¨×•× ×‘×¢×¦××•)\n",
    "            for attempt in range(RETRIES_PER_USER + 1):\n",
    "                try:\n",
    "                    raw = ts.scrape_twitter_profile(uname)   # âš ï¸ ××ª×•×š twitter_selenium.py ×©×œ×š\n",
    "                    row = normalize_row_from_scraper(raw)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < RETRIES_PER_USER:\n",
    "                        time.sleep(0.8)\n",
    "                    else:\n",
    "                        # ×‘××§×¨×” ×›×©×œ â€” × ×©××•×¨ ×©×•×¨×” \"×¨×™×§×”\" ×¢× ×”×©× ×•×”-URL ×‘×œ×‘×“, ×›×“×™ ×œ× ×œ×—×¡×•× ×”×ª×§×“××•×ª\n",
    "                        row = {\n",
    "                            \"username\": uname, \"Full_Name\": None, \"Bio\": None, \"Location\": None,\n",
    "                            \"External_URL\": None, \"Followers_Count\": None, \"Following_Count\": None,\n",
    "                            \"Is_Verified\": None, \"Is_Protected\": None, \"Joined_Date\": None,\n",
    "                            \"Profile_URL\": f\"https://twitter.com/{uname}\", \"Profile_Image\": None\n",
    "                        }\n",
    "\n",
    "            pd.DataFrame([row], columns=COLUMNS).to_csv(\n",
    "                OUT_CSV, mode=\"a\", header=False, index=False, encoding=\"utf-8-sig\"\n",
    "            )\n",
    "            done.add(uname)\n",
    "            pbar.update(1)\n",
    "\n",
    "# ---------- ×“×”-×“×•×¤×œ×™×§×¦×™×” (×œ×™×ª×¨ ×‘×™×˜×—×•×Ÿ) ----------\n",
    "try:\n",
    "    df_out = pd.read_csv(OUT_CSV)\n",
    "    df_out = df_out.drop_duplicates(subset=[\"username\"], keep=\"first\")\n",
    "    df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"âœ… Saved:\", OUT_CSV)\n",
    "    print(df_out.head())\n",
    "except Exception as e:\n",
    "    print(\"Compact error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znmS1Wnnnu_l"
   },
   "outputs": [],
   "source": [
    "ls /content/drive/MyDrive/Iran/POIs/tools\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (IRAN Project)",
   "language": "python",
   "name": "iran"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
