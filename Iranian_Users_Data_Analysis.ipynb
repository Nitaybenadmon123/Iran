{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0w9_WRvuHxVE"
   },
   "source": [
    "Iran Twitter / X Data Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOI74U-kVM5v"
   },
   "source": [
    " # italicized text*POI Scraper (Wikipedia Categories â†’ CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "myZurgeOLVxN"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# File: step1_collect_wikipedia_categories.py (UPDATED)\n",
    "#\n",
    "# Description:\n",
    "#   Recursively collects people/pages from Wikipedia categories (including subcategories),\n",
    "#   and saves a CSV per category with columns: Name, Wikipedia_Link.\n",
    "#   Output structure (per course spec):\n",
    "#       <IRAN_DIR>/\n",
    "#         POIs/\n",
    "#           <slug>/\n",
    "#             <slug>_wikipedia.csv\n",
    "#   Also produces a summary table and a ZIP with all CSVs.\n",
    "# ============================================\n",
    "\n",
    "import requests, time, os, zipfile\n",
    "import pandas as pd\n",
    "from urllib.parse import quote\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# ---------------- IRAN_DIR resolution ----------------\n",
    "# Set the Iran directory path (adjust if needed)\n",
    "IRAN_DIR = r\"C:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\"\n",
    "\n",
    "if not os.path.isdir(IRAN_DIR):\n",
    "    raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran' ×‘× ×ª×™×‘: {IRAN_DIR}\")\n",
    "\n",
    "# Base POIs directory (as per course structure)\n",
    "POIS_DIR = os.path.join(IRAN_DIR, \"POIs\")\n",
    "os.makedirs(POIS_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------- API config ----------------\n",
    "BASE_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "HEADERS = {\"User-Agent\": \"SCE-DataScience-FinalProject/1.0 (contact: student@example.com)\"}\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=0.8, status_forcelist=[403,429,500,502,503,504])\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def api_get(params):\n",
    "    \"\"\"Thin wrapper around MediaWiki API GET with a fallback UA on 403.\"\"\"\n",
    "    r = session.get(BASE_API, params=params, headers=HEADERS, timeout=30)\n",
    "    if r.status_code == 403:\n",
    "        time.sleep(1)\n",
    "        r = session.get(BASE_API, params=params, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def get_category_members(category_title, cmtype=\"page\"):\n",
    "    \"\"\"\n",
    "    Fetches up to all members of a category, paging through 'continue'.\n",
    "    cmtype can be \"page\" (actual pages) or \"subcat\" (subcategories).\n",
    "    \"\"\"\n",
    "    members, params = [], {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"categorymembers\",\n",
    "        \"cmtitle\": category_title,\n",
    "        \"cmlimit\": \"500\",\n",
    "        \"cmtype\": cmtype,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    while True:\n",
    "        data = api_get(params)\n",
    "        members.extend(data.get(\"query\", {}).get(\"categorymembers\", []))\n",
    "        if \"continue\" in data:\n",
    "            params[\"cmcontinue\"] = data[\"continue\"][\"cmcontinue\"]\n",
    "        else:\n",
    "            break\n",
    "    return members\n",
    "\n",
    "def collect_people_from_category(root_category):\n",
    "    \"\"\"\n",
    "    Breadth-first traversal from a root category:\n",
    "      - collects 'page' members as rows {Name, Wikipedia_Link}\n",
    "      - enqueues 'subcat' members for further traversal\n",
    "    Returns a deduplicated DataFrame by Name.\n",
    "    \"\"\"\n",
    "    seen, pages, queue = set(), [], [root_category]\n",
    "    while queue:\n",
    "        cat = queue.pop(0)\n",
    "        if cat in seen:\n",
    "            continue\n",
    "        seen.add(cat)\n",
    "        print(f\"ğŸ“‚ ×¡×•×¨×§: {cat}\")\n",
    "        # pages\n",
    "        for m in get_category_members(cat, \"page\"):\n",
    "            title = m[\"title\"]\n",
    "            link  = \"https://en.wikipedia.org/wiki/\" + quote(title.replace(\" \", \"_\"))\n",
    "            pages.append({\"Name\": title, \"Wikipedia_Link\": link})\n",
    "        # subcategories\n",
    "        for sc in get_category_members(cat, \"subcat\"):\n",
    "            queue.append(sc[\"title\"])\n",
    "        time.sleep(0.15)  # be polite\n",
    "    return pd.DataFrame(pages).drop_duplicates(subset=[\"Name\"]).reset_index(drop=True)\n",
    "\n",
    "def safe_slug(cat):\n",
    "    \"\"\"\n",
    "    Converts a category title to a folder/filename-safe slug.\n",
    "    Example:\n",
    "      \"Category:Iranian physicians\" -> \"iranian_physicians\"\n",
    "    \"\"\"\n",
    "    name = cat.replace(\"Category:\", \"\").strip()\n",
    "    name = name.replace(\" \", \"_\")\n",
    "    for bad in ['\"', \"'\", \"'\", \"\"\", \"\"\", \"(\", \")\", \"/\", \"\\\\\", \":\", \"*\", \"?\", \"<\", \">\", \"|\", \",\", \";\", \"â€”\", \"â€“\"]:\n",
    "        name = name.replace(bad, \"\")\n",
    "    # compress consecutive underscores\n",
    "    while \"__\" in name:\n",
    "        name = name.replace(\"__\", \"_\")\n",
    "    return name.lower()\n",
    "\n",
    "# ---------------- Categories to collect (expanded incl. healthcare) ----------------\n",
    "categories = [\n",
    "    # Existing:\n",
    "    \"Category:Government_ministers_of_Iran\",\n",
    "    \"Category:Presidents_of_Iran\",\n",
    "    \"Category:Vice_presidents_of_Iran\",\n",
    "    \"Category:Iranian_ayatollahs\",\n",
    "    \"Category:Iranian_actors\",\n",
    "    \"Category:Iranian_singers\",\n",
    "    \"Category:Iranian_scientists\",\n",
    "    \"Category:Iranian_economists\",\n",
    "    \"Category:Iranian_writers\",\n",
    "    \"Category:Iranian_football_managers\",\n",
    "\n",
    "    # NEW â€” Healthcare related:\n",
    "    \"Category:Hospitals_in_Iran\",\n",
    "    \"Category:Private_hospitals_in_Iran\",\n",
    "    \"Category:Teaching_hospitals_in_Iran\",\n",
    "    \"Category:Iranian_physicians\",\n",
    "    \"Category:Iranian_cardiologists\",\n",
    "    \"Category:Iranian_women_physicians\",\n",
    "    \"Category:21st-century_Iranian_physicians\",\n",
    "    \"Category:19th-century_Iranian_physicians\",\n",
    "    \"Category:Medical_and_health_organisations_based_in_Iran\",\n",
    "    \"Category:Healthcare_in_Iran\",\n",
    "    \"Category:Medicine_in_Iran\",\n",
    "\n",
    "    # NEW â€” Journalists and Media:\n",
    "    \"Category:Iranian_journalists\",\n",
    "    \"Category:Iranian_women_journalists\",\n",
    "    \"Category:Iranian_reporters\",\n",
    "    \"Category:Iranian_correspondents\",\n",
    "    \"Category:Iranian_editors\",\n",
    "    \"Category:Iranian_news_readers\",\n",
    "\n",
    "    # NEW â€” Activists and Political:\n",
    "    \"Category:Iranian_activists\",\n",
    "    \"Category:Iranian_political_activists\",\n",
    "    \"Category:Iranian_human_rights_activists\",\n",
    "    \"Category:Iranian_women_activists\",\n",
    "    \"Category:Iranian_feminists\",\n",
    "    \"Category:Iranian_dissidents\",\n",
    "    \"Category:Iranian_bloggers\",\n",
    "\n",
    "    # NEW â€” Sports:\n",
    "    \"Category:Iranian_footballers\",\n",
    "    \"Category:Iranian_athletes\",\n",
    "    \"Category:Iranian_wrestlers\",\n",
    "    \"Category:Iranian_taekwondo_practitioners\",\n",
    "    \"Category:Iranian_volleyball_players\",\n",
    "    \"Category:Iranian_weightlifters\",\n",
    "\n",
    "    # NEW â€” Entertainment:\n",
    "    \"Category:Iranian_television_actors\",\n",
    "    \"Category:Iranian_film_actors\",\n",
    "    \"Category:Iranian_pop_singers\",\n",
    "    \"Category:Iranian_rappers\",\n",
    "    \"Category:Iranian_comedians\",\n",
    "]\n",
    "\n",
    "# ---------------- Main loop: collect, save per-spec, summarize ----------------\n",
    "csv_files = []\n",
    "summary_rows = []\n",
    "\n",
    "for cat in categories:\n",
    "    try:\n",
    "        slug = safe_slug(cat)\n",
    "        cat_dir = os.path.join(POIS_DIR, slug)\n",
    "        os.makedirs(cat_dir, exist_ok=True)\n",
    "\n",
    "        csv_name = f\"{slug}_wikipedia.csv\"\n",
    "        csv_path = os.path.join(cat_dir, csv_name)\n",
    "\n",
    "        # Skip if file already exists and is not empty\n",
    "        if os.path.exists(csv_path) and os.path.getsize(csv_path) > 0:\n",
    "            print(f\"â© Skipping {slug}: file already exists.\")\n",
    "            summary_rows.append({\"Category\": cat, \"Slug\": slug, \"SavedRows\": \"SKIPPED\", \"CSV\": csv_path})\n",
    "            continue\n",
    "\n",
    "        df = collect_people_from_category(cat)\n",
    "        n = len(df)\n",
    "\n",
    "        if n == 0:\n",
    "            print(f\"âš ï¸ ××™×Ÿ × ×ª×•× ×™× ×¢×‘×•×¨ {cat}\")\n",
    "            summary_rows.append({\"Category\": cat, \"Slug\": slug, \"SavedRows\": 0, \"CSV\": None})\n",
    "            continue\n",
    "\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        csv_files.append(csv_path)\n",
    "\n",
    "        print(f\"âœ… × ×©××¨: {csv_path} | ×¨×©×•××•×ª: {n}\")\n",
    "        summary_rows.append({\"Category\": cat, \"Slug\": slug, \"SavedRows\": n, \"CSV\": csv_path})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘×§×˜×’×•×¨×™×” {cat}: {e}\")\n",
    "        summary_rows.append({\"Category\": cat, \"Slug\": safe_slug(cat), \"SavedRows\": 0, \"CSV\": None})\n",
    "\n",
    "# --- Summary table ---\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Add numeric column for sorting (convert SKIPPED to -1 for sorting purposes)\n",
    "summary_df['SavedRows_Numeric'] = summary_df['SavedRows'].apply(\n",
    "    lambda x: -1 if x == \"SKIPPED\" else (int(x) if isinstance(x, (int, float)) else 0)\n",
    ")\n",
    "\n",
    "# Sort by the numeric column and drop it\n",
    "summary_df = summary_df.sort_values(\"SavedRows_Numeric\", ascending=False).reset_index(drop=True)\n",
    "summary_df = summary_df.drop(columns=['SavedRows_Numeric'])\n",
    "\n",
    "# Calculate total (only numeric values)\n",
    "total_saved = sum(x for x in summary_df[\"SavedRows\"] if isinstance(x, (int, float)))\n",
    "\n",
    "print(\"\\n================= Summary =================\")\n",
    "for _, r in summary_df.iterrows():\n",
    "    base = os.path.basename(r['CSV']) if (pd.notna(r['CSV']) and r['CSV']) else \"-\"\n",
    "    saved_str = str(r['SavedRows'])\n",
    "    print(f\"â€¢ {r['Category']} -> {r['Slug']}: {saved_str} ×¨×©×•××•×ª  |  ×§×•×‘×¥: {base}\")\n",
    "print(f\"\\nğŸ“Š Total across files: {total_saved} rows\")\n",
    "print(\"===========================================\\n\")\n",
    "\n",
    "# Display summary table\n",
    "display(summary_df)\n",
    "\n",
    "# ---------------- ZIP all CSVs ----------------\n",
    "if csv_files:\n",
    "    zip_path = os.path.join(IRAN_DIR, \"Iran_POIs_Wikipedia_Categories.zip\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for file in csv_files:\n",
    "            # Save relative path in zip\n",
    "            arcname = os.path.relpath(file, start=IRAN_DIR)\n",
    "            zipf.write(file, arcname)\n",
    "    print(f\"ğŸ’¾ Created ZIP with all categories: {zip_path}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No CSV files were created â†’ no ZIP.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImWc_TSkWUSQ"
   },
   "source": [
    " **Step 3: Wikidata Enrichment for ALL POI Folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEId6T_TvQT7"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 3: Wikidata Enrichment for ALL POI Folders (UPDATED)\n",
    "# ============================================\n",
    "# For each subfolder under POIs:\n",
    "#  1) Locate *_wikipedia.csv (or the first .csv)\n",
    "#  2) Detect the Wikipedia link column (fallback to Name)\n",
    "#  3) Resolve wikidata_qid (+ wikidata_url)\n",
    "#  4) Fetch details via SPARQL in batches\n",
    "#  5) Save:\n",
    "#     a) *_with_wikidata_ids_and_links.csv\n",
    "#     b) *_with_wikidata_ids_and_links_wikidata_detailed.csv\n",
    "# ============================================\n",
    "\n",
    "import os, re, time, glob, json, math, random\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------- Dynamic path detection ----------------\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "ROOT_DIR = os.path.join(IRAN_DIR, \"POIs\")\n",
    "os.makedirs(ROOT_DIR, exist_ok=True)\n",
    "print(f\"ğŸ“ ROOT_DIR: {ROOT_DIR}\")\n",
    "\n",
    "# ---------------- HTTP / Endpoints ----------------\n",
    "UA = {\"User-Agent\": \"SCE-DS-FinalProject/1.0 (contact: student@example.com)\"}\n",
    "MEDIAWIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "SPARQL = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "# ---------------- Helpers: retries ----------------\n",
    "def http_get(url, params=None, headers=None, timeout=30, max_tries=4, sleep_base=0.8):\n",
    "    \"\"\"\n",
    "    Simple GET with retries/backoff. Jitters a bit to be polite.\n",
    "    \"\"\"\n",
    "    headers = headers or UA\n",
    "    for attempt in range(1, max_tries + 1):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "            if r.status_code == 403 and headers is UA:\n",
    "                # fallback UA\n",
    "                r = requests.get(url, params=params, headers={\"User-Agent\": \"Mozilla/5.0\"}, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            if attempt == max_tries:\n",
    "                # last attempt: raise\n",
    "                raise\n",
    "            sleep_s = sleep_base * attempt + random.uniform(0, 0.3)\n",
    "            time.sleep(sleep_s)\n",
    "    # Shouldn't reach here\n",
    "    raise RuntimeError(\"GET retries exhausted\")\n",
    "\n",
    "# ---------------- Column detection ----------------\n",
    "def find_wikipedia_column(df: pd.DataFrame) -> str:\n",
    "    # First pass: headers with hints\n",
    "    for name in df.columns:\n",
    "        n = str(name).strip().lower()\n",
    "        if any(k in n for k in [\"wikipedia\", \"wiki\", \"link\", \"url\"]):\n",
    "            return name\n",
    "    # Second pass: sample content\n",
    "    for name in df.columns:\n",
    "        sample = \" \".join(map(str, df[name].dropna().astype(str).head(20).tolist())).lower()\n",
    "        if \"wikipedia.org\" in sample or sample.startswith(\"http\"):\n",
    "            return name\n",
    "    # Fallback\n",
    "    if \"Wikipedia_Link\" in df.columns:\n",
    "        return \"Wikipedia_Link\"\n",
    "    # As a last resort: if Name exists, we'll construct enwiki URLs from it\n",
    "    if \"Name\" in df.columns:\n",
    "        return None  # signal to construct from Name\n",
    "    raise ValueError(\"×œ× ××¦××ª×™ ×¢××•×“×ª ×§×™×©×•×¨ ×œ×•×•×™×§×™×¤×“×™×”, ×•××™×Ÿ ×’× ×¢××•×“×ª 'Name' ×œ×‘× ×™×™×ª ×§×™×©×•×¨×™×.\")\n",
    "\n",
    "def name_to_enwiki_url(name: str) -> str:\n",
    "    from urllib.parse import quote\n",
    "    if not isinstance(name, str) or not name.strip():\n",
    "        return None\n",
    "    title = name.strip().replace(\" \", \"_\")\n",
    "    return f\"https://en.wikipedia.org/wiki/{quote(title)}\"\n",
    "\n",
    "def wikipedia_url_to_title(url: str) -> str | None:\n",
    "    if not isinstance(url, str) or not url:\n",
    "        return None\n",
    "    try:\n",
    "        url = url.split(\"?\")[0].split(\"#\")[0]\n",
    "        title = url.rstrip(\"/\").split(\"/\")[-1]\n",
    "        return title if title else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------- QID resolution (with cache) ----------------\n",
    "qid_cache = {}\n",
    "\n",
    "def get_qid_from_wikipedia_url(url: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Resolve QID for a single Wikipedia page using the pageprops endpoint.\n",
    "    \"\"\"\n",
    "    title = wikipedia_url_to_title(url)\n",
    "    if not title:\n",
    "        return None\n",
    "    if title in qid_cache:\n",
    "        return qid_cache[title]\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"prop\": \"pageprops\",\n",
    "        \"redirects\": 1,\n",
    "        \"titles\": title,\n",
    "    }\n",
    "    try:\n",
    "        r = http_get(MEDIAWIKI_API, params=params, headers=UA, timeout=25)\n",
    "        data = r.json()\n",
    "        page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "        qid = page.get(\"pageprops\", {}).get(\"wikibase_item\")\n",
    "        if qid:\n",
    "            qid_cache[title] = qid\n",
    "        return qid\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------------- SPARQL batch fetch ----------------\n",
    "def batch_fetch_wikidata_details(qids: list[str], batch_size: int = 50) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch properties for many QIDs via SPARQL using VALUES batching.\n",
    "    Returns dict: { QID: {fields...}, ... }\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # Normalize & unique\n",
    "    qids = [q for q in qids if isinstance(q, str) and q.strip()]\n",
    "    uniq = sorted(set(qids))\n",
    "    if not uniq:\n",
    "        return results\n",
    "\n",
    "    def run_batch(subset):\n",
    "        values = \" \".join(f\"wd:{q}\" for q in subset)\n",
    "        query = f\"\"\"\n",
    "        SELECT ?item ?genderLabel ?occupationLabel ?countryLabel ?placeOfBirthLabel ?dateOfBirth WHERE {{\n",
    "          VALUES ?item {{ {values} }}\n",
    "          OPTIONAL {{ ?item wdt:P21 ?gender. }}\n",
    "          OPTIONAL {{ ?item wdt:P106 ?occupation. }}\n",
    "          OPTIONAL {{ ?item wdt:P27 ?country. }}\n",
    "          OPTIONAL {{ ?item wdt:P19 ?placeOfBirth. }}\n",
    "          OPTIONAL {{ ?item wdt:P569 ?dateOfBirth. }}\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en,fa,en-gb\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        r = http_get(SPARQL, params={\"query\": query, \"format\": \"json\"}, headers=UA, timeout=45)\n",
    "        rows = r.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "        # collect multiple rows per item\n",
    "        tmp = {}\n",
    "        for b in rows:\n",
    "            uri = b.get(\"item\", {}).get(\"value\", \"\")\n",
    "            q = uri.rsplit(\"/\", 1)[-1] if uri else None\n",
    "            if not q:\n",
    "                continue\n",
    "            cur = tmp.setdefault(q, {\"wikidata_gender\": set(),\n",
    "                                     \"wikidata_occupation\": set(),\n",
    "                                     \"wikidata_country_of_citizenship\": set(),\n",
    "                                     \"wikidata_place_of_birth\": None,\n",
    "                                     \"wikidata_date_of_birth\": None})\n",
    "            if \"genderLabel\" in b:\n",
    "                cur[\"wikidata_gender\"].add(b[\"genderLabel\"][\"value\"])\n",
    "            if \"occupationLabel\" in b:\n",
    "                cur[\"wikidata_occupation\"].add(b[\"occupationLabel\"][\"value\"])\n",
    "            if \"countryLabel\" in b:\n",
    "                cur[\"wikidata_country_of_citizenship\"].add(b[\"countryLabel\"][\"value\"])\n",
    "            if \"placeOfBirthLabel\" in b and not cur[\"wikidata_place_of_birth\"]:\n",
    "                cur[\"wikidata_place_of_birth\"] = b[\"placeOfBirthLabel\"][\"value\"]\n",
    "            if \"dateOfBirth\" in b and not cur[\"wikidata_date_of_birth\"]:\n",
    "                cur[\"wikidata_date_of_birth\"] = b[\"dateOfBirth\"][\"value\"]\n",
    "        # flatten sets\n",
    "        for q, d in tmp.items():\n",
    "            results[q] = {\n",
    "                \"wikidata_gender\": \"; \".join(sorted(d[\"wikidata_gender\"])) or None,\n",
    "                \"wikidata_occupation\": \"; \".join(sorted(d[\"wikidata_occupation\"])) or None,\n",
    "                \"wikidata_country_of_citizenship\": \"; \".join(sorted(d[\"wikidata_country_of_citizenship\"])) or None,\n",
    "                \"wikidata_place_of_birth\": d[\"wikidata_place_of_birth\"],\n",
    "                \"wikidata_date_of_birth\": d[\"wikidata_date_of_birth\"],\n",
    "            }\n",
    "\n",
    "    for i in range(0, len(uniq), batch_size):\n",
    "        subset = uniq[i:i+batch_size]\n",
    "        # polite pacing\n",
    "        try:\n",
    "            run_batch(subset)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ SPARQL batch failed ({subset[0]}..): {e}. ×× ×¡×” ×©×•×‘ ×‘×§×‘×•×¦×•×ª ×§×˜× ×•×ª ×™×•×ª×¨...\")\n",
    "            # fallback: try half batch to circumvent transient errors\n",
    "            mid = len(subset)//2 or 1\n",
    "            for chunk in (subset[:mid], subset[mid:]):\n",
    "                try:\n",
    "                    run_batch(chunk)\n",
    "                except Exception as ee:\n",
    "                    print(f\"âŒ SPARQL sub-batch failed ({chunk[0]}..): {ee}\")\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return results\n",
    "\n",
    "# ---------------- Main processing ----------------\n",
    "folders = [f for f in sorted(os.listdir(ROOT_DIR)) if os.path.isdir(os.path.join(ROOT_DIR, f))]\n",
    "print(f\"ğŸ—‚ï¸ × ××¦××• {len(folders)} ×ª×™×§×™×•×ª POI ×œ×¢×™×‘×•×“.\")\n",
    "\n",
    "for folder in folders:\n",
    "    FOLDER_PATH = os.path.join(ROOT_DIR, folder)\n",
    "    print(f\"\\nğŸ“‚ ××¢×‘×“ ×ª×™×§×™×™×”: {folder}\")\n",
    "\n",
    "    # Locate input CSV (prefer *_wikipedia.csv)\n",
    "    candidates = sorted(glob.glob(os.path.join(FOLDER_PATH, \"*_wikipedia.csv\")))\n",
    "    if not candidates:\n",
    "        candidates = sorted(glob.glob(os.path.join(FOLDER_PATH, \"*.csv\")))\n",
    "    if not candidates:\n",
    "        print(\"âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\")\n",
    "        continue\n",
    "\n",
    "    INPUT_CSV = candidates[0]\n",
    "    base_name = re.sub(r\"\\.csv$\", \"\", os.path.basename(INPUT_CSV))\n",
    "    \n",
    "    # âš¡ SMART SKIP: Check if Wikidata enrichment already complete\n",
    "    detailed_path = os.path.join(FOLDER_PATH, f\"{base_name}_with_wikidata_ids_and_links_wikidata_detailed.csv\")\n",
    "    if os.path.exists(detailed_path):\n",
    "        try:\n",
    "            df_check = pd.read_csv(detailed_path, nrows=5)\n",
    "            wd_cols = [c for c in df_check.columns if c.startswith('wikidata_')]\n",
    "            if wd_cols and df_check[wd_cols].notna().any().any():\n",
    "                print(f\"âœ… SKIP: Wikidata enrichment already complete\")\n",
    "                print(f\"   File: {os.path.basename(detailed_path)}\")\n",
    "                print(f\"   Found {len(wd_cols)} Wikidata columns with data\")\n",
    "                continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    candidates = sorted(glob.glob(os.path.join(FOLDER_PATH, \"*_wikipedia.csv\")))\n",
    "    if not candidates:\n",
    "        candidates = sorted(glob.glob(os.path.join(FOLDER_PATH, \"*.csv\")))\n",
    "    if not candidates:\n",
    "        print(\"âš ï¸ ×œ× × ××¦× ×§×•×‘×¥ CSV ×‘×ª×™×§×™×™×” ×”×–×•, ××“×œ×’.\")\n",
    "        continue\n",
    "\n",
    "    INPUT_CSV = candidates[0]\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_CSV)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×œ× ×”×¦×œ×—×ª×™ ×œ×§×¨×•× ××ª ×”×§×•×‘×¥ {os.path.basename(INPUT_CSV)}: {e} â€” ××“×œ×’.\")\n",
    "        continue\n",
    "\n",
    "    # Detect / construct Wikipedia links\n",
    "    try:\n",
    "        wiki_col = find_wikipedia_column(df)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ {e} â€” ×× ×¡×” ×œ×‘× ×•×ª ×§×™×©×•×¨×™ ×•×™×§×™×¤×“×™×” ×Ö¾'Name'...\")\n",
    "        wiki_col = None\n",
    "\n",
    "    if wiki_col is None:\n",
    "        if \"Name\" in df.columns:\n",
    "            df[\"Wikipedia_Link\"] = df[\"Name\"].apply(name_to_enwiki_url)\n",
    "            wiki_col = \"Wikipedia_Link\"\n",
    "        else:\n",
    "            print(\"âŒ ××™×Ÿ ×¢××•×“×ª ×§×™×©×•×¨/Name â€” ××“×œ×’ ×¢×œ ×”×ª×™×§×™×™×”.\")\n",
    "            continue\n",
    "\n",
    "    print(\"ğŸ§© ×¢××•×“×ª ×•×™×§×™×¤×“×™×”:\", wiki_col)\n",
    "\n",
    "    # ---- A) Resolve QIDs (with simple in-memory cache) ----\n",
    "    qids = []\n",
    "    for url in df[wiki_col].fillna(\"\"):\n",
    "        qids.append(get_qid_from_wikipedia_url(url))\n",
    "        time.sleep(0.08)  # polite throttle\n",
    "\n",
    "    df[\"wikidata_qid\"] = qids\n",
    "    df[\"wikidata_url\"] = df[\"wikidata_qid\"].apply(lambda q: f\"https://www.wikidata.org/wiki/{q}\" if isinstance(q, str) and q else None)\n",
    "\n",
    "    # Save mid file\n",
    "    mid_path = os.path.join(\n",
    "        FOLDER_PATH,\n",
    "        re.sub(r\"\\.csv$\", \"\", os.path.basename(INPUT_CSV)) + \"_with_wikidata_ids_and_links.csv\"\n",
    "    )\n",
    "    try:\n",
    "        df.to_csv(mid_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"ğŸ’¾ × ×©××¨ ×‘×™× ×™×™×: {os.path.basename(mid_path)}  (×©×•×¨×•×ª: {len(df)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘×©××™×¨×ª ×§×•×‘×¥ ×‘×™× ×™×™×: {e}\")\n",
    "\n",
    "    # ---- B) Fetch details via SPARQL in batches ----\n",
    "    uniq_qids = [q for q in pd.Series(df[\"wikidata_qid\"]).dropna().astype(str).unique().tolist() if q]\n",
    "    details_map = batch_fetch_wikidata_details(uniq_qids, batch_size=50)\n",
    "\n",
    "    details_rows = []\n",
    "    for q in df[\"wikidata_qid\"]:\n",
    "        if isinstance(q, str) and q in details_map:\n",
    "            details_rows.append(details_map[q])\n",
    "        else:\n",
    "            details_rows.append({\n",
    "                \"wikidata_gender\": None,\n",
    "                \"wikidata_occupation\": None,\n",
    "                \"wikidata_country_of_citizenship\": None,\n",
    "                \"wikidata_place_of_birth\": None,\n",
    "                \"wikidata_date_of_birth\": None,\n",
    "            })\n",
    "\n",
    "    details_df = pd.DataFrame(details_rows)\n",
    "    enriched = pd.concat([df, details_df], axis=1)\n",
    "\n",
    "    out_path = os.path.join(\n",
    "        FOLDER_PATH,\n",
    "        re.sub(r\"\\.csv$\", \"\", os.path.basename(INPUT_CSV)) + \"_with_wikidata_ids_and_links_wikidata_detailed.csv\"\n",
    "    )\n",
    "    try:\n",
    "        enriched.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"âœ… × ×©××¨ ×¤×œ×˜ ×¡×•×¤×™: {os.path.basename(out_path)}  (×©×•×¨×•×ª: {len(enriched)})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©×’×™××” ×‘×©××™×¨×ª ×”×¤×œ×˜ ×”×¡×•×¤×™: {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ ×”×•×©×œ× ×¢×™×‘×•×“ ×œ×›×œ ×”×ª×™×§×™×•×ª!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Find Twitter handles for ALL POI folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsVDW3ZmG45R"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 4: Find Twitter handles for ALL POI folders\n",
    "# Adds only: Twitter_username + Twitter_url (no twitter_source)\n",
    "# ============================================\n",
    "\n",
    "import os, re, time, json, glob, requests, pandas as pd, random\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse, unquote\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# ---- Dynamic path detection ----\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "# ×™×¨×•×¥ ×¢×œ ×›×œ ×”×ª×™×§×™×•×ª ×ª×—×ª POIs\n",
    "REL_ROOT = \"POIs\"\n",
    "ROOT = os.path.join(IRAN_DIR, REL_ROOT)\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# ---------- HTTP session with retries ----------\n",
    "UA = {\"User-Agent\": \"SCE-DS-FinalProject/1.0 (contact: student@example.com)\"}\n",
    "session = requests.Session()\n",
    "session.headers.update(UA)\n",
    "\n",
    "# urllib3 v2: allowed_methods (not method_whitelist). ×¢×“×™×£ frozenset\n",
    "retry_cfg = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=0.6,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=frozenset({\"GET\"}),\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retry_cfg))\n",
    "session.mount(\"http://\", HTTPAdapter(max_retries=retry_cfg))\n",
    "\n",
    "SPARQL = \"https://query.wikidata.org/sparql\"\n",
    "MEDIAWIKI_API = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def pick_csv(folder):\n",
    "    for pat in [\"*_with_wikidata_ids_and_links_wikidata_detailed*.csv\",\n",
    "                \"*_with_wikidata_ids_and_links*.csv\",\n",
    "                \"*_wikipedia.csv\",\n",
    "                \"*.csv\"]:\n",
    "        cand = sorted(glob.glob(os.path.join(folder, pat)))\n",
    "        # ×¡× ×Ÿ ×§×‘×¦×™× ×©×›×‘×¨ ××›×™×œ×™× _with_twitter (××œ×” ×”× ×¤×œ×˜×™×, ×œ× ×§×œ×˜×™×)\n",
    "        cand = [c for c in cand if not c.endswith(\"_with_twitter.csv\")]\n",
    "        if cand: return cand[0]\n",
    "    return None\n",
    "\n",
    "def clean_handle(h):\n",
    "    if not isinstance(h, str):\n",
    "        return None\n",
    "    h = h.strip().lstrip(\"@\")\n",
    "    h = re.sub(r\"[/?#].*$\", \"\", h)\n",
    "    m = re.match(r\"^[A-Za-z0-9_]{1,15}$\", h)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def extract_handle_from_url(url):\n",
    "    \"\"\"\n",
    "    ××—×œ×¥ ×™×“×™×ª ××›×ª×•×‘×ª twitter/x ×× ×§×™×™××ª. ×œ×™× ×§×™× ××¡×•×’ /i/user/12345 ×œ× ××›×™×œ×™× ×™×“×™×ª => × ×—×–×™×¨ None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        u = url.split(\"?\")[0].split(\"#\")[0]\n",
    "        host = urlparse(u).netloc.lower()\n",
    "        if any(d in host for d in [\"twitter.com\",\"x.com\",\"mobile.twitter.com\",\"www.twitter.com\",\"www.x.com\"]):\n",
    "            parts = urlparse(u).path.strip(\"/\").split(\"/\")\n",
    "            if parts and parts[0].lower() not in {\"i\",\"intent\",\"share\",\"home\"}:\n",
    "                return clean_handle(parts[0])\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def build_twitter_url(handle):\n",
    "    if not isinstance(handle, str) or not handle.strip():\n",
    "        return None\n",
    "    return f\"https://x.com/{handle.strip().lstrip('@')}\"\n",
    "\n",
    "def find_wikipedia_column(df):\n",
    "    # ×¨××©×™×ª: ×œ×¤×™ ×©× ×¢××•×“×”\n",
    "    for c in df.columns:\n",
    "        n = str(c).lower()\n",
    "        if any(k in n for k in [\"wikipedia\", \"wiki\", \"link\", \"url\"]):\n",
    "            return c\n",
    "    # ×©× ×™×ª: ×œ×¤×™ ×ª×•×›×Ÿ\n",
    "    for c in df.columns:\n",
    "        vals = \" \".join(map(str, df[c].dropna().astype(str).head(15).tolist())).lower()\n",
    "        if \"wikipedia.org\" in vals or \"https://\" in vals or \"http://\" in vals:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def http_get(url, params=None, headers=None, timeout=30, tries=3, backoff=0.7):\n",
    "    headers = headers or UA\n",
    "    for attempt in range(1, tries+1):\n",
    "        try:\n",
    "            r = session.get(url, params=params, headers=headers, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            return r\n",
    "        except Exception as e:\n",
    "            if attempt == tries:\n",
    "                raise\n",
    "            time.sleep(backoff * attempt + random.uniform(0, 0.2))\n",
    "\n",
    "def guess_twitter_from_wiki(title_or_url):\n",
    "    \"\"\"\n",
    "    × ×¡×” ×œ×—×œ×¥ ×™×“×™×ª ××”×“×£ ×‘×•×•×™×§×™×¤×“×™×”:\n",
    "    - External links\n",
    "    - ×˜×§×¡×˜ ×”××§×•×¨ (wikitext) ×¢× regex\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(title_or_url, str) and title_or_url.startswith((\"http://\", \"https://\")):\n",
    "            title = unquote(title_or_url.split(\"?\")[0].split(\"#\")[0].rstrip(\"/\").split(\"/\")[-1])\n",
    "            title = title.replace(\"_\", \" \")\n",
    "        else:\n",
    "            title = str(title_or_url).strip().replace(\"_\", \" \")\n",
    "        if not title:\n",
    "            return None\n",
    "\n",
    "        r = http_get(MEDIAWIKI_API, params={\n",
    "            \"action\": \"parse\",\n",
    "            \"format\": \"json\",\n",
    "            \"page\": title,\n",
    "            \"prop\": \"externallinks|wikitext\",\n",
    "            \"redirects\": 1\n",
    "        }, timeout=30, tries=3)\n",
    "        data = r.json()\n",
    "\n",
    "        links = data.get(\"parse\", {}).get(\"externallinks\", []) or []\n",
    "        for ln in links:\n",
    "            h = extract_handle_from_url(ln)\n",
    "            if h:\n",
    "                return h\n",
    "\n",
    "        wt = data.get(\"parse\", {}).get(\"wikitext\", {}).get(\"*\", \"\")\n",
    "        # ×ª×•×¤×¡ ×’× twitter ×•×’× x.com\n",
    "        for m in re.finditer(r\"(?:https?://)?(?:www\\.)?(?:twitter|x)\\.com/([A-Za-z0-9_]{1,15})\", wt, re.I):\n",
    "            h = clean_handle(m.group(1))\n",
    "            if h:\n",
    "                return h\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def batch_p2002(qids):\n",
    "    \"\"\"\n",
    "    ××—×–×™×¨ ××¤×” { QID: handle or None } ×¢× retry.\n",
    "    \"\"\"\n",
    "    qids = [q for q in qids if isinstance(q, str) and q]\n",
    "    if not qids:\n",
    "        return {}\n",
    "    values = \" \".join(f\"(wd:{q})\" for q in qids)\n",
    "    q = f\"\"\"\n",
    "    SELECT ?item ?twitter WHERE {{\n",
    "      VALUES (?item) {{ {values} }}\n",
    "      OPTIONAL {{ ?item wdt:P2002 ?twitter. }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    # retry ×“×¨×š http_get\n",
    "    r = http_get(SPARQL, params={\"query\": q, \"format\": \"json\"}, timeout=45, tries=3, backoff=0.9)\n",
    "    rows = r.json().get(\"results\", {}).get(\"bindings\", [])\n",
    "    out = {}\n",
    "    for b in rows:\n",
    "        qid = b[\"item\"][\"value\"].rsplit(\"/\",1)[-1]\n",
    "        tw = b.get(\"twitter\", {}).get(\"value\")\n",
    "        out[qid] = clean_handle(tw) if tw else None\n",
    "    return out\n",
    "\n",
    "# cache (QID -> handle)\n",
    "cache_file = os.path.join(ROOT, \"_twitter_cache.json\")\n",
    "twitter_cache = {}\n",
    "if os.path.exists(cache_file):\n",
    "    try:\n",
    "        twitter_cache = json.load(open(cache_file, \"r\", encoding=\"utf-8\"))\n",
    "    except:\n",
    "        twitter_cache = {}\n",
    "\n",
    "# ---------- process all subfolders ----------\n",
    "folders = [os.path.join(ROOT, d) for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))]\n",
    "folders.sort()\n",
    "\n",
    "for FOLDER in folders:\n",
    "    in_csv = pick_csv(FOLDER)\n",
    "    if not in_csv:\n",
    "        print(f\"âš ï¸ ××™×Ÿ CSV ×‘×ª×™×§×™×™×” {os.path.basename(FOLDER)} â€” ×“×™×œ×•×’\")\n",
    "        continue\n",
    "\n",
    "    # âš¡ SMART SKIP: Check if Twitter enrichment already complete\n",
    "    # ×‘×“×™×§×ª ×›×œ ×”×¤×•×¨××˜×™× ×”××¤×©×¨×™×™× ×©×œ ×§×‘×¦×™ Twitter\n",
    "    base_name = os.path.splitext(os.path.basename(in_csv))[0]\n",
    "    possible_outputs = [\n",
    "        os.path.join(FOLDER, base_name + \"_with_twitter.csv\"),\n",
    "        os.path.join(FOLDER, base_name.replace(\"_wikidata_detailed\", \"\") + \"_with_twitter.csv\"),\n",
    "        # ×× ×”×§×•×‘×¥ × ×§×¨× *_detailed.csv, × ×¡×” ×’× ×œ×œ× detailed\n",
    "        os.path.join(FOLDER, base_name.replace(\"_detailed\", \"\") + \"_with_twitter.csv\")\n",
    "    ]\n",
    "    \n",
    "    skip = False\n",
    "    for out_path in possible_outputs:\n",
    "        if os.path.exists(out_path):\n",
    "            try:\n",
    "                df_check = pd.read_csv(out_path, nrows=5)\n",
    "                if 'Twitter_username' in df_check.columns:\n",
    "                    twitter_count = df_check['Twitter_username'].notna().sum()\n",
    "                    if twitter_count > 0:\n",
    "                        print(f\"\\nğŸ“‚ {os.path.basename(FOLDER)}\")\n",
    "                        print(f\"âœ… SKIP: Twitter enrichment already complete\")\n",
    "                        print(f\"   File: {os.path.basename(out_path)} ({twitter_count}+ handles)\")\n",
    "                        skip = True\n",
    "                        break\n",
    "            except Exception:\n",
    "                pass\n",
    "    \n",
    "    if skip:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nğŸ“‚ {os.path.basename(FOLDER)}\")\n",
    "    try:\n",
    "        df = pd.read_csv(in_csv)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×œ× ×”×¦×œ×—×ª×™ ×œ×§×¨×•× ××ª ×”×§×•×‘×¥: {e} â€” ×“×™×œ×•×’\")\n",
    "        continue\n",
    "\n",
    "    wiki_col = find_wikipedia_column(df)\n",
    "    if \"wikidata_qid\" not in df.columns:\n",
    "        df[\"wikidata_qid\"] = None\n",
    "\n",
    "    if \"Twitter_username\" not in df.columns:\n",
    "        df[\"Twitter_username\"] = None\n",
    "\n",
    "    # 1) Wikidata P2002 (with cache)\n",
    "    qids = [q for q in df[\"wikidata_qid\"].dropna().astype(str).unique() if q]\n",
    "    p2002_map = {}\n",
    "    # ×§×•×“× ××”-cache\n",
    "    for q in qids:\n",
    "        if q in twitter_cache:\n",
    "            p2002_map[q] = twitter_cache[q]\n",
    "    # ××” ×©×—×¡×¨ â€” ×œ×©××™×œ×ª× (×‘×§×‘×•×¦×•×ª)\n",
    "    to_query = [q for q in qids if q not in p2002_map]\n",
    "    for i in range(0, len(to_query), 60):\n",
    "        part = to_query[i:i+60]\n",
    "        try:\n",
    "            m = batch_p2002(part)\n",
    "            p2002_map.update(m)\n",
    "            # ×œ×¢×“×›×Ÿ cache ×¨×§ ×›×©×™×© ×™×“×™×ª (×›×“×™ ×œ× â€œ×œ×§×‘×¢â€ None)\n",
    "            twitter_cache.update({k: v for k, v in m.items() if v})\n",
    "            time.sleep(0.25)\n",
    "        except Exception as e:\n",
    "            print(\"âš ï¸ SPARQL batch failed:\", e)\n",
    "\n",
    "    # ×›×ª×™×‘×” ×œ×¤×™ QID\n",
    "    for idx, row in df.iterrows():\n",
    "        qid = row.get(\"wikidata_qid\")\n",
    "        if isinstance(qid, str) and qid in p2002_map and p2002_map[qid]:\n",
    "            if not df.at[idx, \"Twitter_username\"]:\n",
    "                df.at[idx, \"Twitter_username\"] = p2002_map[qid]\n",
    "\n",
    "    # 2) Wikipedia fallback\n",
    "    if wiki_col:\n",
    "        missing = df[\"Twitter_username\"].isna()\n",
    "        for idx, row in df[missing].iterrows():\n",
    "            url = row[wiki_col]\n",
    "            if isinstance(url, str) and url.strip():\n",
    "                h = guess_twitter_from_wiki(url)\n",
    "                if h:\n",
    "                    df.at[idx, \"Twitter_username\"] = h\n",
    "                    # ×× ×™×© ×’× QID â€” ×¢×“×›×Ÿ cache (× ×—×¡×•×š ×‘×”×¤×¢×œ×•×ª ×¢×ª×™×“×™×•×ª)\n",
    "                    qid = row.get(\"wikidata_qid\")\n",
    "                    if isinstance(qid, str) and qid:\n",
    "                        twitter_cache[qid] = h\n",
    "\n",
    "    # × ×™×§×•×™ + URL\n",
    "    df[\"Twitter_username\"] = df[\"Twitter_username\"].apply(lambda h: clean_handle(h) if isinstance(h, str) else None)\n",
    "    df[\"Twitter_url\"] = df[\"Twitter_username\"].apply(build_twitter_url)\n",
    "\n",
    "    # save (UTF-8-SIG to be Excel-friendly)\n",
    "    out_path = os.path.join(FOLDER, os.path.splitext(os.path.basename(in_csv))[0] + \"_with_twitter.csv\")\n",
    "    try:\n",
    "        df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"âœ… Saved: {os.path.basename(out_path)} | found {df['Twitter_username'].notna().sum()} / {len(df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ×©××™×¨×” × ×›×©×œ×”: {e}\")\n",
    "\n",
    "# save cache\n",
    "try:\n",
    "    with open(cache_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(twitter_cache, f, ensure_ascii=False, indent=2)\n",
    "    print(\"\\nğŸ’¾ cache saved:\", cache_file)\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ couldn't save cache:\", e)\n",
    "\n",
    "print(\"\\nğŸ‰ Done â€” Step 4 completed for all POI folders (username + url only).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwD-8mnAmj_z"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Build Manual_Search_POIs.csv from all *_with_twitter.csv\n",
    "# ×××œ× ××•×˜×•××˜×™×ª ××ª ×”×§×•×‘×¥ ×”××¨×›×–×™ ××›×œ ×”×ª×™×§×™×•×ª (××”×¢××•×“×” Twitter_username)\n",
    "# ×‘×”×ª×× ×œ×”× ×—×™×•×ª ×”×§×•×¨×¡: ×©× ×”×§×•×‘×¥ Manual_Search_POIs.csv ×•×©××™×¨×” ×ª×—×ª POIs/\n",
    "# ============================================\n",
    "\n",
    "import os, re, glob, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- Dynamic path detection ----\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "REL_ROOT = \"POIs\"\n",
    "ROOT = os.path.join(IRAN_DIR, REL_ROOT)\n",
    "assert os.path.isdir(ROOT), f\"âŒ ×œ× ×§×™×™××ª ×”×ª×™×§×™×™×”: {ROOT}\"\n",
    "print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# ×©× ×”×§×•×‘×¥ ×œ×¤×™ ×”×”× ×—×™×•×ª (×©×œ×‘ 5)\n",
    "OUTPUT_NAME = \"Manual_Search_POIs.csv\"\n",
    "OUTPUT_PATH = os.path.join(ROOT, OUTPUT_NAME)\n",
    "\n",
    "# -------- Keyword map (folder_slug -> keyword) --------\n",
    "# ×”×¢×¨×”: ×”×©××•×ª ×›××Ÿ ×‘-lowercase ×›×™ ×”×ª×™×§×™×•×ª × ×•×¦×¨×• ×¢× slug ×§×˜×Ÿ (safe_slug)\n",
    "KEYWORD_MAP = {\n",
    "    # Existing\n",
    "    \"government_ministers_of_iran\": \"Iran minister\",\n",
    "    \"presidents_of_iran\": \"Iran president\",\n",
    "    \"vice_presidents_of_iran\": \"Iran vice president\",\n",
    "    \"iranian_ayatollahs\": \"Iran ayatollah\",\n",
    "    \"iranian_actors\": \"Iran actor\",\n",
    "    \"iranian_singers\": \"Iran singer\",\n",
    "    \"iranian_scientists\": \"Iran scientist\",\n",
    "    \"iranian_economists\": \"Iran economist\",\n",
    "    \"iranian_writers\": \"Iran writer\",\n",
    "    \"iranian_football_managers\": \"Iran football manager\",\n",
    "\n",
    "    # Healthcare related\n",
    "    \"hospitals_in_iran\": \"Iran hospital\",\n",
    "    \"private_hospitals_in_iran\": \"Iran hospital\",\n",
    "    \"teaching_hospitals_in_iran\": \"Iran teaching hospital\",\n",
    "    \"iranian_physicians\": \"Iran physician\",\n",
    "    \"iranian_cardiologists\": \"Iran cardiologist\",\n",
    "    \"iranian_women_physicians\": \"Iran physician\",\n",
    "    \"21st-century_iranian_physicians\": \"Iran physician\",\n",
    "    \"19th-century_iranian_physicians\": \"Iran physician\",\n",
    "    \"medical_and_health_organisations_based_in_iran\": \"Iran health org\",\n",
    "    \"healthcare_in_iran\": \"Iran healthcare\",\n",
    "    \"medicine_in_iran\": \"Iran medicine\",\n",
    "}\n",
    "\n",
    "CENTURY_PREFIX = re.compile(r\"^\\d{1,2}(st|nd|rd|th)-century_\")\n",
    "\n",
    "def base_slug(folder_slug: str) -> str:\n",
    "    \"\"\"××¡×™×¨ ×§×™×“×•××ª ×©×œ ×××” ××”×¡×œ××’ (e.g., 19th-century_...)\"\"\"\n",
    "    return CENTURY_PREFIX.sub(\"\", folder_slug)\n",
    "\n",
    "def infer_keyword_from_folder(folder_slug: str) -> str:\n",
    "    \"\"\"××¤×™×§ ××™×œ×ª ×—×™×¤×•×© ×”×’×™×•× ×™×ª ××”×¡×œ××’, ×¢× ××¤×” ×™×“× ×™×ª ×œ×§×™×™×¡×™× ×—×©×•×‘×™×.\"\"\"\n",
    "    slug = folder_slug.lower().strip()\n",
    "    if slug in KEYWORD_MAP:\n",
    "        return KEYWORD_MAP[slug]\n",
    "    slug = base_slug(slug)\n",
    "    if slug in KEYWORD_MAP:\n",
    "        return KEYWORD_MAP[slug]\n",
    "    # ×›×œ×œ ×‘×¨×™×¨×ª ××—×“×œ: ×”×•×¨×“ \"iranian_\" ×•×”Ö¾s ×”×¡×•×¤×™×ª, ×•×”×—×œ×£ _ ×‘×¨×•×•×—\n",
    "    token = slug.replace(\"iranian_\", \"\").replace(\"_\", \" \").strip()\n",
    "    token = token[:-1] if token.endswith(\"s\") else token\n",
    "    return f\"Iran {token}\".strip()\n",
    "\n",
    "def detect_name_col(df: pd.DataFrame) -> str:\n",
    "    \"\"\"×××ª×¨ ×¢××•×“×ª ×©× ×¡×‘×™×¨×”.\"\"\"\n",
    "    for c in df.columns:\n",
    "        n = str(c).strip().lower()\n",
    "        if n in {\"name\", \"poi name\", \"poi_name\", \"full_name\", \"full name\"}:\n",
    "            return c\n",
    "    # fallback: ×”×¢××•×“×” ×”×¨××©×•× ×”\n",
    "    return df.columns[0]\n",
    "\n",
    "def detect_twitter_col(df: pd.DataFrame) -> str | None:\n",
    "    \"\"\"×××ª×¨ ×¢××•×“×ª Twitter_username (××• ×“×•××”).\"\"\"\n",
    "    for c in df.columns:\n",
    "        n = str(c).strip().lower()\n",
    "        if n in {\"twitter_username\", \"twitter\", \"handle\", \"username\"}:\n",
    "            return c\n",
    "    # ×—×¤×© ×¢××•×“×” ×©×™×© ×‘×” ×”×¨×‘×” ×¢×¨×›×™× ×©× ×¨××™× ×›××• ×™×“×™×•×ª\n",
    "    for c in df.columns:\n",
    "        vals = df[c].dropna().astype(str).head(20).tolist()\n",
    "        hits = sum(1 for v in vals if re.match(r\"^@?[A-Za-z0-9_]{1,15}$\", v))\n",
    "        if hits >= max(3, len(vals)//3):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def build_twitter_url(handle: str) -> str | None:\n",
    "    if not isinstance(handle, str) or not handle.strip():\n",
    "        return None\n",
    "    h = handle.strip()\n",
    "    h = h[1:] if h.startswith(\"@\") else h\n",
    "    return f\"https://x.com/{h}\"\n",
    "\n",
    "rows = []\n",
    "folders = [d for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))]\n",
    "folders.sort()\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(ROOT, folder)\n",
    "    # ×—×¤×© ××ª ×§×•×‘×¥ ×”×™×¢×“ ×©× ×•×¦×¨ ×‘×©×œ×‘ 4\n",
    "    cands = sorted(glob.glob(os.path.join(folder_path, \"*_with_twitter.csv\")))\n",
    "    if not cands:\n",
    "        continue\n",
    "    path = cands[0]\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "    except Exception as e:\n",
    "        print(\"âš ï¸ can't read:\", path, e)\n",
    "        continue\n",
    "\n",
    "    twitter_col = detect_twitter_col(df)\n",
    "    if not twitter_col:\n",
    "        continue\n",
    "\n",
    "    name_col = detect_name_col(df)\n",
    "    kw = infer_keyword_from_folder(folder)\n",
    "\n",
    "    sub = df[[name_col, twitter_col]].copy()\n",
    "    sub.rename(columns={name_col: \"poi_name\", twitter_col: \"Twitter_username\"}, inplace=True)\n",
    "\n",
    "    # × ×™×§×•×™, ×¡×™× ×•×Ÿ, ×•×”×©×œ××ª URL\n",
    "    sub[\"Twitter_username\"] = sub[\"Twitter_username\"].astype(str).str.strip()\n",
    "    sub = sub[sub[\"Twitter_username\"].str.len() > 0]\n",
    "    sub[\"Twitter_username\"] = sub[\"Twitter_username\"].str.lstrip(\"@\")\n",
    "    sub[\"Twitter_url\"] = sub[\"Twitter_username\"].apply(build_twitter_url)\n",
    "    sub[\"keyword\"] = kw\n",
    "    sub[\"source_folder\"] = folder  # ×××™×–×” ×§×˜×’×•×¨×™×” ×”×’×™×¢\n",
    "\n",
    "    rows.append(sub[[\"keyword\", \"poi_name\", \"Twitter_username\", \"Twitter_url\", \"source_folder\"]])\n",
    "\n",
    "# ××™×—×•×“ ×•×©××™×¨×”\n",
    "if rows:\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    # ×¡×™× ×•×Ÿ ×›×¤×™×œ×•×™×•×ª ×¢×œ ×‘×¡×™×¡ ×©×œ×™×©×™×™×” (×©× ××©×ª××© + ××™×œ×ª ×—×™×¤×•×© + ×©× POI)\n",
    "    out.drop_duplicates(subset=[\"Twitter_username\", \"keyword\", \"poi_name\"], inplace=True, ignore_index=True)\n",
    "    # ×•×¨×™×× ×˜ × ×•×¡×£: ×× ××•×ª×• handle ×”×•×¤×™×¢ ×‘×›××” ×ª×™×§×™×•×ª â€” × ×©××•×¨ ××ª ×”×”×•×¤×¢×” ×”×¨××©×•× ×” ×‘×œ×‘×“\n",
    "    out.sort_values([\"Twitter_username\", \"keyword\", \"poi_name\"], inplace=True, ignore_index=True)\n",
    "    out.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… Saved: {OUTPUT_PATH} | rows: {len(out)}\")\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(out.head(20))\n",
    "    except Exception:\n",
    "        pass\n",
    "else:\n",
    "    print(\"â„¹ï¸ ×œ× × ××¦××• *_with_twitter.csv ×‘×ª×™×§×™×•×ª, ××• ×©××™×Ÿ ×™×“×™×•×ª ×œ××œ×.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtQf4kgx6Z5Q"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Add \"Twitter_status\" column to mark suspended/invalid/missing users\n",
    "# Works on Manual_Search_POIs.csv\n",
    "# ============================================\n",
    "\n",
    "import os, re, pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# ---- Dynamic path detection ----\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "# ---- Config ----\n",
    "REL_ROOT  = \"POIs\"\n",
    "FILE_NAME = \"Manual_Search_POIs.csv\"   # <-- ××¢×•×“×›×Ÿ ×œ×©× ×”×§×•×‘×¥ ×”×—×“×©\n",
    "\n",
    "ROOT = os.path.join(IRAN_DIR, REL_ROOT)\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "path = os.path.join(ROOT, FILE_NAME)\n",
    "assert os.path.exists(path), f\"âŒ File not found: {path}\"\n",
    "print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# ---- Load ----\n",
    "try:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---- Helpers ----\n",
    "HANDLE_RE = re.compile(r\"^[A-Za-z0-9_]{1,15}$\")\n",
    "\n",
    "def classify_status(username: str) -> str:\n",
    "    if pd.isna(username):\n",
    "        return \"missing\"\n",
    "    s = str(username).strip().lstrip(\"@\")\n",
    "    if not s:\n",
    "        return \"missing\"\n",
    "    # ××¡×¤×¨ ×‘×œ×‘×“: ×œ×¢×™×ª×™× ××¦×‘×™×¢ ×¢×œ ×—×©×‘×•×Ÿ ××•×©×¢×”/××•××¨ ×œ××–×”×”\n",
    "    if s.isdigit():\n",
    "        return \"suspended\"\n",
    "    # ×™×“×™×ª ×œ× ×—×•×§×™×ª (××•×¨×š >15 ××• ×ª×•×•×™× ×œ× ×ª×§×™× ×™×/×¨×•×•×—×™×)\n",
    "    if (len(s) > 15) or (HANDLE_RE.fullmatch(s) is None):\n",
    "        return \"invalid\"\n",
    "    return \"active\"\n",
    "\n",
    "# ---- Compute & Save ----\n",
    "df[\"Twitter_status\"] = df[\"Twitter_username\"].apply(classify_status)\n",
    "\n",
    "df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… Updated file: {path} ({len(df)} rows)\")\n",
    "try:\n",
    "    display(df.head(20))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lEiA_7JbyFvl"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ğŸ§® Step 6 â€“ Enhanced Statistics & Visuals (UPDATED)\n",
    "# Builds robust stats table + coverage + charts (portable)\n",
    "# ============================================\n",
    "\n",
    "from IPython.display import display\n",
    "import os, glob, re, pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Dynamic path detection ----------\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "REL_ROOT     = \"POIs\"\n",
    "OUT_BASENAME = \"POI_statistics.csv\"             # basic\n",
    "OUT_ENHANCED = \"POI_statistics_enhanced.csv\"    # enhanced with coverage\n",
    "FIG_DIR      = \"figures\"\n",
    "\n",
    "ROOT = os.path.join(IRAN_DIR, REL_ROOT)\n",
    "os.makedirs(ROOT, exist_ok=True)\n",
    "print(\"ğŸ“ ROOT:\", ROOT)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def safe_read_csv(path, nrows=None):\n",
    "    for enc in (None, \"utf-8\", \"utf-8-sig\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, nrows=nrows, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # × ×™×¡×™×•×Ÿ ××—×¨×•×Ÿ ×¢× engine=python ×•-skip\n",
    "    try:\n",
    "        return pd.read_csv(path, nrows=nrows, engine=\"python\", encoding=\"utf-8-sig\", on_bad_lines=\"skip\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def is_wikipedia_csv(path):\n",
    "    \"\"\"Detect Wikipedia CSV by content (columns/values), not filename alone.\"\"\"\n",
    "    name = os.path.basename(path).lower()\n",
    "    if any(k in name for k in [\"wikidata\", \"with_twitter\"]):\n",
    "        return False\n",
    "    df = safe_read_csv(path, nrows=10)\n",
    "    if df is None or df.empty:\n",
    "        return False\n",
    "    cols = [str(c).strip().lower() for c in df.columns]\n",
    "    if any(any(k in c for k in [\"wikipedia\", \"wiki\", \"link\", \"url\"]) for c in cols):\n",
    "        return True\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            sample = \" \".join(map(str, df[c].dropna().astype(str).head(10).tolist())).lower()\n",
    "            if \"wikipedia.org\" in sample or sample.startswith(\"http\"):\n",
    "                return True\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def detect_twitter_col(df):\n",
    "    candidates = {\"twitter_username\",\"twitter user\",\"twitter_user\",\"twitter handle\",\"handle\",\"twitter\"}\n",
    "    for c in df.columns:\n",
    "        if str(c).strip().lower() in candidates:\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        norm = re.sub(r'[^a-z]', '', str(c).lower())\n",
    "        if \"twitter\" in norm and (\"username\" in norm or \"handle\" in norm or \"user\" in norm or \"name\" in norm):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def detect_qid_col(df):\n",
    "    for c in df.columns:\n",
    "        if str(c).strip().lower() in {\"wikidata_qid\",\"qid\",\"wikidata id\",\"wikidataid\"}:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ---------- Build basic stats ----------\n",
    "rows = []\n",
    "folders = sorted([d for d in os.listdir(ROOT) if os.path.isdir(os.path.join(ROOT, d))])\n",
    "\n",
    "for folder in folders:\n",
    "    fpath = os.path.join(ROOT, folder)\n",
    "    csvs = sorted(glob.glob(os.path.join(fpath, \"*.csv\")))\n",
    "    if not csvs:\n",
    "        continue\n",
    "\n",
    "    wiki_file = None\n",
    "    wikidata_file = None\n",
    "    twitter_file = None\n",
    "\n",
    "    # Twitter file by name\n",
    "    for p in csvs:\n",
    "        if re.search(r\"with_twitter\", os.path.basename(p), re.I):\n",
    "            twitter_file = p\n",
    "            break\n",
    "\n",
    "    # Wikidata: prefer detailed\n",
    "    for p in csvs:\n",
    "        if re.search(r\"wikidata.*detailed\", os.path.basename(p), re.I):\n",
    "            wikidata_file = p\n",
    "            break\n",
    "    if wikidata_file is None:\n",
    "        for p in csvs:\n",
    "            if re.search(r\"with_wikidata_ids_and_links\", os.path.basename(p), re.I):\n",
    "                wikidata_file = p\n",
    "                break\n",
    "\n",
    "    # Wikipedia by content (fallback to \"<folder>.csv\")\n",
    "    for p in csvs:\n",
    "        if is_wikipedia_csv(p):\n",
    "            wiki_file = p\n",
    "            break\n",
    "    if wiki_file is None:\n",
    "        fallback = os.path.join(fpath, f\"{folder}.csv\")\n",
    "        if os.path.exists(fallback):\n",
    "            wiki_file = fallback\n",
    "\n",
    "    wikipedia_count = 0\n",
    "    wikidata_count  = 0\n",
    "    twitter_count   = 0\n",
    "\n",
    "    # Wikipedia count\n",
    "    if wiki_file:\n",
    "        dfw = safe_read_csv(wiki_file)\n",
    "        if dfw is not None:\n",
    "            wikipedia_count = len(dfw)\n",
    "\n",
    "    # Wikidata count (count non-empty QIDs if column exists)\n",
    "    if wikidata_file:\n",
    "        dfd = safe_read_csv(wikidata_file)\n",
    "        if dfd is not None:\n",
    "            qcol = detect_qid_col(dfd)\n",
    "            wikidata_count = dfd[qcol].notna().sum() if qcol else len(dfd)\n",
    "\n",
    "    # Twitter count (valid handles only)\n",
    "    if twitter_file:\n",
    "        dft = safe_read_csv(twitter_file)\n",
    "        if dft is not None:\n",
    "            tw_col = detect_twitter_col(dft)\n",
    "            if tw_col:\n",
    "                s = dft[tw_col].astype(str).str.strip()\n",
    "                valid = s[(s != \"\") &\n",
    "                          (~s.str.fullmatch(r\"\\d+\")) &\n",
    "                          (~s.str.lower().isin({\"nan\",\"none\",\"null\",\"suspended\"}))]\n",
    "                twitter_count = valid.shape[0]\n",
    "\n",
    "    rows.append({\n",
    "        \"Category\": folder,\n",
    "        \"Wikipedia count\": wikipedia_count,\n",
    "        \"Wikidata count\": wikidata_count,\n",
    "        \"Twitter user count\": twitter_count\n",
    "    })\n",
    "\n",
    "basic = pd.DataFrame(rows).sort_values(\"Category\").reset_index(drop=True)\n",
    "basic_path = os.path.join(ROOT, OUT_BASENAME)\n",
    "basic.to_csv(basic_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… Saved basic table: {basic_path}\")\n",
    "display(basic)\n",
    "\n",
    "# ---------- Enhanced: coverage, gaps, totals ----------\n",
    "enh = basic.copy()\n",
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    enh[\"Wikidata coverage (%)\"] = (100 * enh[\"Wikidata count\"] / enh[\"Wikipedia count\"]).fillna(0).round(1)\n",
    "    enh[\"Twitter coverage (%)\"]  = (100 * enh[\"Twitter user count\"] / enh[\"Wikipedia count\"]).fillna(0).round(1)\n",
    "\n",
    "enh[\"Wikidata gap\"] = (enh[\"Wikipedia count\"] - enh[\"Wikidata count\"]).clip(lower=0)\n",
    "enh[\"Twitter gap\"]  = (enh[\"Wikipedia count\"] - enh[\"Twitter user count\"]).clip(lower=0)\n",
    "\n",
    "tot = pd.DataFrame([{\n",
    "    \"Category\": \"TOTAL\",\n",
    "    \"Wikipedia count\": enh[\"Wikipedia count\"].sum(),\n",
    "    \"Wikidata count\": enh[\"Wikidata count\"].sum(),\n",
    "    \"Twitter user count\": enh[\"Twitter user count\"].sum(),\n",
    "}])\n",
    "tot[\"Wikidata coverage (%)\"] = (100 * tot[\"Wikidata count\"] / tot[\"Wikipedia count\"]).round(1) if tot[\"Wikipedia count\"].iloc[0] else 0.0\n",
    "tot[\"Twitter coverage (%)\"]  = (100 * tot[\"Twitter user count\"] / tot[\"Wikipedia count\"]).round(1) if tot[\"Wikipedia count\"].iloc[0] else 0.0\n",
    "tot[\"Wikidata gap\"] = tot[\"Wikipedia count\"] - tot[\"Wikidata count\"]\n",
    "tot[\"Twitter gap\"]  = tot[\"Wikipedia count\"] - tot[\"Twitter user count\"]\n",
    "\n",
    "enhanced = pd.concat([enh, tot], ignore_index=True)\n",
    "enhanced_path = os.path.join(ROOT, OUT_ENHANCED)\n",
    "enhanced.to_csv(enhanced_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… Saved enhanced table: {enhanced_path}\")\n",
    "display(enhanced)\n",
    "\n",
    "# ---------- Charts ----------\n",
    "fig_dir = os.path.join(ROOT, FIG_DIR)\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# ××¡× × ×™× ××ª TOTAL ×¢×‘×•×¨ ×’×¨×¤×™× ×¤×¨-×§×˜×’×•×¨×™×”\n",
    "per_cat = enhanced[enhanced[\"Category\"] != \"TOTAL\"].reset_index(drop=True)\n",
    "\n",
    "# 1) Counts by category (lines) â€” ×œ×œ× TOTAL\n",
    "plt.figure(figsize=(12,5))\n",
    "for col in [\"Wikipedia count\", \"Wikidata count\", \"Twitter user count\"]:\n",
    "    plt.plot(per_cat[\"Category\"], per_cat[col], marker='o', label=col)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"POIs â€“ Counts by Source\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig1_path = os.path.join(fig_dir, \"counts_by_category.png\")\n",
    "plt.savefig(fig1_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ Saved: {fig1_path}\")\n",
    "\n",
    "# 2) Coverage by category (%) â€” ×œ×œ× TOTAL\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(per_cat[\"Category\"], per_cat[\"Wikidata coverage (%)\"], alpha=0.8, label=\"Wikidata coverage (%)\")\n",
    "plt.bar(per_cat[\"Category\"], per_cat[\"Twitter coverage (%)\"], alpha=0.6, label=\"Twitter coverage (%)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"POIs â€“ Coverage by Category (%)\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Coverage (%)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig2_path = os.path.join(fig_dir, \"coverage_by_category.png\")\n",
    "plt.savefig(fig2_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ Saved: {fig2_path}\")\n",
    "\n",
    "# 3) Stacked bars: gaps to Wikipedia â€” ×œ×œ× TOTAL\n",
    "plt.figure(figsize=(12,5))\n",
    "bar_x = range(len(per_cat))\n",
    "plt.bar(bar_x, per_cat[\"Wikidata count\"], label=\"Wikidata count\")\n",
    "plt.bar(bar_x, per_cat[\"Wikidata gap\"], bottom=per_cat[\"Wikidata count\"], label=\"Wikidata gap\")\n",
    "plt.xticks(bar_x, per_cat[\"Category\"], rotation=45, ha='right')\n",
    "plt.title(\"Wikidata Count + Gap to Wikipedia\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Entities\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "fig3_path = os.path.join(fig_dir, \"wikidata_gap_stacked.png\")\n",
    "plt.savefig(fig3_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"ğŸ“ˆ Saved: {fig3_path}\")\n",
    "\n",
    "print(\"\\nâœ… Done. Files written to:\")\n",
    "print(f\"- {basic_path}\")\n",
    "print(f\"- {enhanced_path}\")\n",
    "print(f\"- {fig1_path}\")\n",
    "print(f\"- {fig2_path}\")\n",
    "print(f\"- {fig3_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oE4jOKaKvrH3"
   },
   "source": [
    "**API Twitter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYMVJXn7QUPq"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 10 â€” Candidates user metadata (built on your twitter_selenium scraper)\n",
    "# Output: /content/drive/MyDrive/Iran/POIs/Candidates/Candidates_user_data.csv\n",
    "# ============================================\n",
    "\n",
    "# --- ×× ×¦×¨×™×š ×”×ª×§× ×•×ª ×‘×¡×™×¡ (×‘×˜×œ ×”×¢×¨×” ×•×”×¨×™×¥ ×¤×¢× ××—×ª) ---\n",
    "# !apt-get update -y && apt-get install -y chromium-browser chromium-chromedriver\n",
    "# !pip install -U selenium tqdm pandas\n",
    "\n",
    "# ---------- CONFIG (Dynamic path detection) ----------\n",
    "import os, json, time, shutil, pandas as pd, sys, importlib.util, re\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Auto-detect project root\n",
    "IRAN_DIR = os.getcwd()\n",
    "if not os.path.basename(IRAN_DIR) == 'Iran':\n",
    "    for parent in Path(IRAN_DIR).parents:\n",
    "        if parent.name == 'Iran' and os.path.isdir(os.path.join(parent, 'POIs')):\n",
    "            IRAN_DIR = str(parent)\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"âŒ ×œ× × ××¦××” ×ª×™×§×™×™×ª 'Iran'. × × ×œ×”×¨×™×¥ ××ª×•×š ×ª×™×§×™×™×ª ×”×¤×¨×•×™×§×˜.\")\n",
    "\n",
    "BASE_DRIVE_DIR = os.path.join(IRAN_DIR, 'POIs')\n",
    "CAND_DIR       = os.path.join(BASE_DRIVE_DIR, 'Candidates')\n",
    "\n",
    "USERS_CSV      = os.path.join(BASE_DRIVE_DIR, 'POI_twitter_users_data.csv')     # ×§×œ×˜: ×”-POIs ×”××§×•×¨×™×™× (×¢××•×“×”: username)\n",
    "CONN_CSV       = os.path.join(CAND_DIR, 'POIs_candidate_connections.csv')       # ×§×œ×˜: ×©×œ×‘ 9 (×¢××•×“×•×ª: target_username, other_username)\n",
    "OUT_CSV        = os.path.join(CAND_DIR, 'Candidates_user_data.csv')             # ×¤×œ×˜: ×©×œ×‘ 10\n",
    "\n",
    "# ğŸ”¹ ×§×•×‘×¥ ×”-scraper ×©×œ×š\n",
    "SCRAPER_PATH = os.path.join(BASE_DRIVE_DIR, 'tools', 'twitter_selenium.py')\n",
    "\n",
    "# ×©×œ×™×˜×” ×‘×”×¨×¦×”\n",
    "SAMPLE_LIMIT      = 4000     # â† ×‘×“×™×§×ª ×¢×©×Ÿ ×¢×œ 10; ×©× ×” ×œ-None ×›×“×™ ×œ×¨×•×¥ ×¢×œ ×›×•×œ×\n",
    "BATCH_SIZE        = 40      # ×›××” ××©×ª××©×™× ×‘×›×œ ×× ×”\n",
    "RETRIES_PER_USER  = 2       # × ×™×¡×™×•× ×•×ª ×œ×›×œ ××©×ª××©\n",
    "OPEN_PAUSE        = 1.4     # ×©× ×™×•×ª ×”××ª× ×” ××—×¨×™ ×¤×ª×™×—×ª ×¤×¨×•×¤×™×œ\n",
    "\n",
    "# ××™×¤×•×™ ×¢××•×“×•×ª: ××”×¡×§×¨×•×œ×¨ ×©×œ×š ××œ ×”×¡×›×™××” ×”×ª×§× ×™×ª ×©×œ ×”×¤×¨×•×™×§×˜\n",
    "COLUMNS = [\n",
    "    \"username\",\"Full_Name\",\"Bio\",\"Location\",\"External_URL\",\n",
    "    \"Followers_Count\",\"Following_Count\",\"Is_Verified\",\"Is_Protected\",\n",
    "    \"Joined_Date\",\"Profile_URL\",\"Profile_Image\"\n",
    "]\n",
    "\n",
    "Path(CAND_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"ğŸ“ Working directory: {IRAN_DIR}\")\n",
    "\n",
    "# ---------- LOAD YOUR SCRAPER MODULE ----------\n",
    "def load_scraper(scraper_path: str):\n",
    "    if not os.path.exists(scraper_path):\n",
    "        raise FileNotFoundError(f\"twitter_selenium.py not found at: {scraper_path}\")\n",
    "    spec = importlib.util.spec_from_file_location(\"twitter_selenium\", scraper_path)\n",
    "    mod  = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"twitter_selenium\"] = mod\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "ts = load_scraper(SCRAPER_PATH)  # ××›×™×œ scrape_twitter_profile(username)\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def normalize_row_from_scraper(d: dict):\n",
    "    \"\"\"\n",
    "    ×××¤×” ××ª ×¤×œ×˜ ×”×¡×§×¨×•×œ×¨ ×©×œ×š ×œ×©×“×•×ª ×©×œ ×”×¤×¨×•×™×§×˜.\n",
    "    scraper keys (your file): user_name, name, bio, location, url, joined_date,\n",
    "                              followers, following, verified, profile_image, profile_url\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"username\":       (d.get(\"user_name\") or \"\").lstrip(\"@\"),\n",
    "        \"Full_Name\":      d.get(\"name\"),\n",
    "        \"Bio\":            d.get(\"bio\"),\n",
    "        \"Location\":       d.get(\"location\"),\n",
    "        \"External_URL\":   d.get(\"url\"),\n",
    "        \"Followers_Count\":d.get(\"followers\"),\n",
    "        \"Following_Count\":d.get(\"following\"),\n",
    "        \"Is_Verified\":    bool(d.get(\"verified\")),\n",
    "        \"Is_Protected\":   None,  # ×”×¡×§×¨×•×œ×¨ ×©×œ×š ×œ× ××—×œ×¥; ××¤×©×¨ ×œ×”×©××™×¨ None (×œ× × ×“×¨×© ×‘×¡×¢×™×£)\n",
    "        \"Joined_Date\":    d.get(\"joined_date\"),\n",
    "        \"Profile_URL\":    d.get(\"profile_url\"),\n",
    "        \"Profile_Image\":  d.get(\"profile_image\")\n",
    "    }\n",
    "\n",
    "# ---------- BUILD USER SET (×©×œ×‘ 9 + ×”××§×•×¨, ×œ×œ× ×›×¤×™×œ×•×™×•×ª) ----------\n",
    "user_set = set()\n",
    "\n",
    "if os.path.exists(CONN_CSV):\n",
    "    df_conn = pd.read_csv(CONN_CSV)\n",
    "    if \"target_username\" in df_conn.columns:\n",
    "        user_set.update(df_conn[\"target_username\"].dropna().astype(str).str.replace(\"@\",\"\", regex=False).str.strip())\n",
    "    if \"other_username\" in df_conn.columns:\n",
    "        user_set.update(df_conn[\"other_username\"].dropna().astype(str).str.replace(\"@\",\"\", regex=False).str.strip())\n",
    "\n",
    "if os.path.exists(USERS_CSV):\n",
    "    df_users = pd.read_csv(USERS_CSV)\n",
    "    if \"username\" in df_users.columns:\n",
    "        user_set.update(df_users[\"username\"].dropna().astype(str).str.replace(\"@\",\"\", regex=False).str.strip())\n",
    "\n",
    "users_all = [u for u in dict.fromkeys([u for u in user_set if u])]  # ×™×™×—×•×“ + ×©××™×¨×ª ×¡×“×¨\n",
    "if SAMPLE_LIMIT:\n",
    "    users_all = users_all[:SAMPLE_LIMIT]\n",
    "\n",
    "print(f\"Users to enrich: {len(users_all)} â†’ {users_all[:5]}\")\n",
    "\n",
    "# ---------- HEADER BOOTSTRAP (××‘×˜×™×— ×›×•×ª×¨×•×ª) ----------\n",
    "if not os.path.exists(OUT_CSV):\n",
    "    pd.DataFrame(columns=COLUMNS).to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- RESUME (×“×œ×’ ×¢×œ ××™ ×©×›×‘×¨ × ×©××¨) ----------\n",
    "done = set()\n",
    "try:\n",
    "    prev = pd.read_csv(OUT_CSV, usecols=[\"username\"])\n",
    "    done = set(prev[\"username\"].dropna().astype(str))\n",
    "    print(f\"â†» Resume: skipping {len(done)} already saved.\")\n",
    "except Exception as e:\n",
    "    print(\"Resume read issue:\", e)\n",
    "\n",
    "# ---------- RUN (×‘×× ×•×ª) ----------\n",
    "from math import ceil\n",
    "total   = len(users_all)\n",
    "batches = ceil(total / BATCH_SIZE)\n",
    "idx = 0\n",
    "\n",
    "for b in range(batches):\n",
    "    chunk = [u for u in users_all[idx: idx+BATCH_SIZE] if u not in done]\n",
    "    idx += BATCH_SIZE\n",
    "    if not chunk:\n",
    "        continue\n",
    "\n",
    "    with tqdm(total=len(chunk), desc=f\"Batch {b+1}/{batches}\", unit=\"user\") as pbar:\n",
    "        for uname in chunk:\n",
    "            row = None\n",
    "            # × ×™×¡×™×•× ×•×ª ×—×•×–×¨×™× ×œ×›×œ ××©×ª××© (×”×¡×§×¨×•×œ×¨ ×™×¤×ª×—/×™×¡×’×•×¨ ×›×¨×•× ×‘×¢×¦××•)\n",
    "            for attempt in range(RETRIES_PER_USER + 1):\n",
    "                try:\n",
    "                    raw = ts.scrape_twitter_profile(uname)   # âš ï¸ ××ª×•×š twitter_selenium.py ×©×œ×š\n",
    "                    row = normalize_row_from_scraper(raw)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    if attempt < RETRIES_PER_USER:\n",
    "                        time.sleep(0.8)\n",
    "                    else:\n",
    "                        # ×‘××§×¨×” ×›×©×œ â€” × ×©××•×¨ ×©×•×¨×” \"×¨×™×§×”\" ×¢× ×”×©× ×•×”-URL ×‘×œ×‘×“, ×›×“×™ ×œ× ×œ×—×¡×•× ×”×ª×§×“××•×ª\n",
    "                        row = {\n",
    "                            \"username\": uname, \"Full_Name\": None, \"Bio\": None, \"Location\": None,\n",
    "                            \"External_URL\": None, \"Followers_Count\": None, \"Following_Count\": None,\n",
    "                            \"Is_Verified\": None, \"Is_Protected\": None, \"Joined_Date\": None,\n",
    "                            \"Profile_URL\": f\"https://twitter.com/{uname}\", \"Profile_Image\": None\n",
    "                        }\n",
    "\n",
    "            pd.DataFrame([row], columns=COLUMNS).to_csv(\n",
    "                OUT_CSV, mode=\"a\", header=False, index=False, encoding=\"utf-8-sig\"\n",
    "            )\n",
    "            done.add(uname)\n",
    "            pbar.update(1)\n",
    "\n",
    "# ---------- ×“×”-×“×•×¤×œ×™×§×¦×™×” (×œ×™×ª×¨ ×‘×™×˜×—×•×Ÿ) ----------\n",
    "try:\n",
    "    df_out = pd.read_csv(OUT_CSV)\n",
    "    df_out = df_out.drop_duplicates(subset=[\"username\"], keep=\"first\")\n",
    "    df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"âœ… Saved:\", OUT_CSV)\n",
    "    print(df_out.head())\n",
    "except Exception as e:\n",
    "    print(\"Compact error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 â€“ POI Statistics Summary\n",
    "Generate comprehensive statistics for all POI categories: Wikipedia count, Wikidata count, and Twitter count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 6 â€“ POI Statistics Summary (COMPLETE)\n",
    "# Counts UNIQUE Wikipedia, Wikidata, and Twitter entries per category\n",
    "# Includes: Statistics table, Bar chart, and Venn diagram\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "\n",
    "# ---------- Auto-detect Iran project root ----------\n",
    "def find_iran_root():\n",
    "    \"\"\"Find the Iran project root (folder named 'Iran' with 'POIs' subfolder)\"\"\"\n",
    "    current = Path.cwd()\n",
    "    \n",
    "    # Check current directory\n",
    "    if current.name == 'Iran' and (current / 'POIs').is_dir():\n",
    "        return str(current)\n",
    "    \n",
    "    # Check parent directories\n",
    "    for parent in current.parents:\n",
    "        if parent.name == 'Iran' and (parent / 'POIs').is_dir():\n",
    "            return str(parent)\n",
    "    \n",
    "    raise FileNotFoundError(\"âŒ Could not find Iran project root (folder named 'Iran' with 'POIs' subfolder)\")\n",
    "\n",
    "IRAN_DIR = find_iran_root()\n",
    "POIS_DIR = os.path.join(IRAN_DIR, \"POIs\")\n",
    "print(f\"âœ… Found Iran project root: {IRAN_DIR}\")\n",
    "print(f\"ğŸ“ POIs directory: {POIS_DIR}\\n\")\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def safe_read_csv(filepath):\n",
    "    \"\"\"Safely read CSV with multiple encoding attempts\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return None\n",
    "    \n",
    "    for encoding in ['utf-8-sig', 'utf-8', 'latin-1']:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, encoding=encoding)\n",
    "            return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    print(f\"âš ï¸  Could not read: {os.path.basename(filepath)}\")\n",
    "    return None\n",
    "\n",
    "def find_file_by_pattern(folder_path, patterns):\n",
    "    \"\"\"Find first file matching any of the given patterns\"\"\"\n",
    "    for pattern in patterns:\n",
    "        matches = list(Path(folder_path).glob(pattern))\n",
    "        if matches:\n",
    "            return str(matches[0])\n",
    "    return None\n",
    "\n",
    "def find_name_column(df):\n",
    "    \"\"\"Find the main name/title column in a DataFrame\"\"\"\n",
    "    priority_cols = ['name', 'title', 'poi_name', 'person_name', 'entity_name']\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    \n",
    "    for priority in priority_cols:\n",
    "        if priority in cols_lower:\n",
    "            return cols_lower[priority]\n",
    "    \n",
    "    # Return first column as fallback\n",
    "    return df.columns[0] if len(df.columns) > 0 else None\n",
    "\n",
    "def find_twitter_column(df):\n",
    "    \"\"\"Find Twitter username column (case-insensitive)\"\"\"\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'twitter' in col_lower and ('username' in col_lower or 'user' in col_lower or 'handle' in col_lower):\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "def find_qid_column(df):\n",
    "    \"\"\"Find Wikidata QID column (case-insensitive)\"\"\"\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if 'wikidata' in col_lower and 'qid' in col_lower:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "# ---------- Process all POIs folders ----------\n",
    "results = []\n",
    "all_entities = []  # For Venn diagram\n",
    "\n",
    "folders = sorted([f for f in os.listdir(POIS_DIR) \n",
    "                  if os.path.isdir(os.path.join(POIS_DIR, f)) \n",
    "                  and not f.startswith('.')\n",
    "                  and f not in ['figures', 'tools', 'Candidates']])\n",
    "\n",
    "print(f\"ğŸ” Processing {len(folders)} categories...\\n\")\n",
    "\n",
    "for idx, folder_name in enumerate(folders, 1):\n",
    "    folder_path = os.path.join(POIS_DIR, folder_name)\n",
    "    print(f\"[{idx}/{len(folders)}] Processing: {folder_name}\")\n",
    "    \n",
    "    # Detect files\n",
    "    wikipedia_file = find_file_by_pattern(folder_path, [\n",
    "        f\"{folder_name}_wikipedia.csv\",\n",
    "        \"*_wikipedia.csv\",\n",
    "        f\"{folder_name}.csv\"\n",
    "    ])\n",
    "    \n",
    "    wikidata_file = find_file_by_pattern(folder_path, [\n",
    "        \"*_wikidata_detailed.csv\",\n",
    "        \"*_with_wikidata_ids_and_links_wikidata_detailed.csv\",\n",
    "        \"*_with_wikidata*.csv\"\n",
    "    ])\n",
    "    \n",
    "    twitter_file = find_file_by_pattern(folder_path, [\n",
    "        \"*_with_twitter.csv\",\n",
    "        \"*twitter*.csv\"\n",
    "    ])\n",
    "    \n",
    "    # Track entities for this category\n",
    "    category_entities = set()\n",
    "    \n",
    "    # ========== 1. Count UNIQUE Wikipedia entries ==========\n",
    "    wikipedia_count = 0\n",
    "    if wikipedia_file:\n",
    "        df_wiki = safe_read_csv(wikipedia_file)\n",
    "        if df_wiki is not None:\n",
    "            name_col = find_name_column(df_wiki)\n",
    "            if name_col:\n",
    "                # Remove duplicates by name\n",
    "                unique_names = df_wiki[name_col].dropna().drop_duplicates()\n",
    "                wikipedia_count = len(unique_names)\n",
    "                category_entities.update(unique_names.str.lower().str.strip())\n",
    "            else:\n",
    "                wikipedia_count = len(df_wiki)\n",
    "            print(f\"   ğŸ“– Wikipedia: {wikipedia_count} unique entries\")\n",
    "    \n",
    "    # ========== 2. Count UNIQUE Wikidata entries ==========\n",
    "    wikidata_count = 0\n",
    "    if wikidata_file:\n",
    "        df_wikidata = safe_read_csv(wikidata_file)\n",
    "        if df_wikidata is not None:\n",
    "            qid_col = find_qid_column(df_wikidata)\n",
    "            if qid_col:\n",
    "                # Count unique non-null QIDs\n",
    "                unique_qids = df_wikidata[qid_col].dropna().drop_duplicates()\n",
    "                wikidata_count = len(unique_qids)\n",
    "            else:\n",
    "                # No QID column found, use unique names\n",
    "                name_col = find_name_column(df_wikidata)\n",
    "                if name_col:\n",
    "                    unique_names = df_wikidata[name_col].dropna().drop_duplicates()\n",
    "                    wikidata_count = len(unique_names)\n",
    "                else:\n",
    "                    wikidata_count = len(df_wikidata)\n",
    "            print(f\"   ğŸ”— Wikidata: {wikidata_count} unique entries\")\n",
    "    \n",
    "    # ========== 3. Count UNIQUE Twitter entries ==========\n",
    "    twitter_count = 0\n",
    "    if twitter_file:\n",
    "        df_twitter = safe_read_csv(twitter_file)\n",
    "        if df_twitter is not None:\n",
    "            twitter_col = find_twitter_column(df_twitter)\n",
    "            if twitter_col:\n",
    "                # Remove duplicates by Twitter username\n",
    "                valid_twitter = df_twitter[twitter_col].dropna()\n",
    "                valid_twitter = valid_twitter[valid_twitter.astype(str).str.strip() != '']\n",
    "                valid_twitter = valid_twitter[~valid_twitter.astype(str).str.lower().isin(['nan', 'none', 'null', 'suspended'])]\n",
    "                unique_twitter = valid_twitter.drop_duplicates()\n",
    "                twitter_count = len(unique_twitter)\n",
    "                print(f\"   ğŸ¦ Twitter: {twitter_count} unique users\")\n",
    "    \n",
    "    # Store results for this category\n",
    "    results.append({\n",
    "        'folder': folder_name,\n",
    "        'category_name': folder_name.replace('_', ' ').title(),\n",
    "        'wikipedia_count': wikipedia_count,\n",
    "        'wikidata_count': wikidata_count,\n",
    "        'twitter_count': twitter_count\n",
    "    })\n",
    "    \n",
    "    # Add to global entities list for Venn diagram\n",
    "    all_entities.append({\n",
    "        'has_wikipedia': wikipedia_count > 0,\n",
    "        'has_wikidata': wikidata_count > 0,\n",
    "        'has_twitter': twitter_count > 0,\n",
    "        'wiki_count': wikipedia_count,\n",
    "        'wd_count': wikidata_count,\n",
    "        'tw_count': twitter_count\n",
    "    })\n",
    "\n",
    "# ---------- Create summary DataFrame ----------\n",
    "df_stats = pd.DataFrame(results)\n",
    "\n",
    "# Calculate success rate (Twitter coverage)\n",
    "df_stats['success_rate'] = np.where(\n",
    "    df_stats['wikipedia_count'] > 0,\n",
    "    (df_stats['twitter_count'] / df_stats['wikipedia_count'] * 100).round(1),\n",
    "    0\n",
    ")\n",
    "\n",
    "# Add totals row\n",
    "totals = {\n",
    "    'folder': 'TOTAL',\n",
    "    'category_name': 'TOTAL',\n",
    "    'wikipedia_count': df_stats['wikipedia_count'].sum(),\n",
    "    'wikidata_count': df_stats['wikidata_count'].sum(),\n",
    "    'twitter_count': df_stats['twitter_count'].sum(),\n",
    "    'success_rate': 0  # Will calculate below\n",
    "}\n",
    "if totals['wikipedia_count'] > 0:\n",
    "    totals['success_rate'] = round(totals['twitter_count'] / totals['wikipedia_count'] * 100, 1)\n",
    "\n",
    "df_stats = pd.concat([df_stats, pd.DataFrame([totals])], ignore_index=True)\n",
    "\n",
    "# ---------- Save to CSV ----------\n",
    "output_path = os.path.join(POIS_DIR, \"POI_statistics.csv\")\n",
    "df_stats.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… Statistics saved to: {output_path}\")\n",
    "print(f\"ğŸ“Š Total categories: {len(results)}\")\n",
    "print(f\"ğŸ“– Total Wikipedia entries: {totals['wikipedia_count']}\")\n",
    "print(f\"ğŸ”— Total Wikidata entries: {totals['wikidata_count']}\")\n",
    "print(f\"ğŸ¦ Total Twitter users: {totals['twitter_count']}\")\n",
    "print(f\"ğŸ“ˆ Overall success rate: {totals['success_rate']}%\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# ---------- Display summary table ----------\n",
    "display(df_stats)\n",
    "\n",
    "# ---------- VISUALIZATION 1: Success Rate Bar Chart ----------\n",
    "print(\"\\nğŸ“Š Generating success rate bar chart...\")\n",
    "\n",
    "# Exclude TOTAL row for visualization\n",
    "df_viz = df_stats[df_stats['folder'] != 'TOTAL'].copy()\n",
    "df_viz = df_viz.sort_values('success_rate', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.bar(range(len(df_viz)), df_viz['success_rate'], \n",
    "               color='steelblue', alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (idx, row) in enumerate(df_viz.iterrows()):\n",
    "    plt.text(i, row['success_rate'] + 1, f\"{row['success_rate']:.1f}%\", \n",
    "             ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Category', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Twitter Success Rate (%)', fontsize=12, fontweight='bold')\n",
    "plt.title('Twitter Account Discovery Success Rate by Category\\n(Twitter users / Wikipedia entries)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xticks(range(len(df_viz)), df_viz['category_name'], rotation=45, ha='right', fontsize=10)\n",
    "plt.ylim(0, max(df_viz['success_rate']) * 1.15)\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "chart_path = os.path.join(POIS_DIR, \"figures\")\n",
    "os.makedirs(chart_path, exist_ok=True)\n",
    "chart_file = os.path.join(chart_path, \"twitter_success_rate.png\")\n",
    "plt.savefig(chart_file, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"âœ… Bar chart saved: {chart_file}\")\n",
    "\n",
    "# ---------- VISUALIZATION 2: Venn Diagram ----------\n",
    "print(\"\\nğŸ“Š Generating Venn diagram...\")\n",
    "\n",
    "# Calculate totals for Venn diagram\n",
    "total_wiki = sum(e['wiki_count'] for e in all_entities)\n",
    "total_wikidata = sum(e['wd_count'] for e in all_entities)\n",
    "total_twitter = sum(e['tw_count'] for e in all_entities)\n",
    "\n",
    "# Overlaps (simplified - using minimum counts as proxy)\n",
    "wiki_wd = min(total_wiki, total_wikidata)\n",
    "wiki_tw = min(total_wiki, total_twitter)\n",
    "wd_tw = min(total_wikidata, total_twitter)\n",
    "wiki_wd_tw = min(total_wiki, total_wikidata, total_twitter)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "venn_diagram = venn3(\n",
    "    subsets=(\n",
    "        total_wiki - wiki_wd - wiki_tw + wiki_wd_tw,  # Only Wikipedia\n",
    "        total_wikidata - wiki_wd - wd_tw + wiki_wd_tw,  # Only Wikidata\n",
    "        wiki_wd - wiki_wd_tw,  # Wiki + Wikidata\n",
    "        total_twitter - wiki_tw - wd_tw + wiki_wd_tw,  # Only Twitter\n",
    "        wiki_tw - wiki_wd_tw,  # Wiki + Twitter\n",
    "        wd_tw - wiki_wd_tw,  # Wikidata + Twitter\n",
    "        wiki_wd_tw  # All three\n",
    "    ),\n",
    "    set_labels=('Wikipedia', 'Wikidata', 'Twitter')\n",
    ")\n",
    "\n",
    "plt.title('Overlap Between Wikipedia, Wikidata, and Twitter Coverage\\n(Total POI Entities)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "venn_file = os.path.join(chart_path, \"wikipedia_wikidata_twitter_venn.png\")\n",
    "plt.savefig(venn_file, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"âœ… Venn diagram saved: {venn_file}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ğŸ‰ STEP 6 COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"ğŸ“„ Summary table: {output_path}\")\n",
    "print(f\"ğŸ“Š Bar chart: {chart_file}\")\n",
    "print(f\"ğŸ“Š Venn diagram: {venn_file}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Scrape Twitter User Metadata Using Selenium\n",
    "\n",
    "In this step, we scrape detailed metadata for each Twitter username found in our POI dataset using Selenium WebDriver. This approach directly accesses Twitter profile pages without API rate limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "===============================================================================\n",
    "STEP 7: Selenium-Based Twitter User Metadata Scraper\n",
    "===============================================================================\n",
    "\n",
    "This script scrapes Twitter user profiles using Selenium WebDriver to extract:\n",
    "- Username, Display Name, Description (Bio)\n",
    "- Followers, Following, Tweets count\n",
    "- Location, Profile Image, Verification status\n",
    "- Account status (active/suspended/protected/not found)\n",
    "\n",
    "Features:\n",
    "- Headless Chrome for faster execution\n",
    "- Automatic retry logic with exponential backoff\n",
    "- Random delays to avoid detection\n",
    "- Robust error handling for missing elements\n",
    "- Progress tracking with intermediate saves\n",
    "- Works without Twitter API credentials\n",
    "\n",
    "Author: Iran DS Project - STEP 7\n",
    "Date: December 2024\n",
    "===============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import (\n",
    "    TimeoutException, \n",
    "    NoSuchElementException, \n",
    "    WebDriverException\n",
    ")\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# snscrape import for fallback (statuses_count and created_at)\n",
    "try:\n",
    "    import snscrape.modules.twitter as sntwitter\n",
    "    SNSCRAPE_AVAILABLE = True\n",
    "    print(\"âœ… snscrape available for fallback data collection\")\n",
    "except ImportError:\n",
    "    SNSCRAPE_AVAILABLE = False\n",
    "    print(\"âš ï¸  snscrape not available - will skip statuses_count and created_at\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸš€ TWITTER USER METADATA SCRAPER - SELENIUM\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Find Iran project root directory\n",
    "def find_iran_root():\n",
    "    \"\"\"Locate the Iran directory containing POIs folder\"\"\"\n",
    "    cwd = Path(os.getcwd())\n",
    "    if (cwd / 'POIs').exists():\n",
    "        return cwd\n",
    "    for parent in cwd.parents:\n",
    "        if (parent / 'POIs').exists():\n",
    "            return parent\n",
    "    raise FileNotFoundError(\"Could not find Iran directory with POIs folder\")\n",
    "\n",
    "IRAN_ROOT = find_iran_root()\n",
    "POIS_DIR = IRAN_ROOT / 'POIs'\n",
    "INPUT_FILE = POIS_DIR / 'Manual_Search_POIs_Unique.csv'\n",
    "OUTPUT_FILE = POIS_DIR / 'POI_twitter_users_data.csv'\n",
    "TEMP_FILE = POIS_DIR / 'POI_twitter_users_data_temp.csv'\n",
    "FAILED_FILE = POIS_DIR / 'POI_twitter_scraping_failed.txt'\n",
    "\n",
    "# Scraping parameters\n",
    "MAX_RETRIES = 3\n",
    "PAGE_LOAD_TIMEOUT = 15  # seconds\n",
    "ELEMENT_WAIT_TIMEOUT = 10  # seconds\n",
    "MIN_DELAY = 2  # minimum seconds between requests\n",
    "MAX_DELAY = 5  # maximum seconds between requests\n",
    "SAVE_INTERVAL = 25  # save progress every N users\n",
    "\n",
    "print(f\"ğŸ“‚ Project Root: {IRAN_ROOT}\")\n",
    "print(f\"ğŸ“‚ POIs Directory: {POIS_DIR}\")\n",
    "print(f\"ğŸ“„ Input File: {INPUT_FILE.name}\")\n",
    "print(f\"ğŸ“„ Output File: {OUTPUT_FILE.name}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD USERNAMES\n",
    "# ============================================================================\n",
    "\n",
    "def load_unique_usernames():\n",
    "    \"\"\"\n",
    "    Load Twitter usernames from CSV and prepare for scraping\n",
    "    \n",
    "    Returns:\n",
    "        list: Unique, cleaned Twitter usernames\n",
    "    \"\"\"\n",
    "    if not INPUT_FILE.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {INPUT_FILE}\")\n",
    "    \n",
    "    print(f\"ğŸ“– Reading usernames from: {INPUT_FILE.name}\")\n",
    "    df = pd.read_csv(INPUT_FILE, encoding='utf-8-sig')\n",
    "    \n",
    "    if 'Twitter_username' not in df.columns:\n",
    "        raise ValueError(f\"Column 'Twitter_username' not found in {INPUT_FILE.name}\")\n",
    "    \n",
    "    # Extract usernames and clean\n",
    "    usernames = df['Twitter_username'].dropna().astype(str)\n",
    "    \n",
    "    # Remove year-only entries (e.g., 2015, 2023)\n",
    "    usernames = usernames[~usernames.str.fullmatch(r'\\d{4}')]\n",
    "    \n",
    "    # Remove purely numeric entries\n",
    "    usernames = usernames[~usernames.str.fullmatch(r'\\d+')]\n",
    "    \n",
    "    # Remove duplicates and sort\n",
    "    usernames = usernames.drop_duplicates().sort_values().reset_index(drop=True)\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(usernames)} unique Twitter usernames\\n\")\n",
    "    return usernames.tolist()\n",
    "\n",
    "# ============================================================================\n",
    "# SELENIUM SETUP\n",
    "# ============================================================================\n",
    "\n",
    "def setup_chrome_driver():\n",
    "    \"\"\"\n",
    "    Initialize Chrome WebDriver with optimized options\n",
    "    \n",
    "    Returns:\n",
    "        webdriver.Chrome: Configured Chrome driver instance\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”§ Setting up Chrome WebDriver...\")\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    \n",
    "    # Headless mode for faster execution\n",
    "    chrome_options.add_argument('--headless=new')\n",
    "    \n",
    "    # Performance optimizations\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    \n",
    "    # Disable images for faster loading\n",
    "    chrome_options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "    \n",
    "    # Set user agent to avoid detection\n",
    "    chrome_options.add_argument(\n",
    "        'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) '\n",
    "        'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "        'Chrome/120.0.0.0 Safari/537.36'\n",
    "    )\n",
    "    \n",
    "    # Additional preferences\n",
    "    prefs = {\n",
    "        'profile.default_content_setting_values': {\n",
    "            'images': 2,  # Disable images\n",
    "            'javascript': 1  # Enable JavaScript (required for Twitter)\n",
    "        }\n",
    "    }\n",
    "    chrome_options.add_experimental_option('prefs', prefs)\n",
    "    chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    \n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        driver.set_page_load_timeout(PAGE_LOAD_TIMEOUT)\n",
    "        print(\"âœ… Chrome WebDriver initialized successfully\\n\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to initialize Chrome WebDriver: {e}\")\n",
    "        raise\n",
    "\n",
    "# ============================================================================\n",
    "# SCRAPING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def check_driver_alive(driver):\n",
    "    \"\"\"\n",
    "    Check if WebDriver session is still alive\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if driver is alive, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        _ = driver.current_url\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def extract_number_from_text(text):\n",
    "    \"\"\"\n",
    "    Extract numeric value from text like '1,234' or '5.2K' or '1.2M'\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text containing a number\n",
    "        \n",
    "    Returns:\n",
    "        int: Extracted number, or 0 if extraction fails\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    try:\n",
    "        # Remove commas\n",
    "        text = text.replace(',', '').strip()\n",
    "        \n",
    "        # Handle K (thousands) and M (millions)\n",
    "        if 'K' in text.upper():\n",
    "            return int(float(text.upper().replace('K', '')) * 1000)\n",
    "        elif 'M' in text.upper():\n",
    "            return int(float(text.upper().replace('M', '')) * 1000000)\n",
    "        else:\n",
    "            # Extract first number found\n",
    "            match = re.search(r'[\\d.]+', text)\n",
    "            if match:\n",
    "                return int(float(match.group()))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def scrape_twitter_user(driver, username, retry_count=0):\n",
    "    \"\"\"\n",
    "    Scrape a single Twitter user's profile\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        username (str): Twitter username (without @)\n",
    "        retry_count (int): Current retry attempt\n",
    "        \n",
    "    Returns:\n",
    "        dict: User data dictionary, or None if failed\n",
    "    \"\"\"\n",
    "    url = f\"https://twitter.com/{username}\"\n",
    "    \n",
    "    try:\n",
    "        # Navigate to profile\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for page to load and scroll to ensure all content loads\n",
    "        time.sleep(random.uniform(2.5, 4.0))\n",
    "        \n",
    "        # Scroll down a bit to trigger lazy loading\n",
    "        driver.execute_script(\"window.scrollTo(0, 500);\")\n",
    "        time.sleep(1)\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Initialize result dictionary\n",
    "        result = {\n",
    "            'username': username,\n",
    "            'display_name': '',\n",
    "            'description': '',\n",
    "            'followers_count': 0,\n",
    "            'following_count': 0,\n",
    "            'statuses_count': 0,\n",
    "            'location': '',\n",
    "            'profile_image_url': '',\n",
    "            'verified': False,\n",
    "            'account_status': 'active'\n",
    "        }\n",
    "        \n",
    "        # Get page source for status checks\n",
    "        page_source_lower = driver.page_source.lower()\n",
    "        \n",
    "        # Check for suspended account (be more specific)\n",
    "        if 'account suspended' in page_source_lower or 'has been suspended' in page_source_lower:\n",
    "            result['account_status'] = 'suspended'\n",
    "            return result\n",
    "        \n",
    "        # Check for not found (before protected, as it's terminal)\n",
    "        if \"this account doesn't exist\" in page_source_lower or \"this account doesn't exist\" in page_source_lower:\n",
    "            result['account_status'] = 'not_found'\n",
    "            return result\n",
    "        \n",
    "        # Check for deactivated account\n",
    "        if 'account deactivated' in page_source_lower or 'has been deactivated' in page_source_lower:\n",
    "            result['account_status'] = 'deactivated'\n",
    "            return result\n",
    "        \n",
    "        # Check for protected account (tweets are protected)\n",
    "        if 'these tweets are protected' in page_source_lower or 'protected tweets' in page_source_lower:\n",
    "            result['account_status'] = 'protected'\n",
    "            # Continue to extract available data (name, bio, etc.)\n",
    "        \n",
    "        wait = WebDriverWait(driver, ELEMENT_WAIT_TIMEOUT)\n",
    "        \n",
    "        # Extract display name\n",
    "        try:\n",
    "            # Multiple selectors for display name\n",
    "            selectors = [\n",
    "                \"//div[@data-testid='UserName']//span[1]\",\n",
    "                \"//div[contains(@class, 'profile')]//span[contains(text(), '@')]/../preceding-sibling::span\",\n",
    "                \"//h2[@role='heading']//span\"\n",
    "            ]\n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    name_elem = driver.find_element(By.XPATH, selector)\n",
    "                    if name_elem.text and not name_elem.text.startswith('@'):\n",
    "                        result['display_name'] = name_elem.text.strip()\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Extract description/bio\n",
    "        try:\n",
    "            bio_elem = driver.find_element(By.XPATH, \"//div[@data-testid='UserDescription']\")\n",
    "            result['description'] = bio_elem.text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Extract followers count\n",
    "        try:\n",
    "            followers_elem = driver.find_element(By.XPATH, \"//a[contains(@href, '/verified_followers') or contains(@href, '/followers')]//span[1]\")\n",
    "            result['followers_count'] = extract_number_from_text(followers_elem.text)\n",
    "        except:\n",
    "            # Alternative selector\n",
    "            try:\n",
    "                followers_text = driver.find_element(By.XPATH, \"//span[contains(text(), 'Followers')]/../span[1]\").text\n",
    "                result['followers_count'] = extract_number_from_text(followers_text)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Extract following count\n",
    "        try:\n",
    "            following_elem = driver.find_element(By.XPATH, \"//a[contains(@href, '/following')]//span[1]\")\n",
    "            result['following_count'] = extract_number_from_text(following_elem.text)\n",
    "        except:\n",
    "            # Alternative selector\n",
    "            try:\n",
    "                following_text = driver.find_element(By.XPATH, \"//span[contains(text(), 'Following')]/../span[1]\").text\n",
    "                result['following_count'] = extract_number_from_text(following_text)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Extract tweets/statuses count - Twitter API v2 style\n",
    "        try:\n",
    "            # Method 1: Look in the page source for JSON data\n",
    "            page_source = driver.page_source\n",
    "            \n",
    "            # Twitter embeds user data in JSON format\n",
    "            json_patterns = [\n",
    "                r'\"statuses_count\"\\s*:\\s*(\\d+)',\n",
    "                r'\"tweet_count\"\\s*:\\s*(\\d+)',\n",
    "                r'\\\"statuses_count\\\\\":(\\d+)',\n",
    "                r'\\\"tweet_count\\\\\":(\\d+)',\n",
    "                r'\"media_count\"\\s*:\\s*(\\d+)',  # Sometimes shown as media\n",
    "            ]\n",
    "            \n",
    "            for pattern in json_patterns:\n",
    "                match = re.search(pattern, page_source)\n",
    "                if match:\n",
    "                    count = int(match.group(1))\n",
    "                    if count > 0:  # Make sure we found a real count\n",
    "                        result['statuses_count'] = count\n",
    "                        break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 2: If still 0, try scraping visible elements\n",
    "        if result['statuses_count'] == 0:\n",
    "            try:\n",
    "                # Wait a bit more for content to load\n",
    "                time.sleep(1)\n",
    "                \n",
    "                # Try to find any span with numbers that might be tweet count\n",
    "                # Usually appears near followers/following\n",
    "                all_spans = driver.find_elements(By.TAG_NAME, \"span\")\n",
    "                \n",
    "                # Look for pattern: number followed by \"posts\" or near followers\n",
    "                for i, span in enumerate(all_spans):\n",
    "                    text = span.text.strip()\n",
    "                    if text and any(c.isdigit() for c in text):\n",
    "                        # Check if next spans contain \"Following\" or \"Followers\"\n",
    "                        context = \"\"\n",
    "                        if i + 1 < len(all_spans):\n",
    "                            context += all_spans[i + 1].text.lower()\n",
    "                        if i + 2 < len(all_spans):\n",
    "                            context += \" \" + all_spans[i + 2].text.lower()\n",
    "                        \n",
    "                        # If we see followers/following nearby, the first number might be posts\n",
    "                        if \"following\" in context or \"follower\" in context:\n",
    "                            potential_count = extract_number_from_text(text)\n",
    "                            if potential_count > 0:\n",
    "                                result['statuses_count'] = potential_count\n",
    "                                break\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Extract location\n",
    "        try:\n",
    "            location_elem = driver.find_element(By.XPATH, \"//span[@data-testid='UserLocation']\")\n",
    "            result['location'] = location_elem.text.strip()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Extract profile image URL\n",
    "        try:\n",
    "            img_elem = driver.find_element(By.XPATH, \"//div[@data-testid='UserAvatar-Container-unknown']//img | //a[contains(@href, '/photo')]//img\")\n",
    "            result['profile_image_url'] = img_elem.get_attribute('src')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Check for verification badge\n",
    "        try:\n",
    "            verified_elem = driver.find_element(By.XPATH, \"//svg[@aria-label='Verified account']\")\n",
    "            result['verified'] = True\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except TimeoutException:\n",
    "        if retry_count < MAX_RETRIES:\n",
    "            print(f\"      â±ï¸  Timeout, retrying ({retry_count + 1}/{MAX_RETRIES})...\")\n",
    "            time.sleep(random.uniform(3, 6))\n",
    "            return scrape_twitter_user(driver, username, retry_count + 1)\n",
    "        else:\n",
    "            print(f\"      âŒ Failed after {MAX_RETRIES} retries (timeout)\")\n",
    "            return None\n",
    "    \n",
    "    except WebDriverException as e:\n",
    "        # WebDriver crashed or session lost\n",
    "        error_msg = str(e).lower()\n",
    "        if 'invalid session' in error_msg or 'session' in error_msg:\n",
    "            print(f\"      ğŸ”„ WebDriver session lost - needs restart\")\n",
    "            return 'SESSION_LOST'\n",
    "        else:\n",
    "            print(f\"      âŒ WebDriver error: {str(e)[:100]}\")\n",
    "            return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"      âŒ Error: {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN SCRAPING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    # Load usernames\n",
    "    usernames = load_unique_usernames()\n",
    "    total_users = len(usernames)\n",
    "    \n",
    "    # Initialize Chrome driver\n",
    "    driver = setup_chrome_driver()\n",
    "    \n",
    "    # Storage for results\n",
    "    results = []\n",
    "    failed_usernames = []\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"ğŸš€ STARTING SCRAPING: {total_users} users to process\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        for idx, username in enumerate(usernames, 1):\n",
    "            print(f\"[{idx}/{total_users}] Scraping: @{username}\")\n",
    "            \n",
    "            # Check if driver is still alive\n",
    "            if not check_driver_alive(driver):\n",
    "                print(\"   ğŸ”„ WebDriver session lost, restarting...\")\n",
    "                driver.quit()\n",
    "                time.sleep(3)\n",
    "                driver = setup_chrome_driver()\n",
    "                print(\"   âœ… WebDriver restarted successfully\")\n",
    "            \n",
    "            # Scrape user\n",
    "            user_data = scrape_twitter_user(driver, username)\n",
    "            \n",
    "            # Handle session lost\n",
    "            if user_data == 'SESSION_LOST':\n",
    "                print(\"   ğŸ”„ Restarting WebDriver...\")\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "                time.sleep(3)\n",
    "                driver = setup_chrome_driver()\n",
    "                print(\"   âœ… WebDriver restarted, retrying user...\")\n",
    "                \n",
    "                # Retry scraping this user\n",
    "                user_data = scrape_twitter_user(driver, username)\n",
    "                if user_data == 'SESSION_LOST' or user_data is None:\n",
    "                    print(\"   âŒ Failed again after restart\")\n",
    "                    failed_usernames.append(username)\n",
    "                    continue\n",
    "            \n",
    "            if user_data:\n",
    "                results.append(user_data)\n",
    "                \n",
    "                # Display status\n",
    "                status = user_data['account_status']\n",
    "                if status == 'active':\n",
    "                    print(f\"   âœ… {user_data['display_name']} | \"\n",
    "                          f\"{user_data['followers_count']:,} followers | \"\n",
    "                          f\"{user_data['following_count']:,} following\")\n",
    "                else:\n",
    "                    print(f\"   âš ï¸  Account status: {status.upper()}\")\n",
    "            else:\n",
    "                failed_usernames.append(username)\n",
    "                print(f\"   âŒ Failed to retrieve data\")\n",
    "            \n",
    "            # Random delay between requests\n",
    "            if idx < total_users:\n",
    "                delay = random.uniform(MIN_DELAY, MAX_DELAY)\n",
    "                time.sleep(delay)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            if idx % SAVE_INTERVAL == 0 and results:\n",
    "                temp_df = pd.DataFrame(results)\n",
    "                temp_df.to_csv(TEMP_FILE, index=False, encoding='utf-8-sig')\n",
    "                print(f\"   ğŸ’¾ Saved {len(results)} results to temp file\")\n",
    "                print()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ“Š PROCESSING RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Create final DataFrame\n",
    "        if results:\n",
    "            df_final = pd.DataFrame(results)\n",
    "            \n",
    "            # Sort by followers count (descending)\n",
    "            df_final = df_final.sort_values('followers_count', ascending=False).reset_index(drop=True)\n",
    "            \n",
    "            # Save to CSV\n",
    "            df_final.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            print(f\"\\nâœ… Successfully scraped {len(df_final)} users\")\n",
    "            print(f\"ğŸ“ Saved to: {OUTPUT_FILE}\")\n",
    "            \n",
    "            # Print statistics\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ğŸ“ˆ SUMMARY STATISTICS\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            status_counts = df_final['account_status'].value_counts()\n",
    "            print(\"\\nğŸ” Account Status Distribution:\")\n",
    "            for status, count in status_counts.items():\n",
    "                print(f\"   {status.upper()}: {count} ({count/len(df_final)*100:.1f}%)\")\n",
    "            \n",
    "            active_users = df_final[df_final['account_status'] == 'active']\n",
    "            if len(active_users) > 0:\n",
    "                print(f\"\\nğŸ“Š Active Accounts Statistics:\")\n",
    "                print(f\"   Total Followers: {active_users['followers_count'].sum():,}\")\n",
    "                print(f\"   Average Followers: {active_users['followers_count'].mean():.0f}\")\n",
    "                print(f\"   Median Followers: {active_users['followers_count'].median():.0f}\")\n",
    "                print(f\"   Max Followers: {active_users['followers_count'].max():,}\")\n",
    "                print(f\"   Verified Accounts: {active_users['verified'].sum()} ({active_users['verified'].sum()/len(active_users)*100:.1f}%)\")\n",
    "            \n",
    "            # Display top 20 users\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ğŸ† TOP 20 USERS BY FOLLOWERS\")\n",
    "            print(\"=\" * 80)\n",
    "            print()\n",
    "            display(df_final[['username', 'display_name', 'followers_count', 'following_count', 'account_status']].head(20))\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nâŒ No user data collected\")\n",
    "        \n",
    "        # Save failed usernames\n",
    "        if failed_usernames:\n",
    "            with open(FAILED_FILE, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"Failed to scrape {len(failed_usernames)} usernames:\\n\\n\")\n",
    "                for uname in failed_usernames:\n",
    "                    f.write(f\"{uname}\\n\")\n",
    "            print(f\"\\nâš ï¸  Failed to scrape {len(failed_usernames)} usernames\")\n",
    "            print(f\"ğŸ“ List saved to: {FAILED_FILE}\")\n",
    "        \n",
    "        # Clean up temp file if exists\n",
    "        if TEMP_FILE.exists():\n",
    "            TEMP_FILE.unlink()\n",
    "            print(f\"\\nğŸ—‘ï¸  Cleaned up temporary file\")\n",
    "        \n",
    "    finally:\n",
    "        # Always close the driver\n",
    "        driver.quit()\n",
    "        print(\"\\nğŸ”’ Chrome WebDriver closed\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ‰ STEP 7 COMPLETE: Twitter User Metadata Collection\")\n",
    "    print(f\"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Execute main function\n",
    "if __name__ == \"__main__\" or True:  # Always run in notebook\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Update Statuses Count with New Authentication\n",
    "\n",
    "Now that we have a logged-in Twitter account, we can extract the posts count that appears below the username on profile pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Update Statuses Count (Posts) for all POIs\n",
    "Using Selenium with authenticated session\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š UPDATING STATUSES COUNT FOR ALL POIs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Paths\n",
    "IRAN_ROOT = Path(r\"c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\")\n",
    "POIS_DIR = IRAN_ROOT / 'POIs'\n",
    "INPUT_CSV = POIS_DIR / 'POI_twitter_users_data.csv'\n",
    "COOKIES_FILE = POIS_DIR / 'x_cookies.json'\n",
    "\n",
    "print(f\"ğŸ“„ Input: {INPUT_CSV}\")\n",
    "print(f\"ğŸ” Cookies: {COOKIES_FILE} ({'âœ… Found' if COOKIES_FILE.exists() else 'âŒ Missing'})\\n\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(INPUT_CSV, encoding='utf-8-sig')\n",
    "print(f\"âœ… Loaded {len(df)} users\")\n",
    "print(f\"   Users with statuses_count = 0: {(df['statuses_count'] == 0).sum()}\\n\")\n",
    "\n",
    "# Setup Selenium\n",
    "print(\"ğŸŒ Setting up Chrome WebDriver...\")\n",
    "options = Options()\n",
    "options.add_argument('--headless=new')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.set_page_load_timeout(20)\n",
    "\n",
    "# Load cookies\n",
    "if COOKIES_FILE.exists():\n",
    "    driver.get(\"https://twitter.com\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    with open(COOKIES_FILE, 'r', encoding='utf-8') as f:\n",
    "        cookies = json.load(f)\n",
    "    \n",
    "    for cookie in cookies:\n",
    "        try:\n",
    "            cookie.pop('sameSite', None)\n",
    "            cookie.pop('storeId', None)\n",
    "            driver.add_cookie(cookie)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(\"âœ… Cookies loaded\\n\")\n",
    "else:\n",
    "    print(\"âš ï¸  No cookies found - results may be limited\\n\")\n",
    "\n",
    "# Function to extract posts count from profile header\n",
    "def extract_posts_count(driver, username, debug=False):\n",
    "    \"\"\"\n",
    "    Extract posts count directly from Twitter profile header DOM element.\n",
    "    The post count appears in a div with text ending in 'posts' (e.g., '45.8K posts')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"https://twitter.com/{username}\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for page to load - looking specifically for the posts element\n",
    "        time.sleep(5)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\n[DEBUG] Loading profile: {url}\")\n",
    "        \n",
    "        # Check if we hit a login/captcha page (Twitter blocking detection)\n",
    "        page_source = driver.page_source.lower()\n",
    "        if 'log in' in page_source and 'sign up' in page_source and 'posts' not in page_source:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] âš ï¸ Twitter login page detected - might be blocked!\")\n",
    "            return \"BLOCKED\"\n",
    "        \n",
    "        # Method 1: Find div element containing \"posts\" text in profile header\n",
    "        # XPath: //div[contains(text(), 'posts')] or //div[contains(text(), 'Posts')]\n",
    "        try:\n",
    "            # Look for elements with \"posts\" text (case-insensitive by checking both)\n",
    "            posts_elements = driver.find_elements(By.XPATH, \n",
    "                \"//div[contains(translate(text(), 'POSTS', 'posts'), 'posts')]\")\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Found {len(posts_elements)} elements with 'posts' text\")\n",
    "            \n",
    "            for elem in posts_elements:\n",
    "                text = elem.text.strip()\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] Checking element text: '{text}'\")\n",
    "                \n",
    "                # Look for pattern: \"45.8K posts\" or \"3,368 posts\" or \"123 posts\"\n",
    "                # Text should end with \"posts\" or \"Posts\"\n",
    "                if text.lower().endswith('posts') or text.lower().endswith('post'):\n",
    "                    # Extract the numeric part (everything before \"posts\")\n",
    "                    count_str = text.lower().replace('posts', '').replace('post', '').strip()\n",
    "                    count_str = count_str.replace(',', '').upper()\n",
    "                    \n",
    "                    if debug:\n",
    "                        print(f\"[DEBUG] Extracted count string: '{count_str}'\")\n",
    "                    \n",
    "                    # Handle K, M, B suffixes\n",
    "                    try:\n",
    "                        if 'K' in count_str:\n",
    "                            count = int(float(count_str.replace('K', '')) * 1000)\n",
    "                        elif 'M' in count_str:\n",
    "                            count = int(float(count_str.replace('M', '')) * 1000000)\n",
    "                        elif 'B' in count_str:\n",
    "                            count = int(float(count_str.replace('B', '')) * 1000000000)\n",
    "                        else:\n",
    "                            count = int(count_str)\n",
    "                        \n",
    "                        if debug:\n",
    "                            print(f\"[DEBUG] Successfully parsed: {text} â†’ {count:,}\")\n",
    "                        \n",
    "                        return count\n",
    "                    except ValueError:\n",
    "                        if debug:\n",
    "                            print(f\"[DEBUG] Could not parse '{count_str}' as number\")\n",
    "                        continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] XPath search error: {e}\")\n",
    "        \n",
    "        # Method 2: Fallback - search in all divs with dir=\"ltr\" attribute\n",
    "        try:\n",
    "            ltr_divs = driver.find_elements(By.XPATH, \"//div[@dir='ltr']\")\n",
    "            for div in ltr_divs:\n",
    "                text = div.text.strip()\n",
    "                if text.lower().endswith('posts') or text.lower().endswith('post'):\n",
    "                    count_str = text.lower().replace('posts', '').replace('post', '').strip()\n",
    "                    count_str = count_str.replace(',', '').upper()\n",
    "                    \n",
    "                    try:\n",
    "                        if 'K' in count_str:\n",
    "                            count = int(float(count_str.replace('K', '')) * 1000)\n",
    "                        elif 'M' in count_str:\n",
    "                            count = int(float(count_str.replace('M', '')) * 1000000)\n",
    "                        elif 'B' in count_str:\n",
    "                            count = int(float(count_str.replace('B', '')) * 1000000000)\n",
    "                        else:\n",
    "                            count = int(count_str)\n",
    "                        \n",
    "                        return count\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Could not find posts count on page\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)[:60]}\")\n",
    "        return None\n",
    "\n",
    "# Function to reload cookies and refresh session\n",
    "def reload_cookies(driver, cookies_file):\n",
    "    \"\"\"Reload cookies to maintain authenticated session\"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://twitter.com\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Clear existing cookies\n",
    "        driver.delete_all_cookies()\n",
    "        \n",
    "        # Load fresh cookies\n",
    "        with open(cookies_file, 'r', encoding='utf-8') as f:\n",
    "            cookies = json.load(f)\n",
    "        \n",
    "        for cookie in cookies:\n",
    "            try:\n",
    "                cookie.pop('sameSite', None)\n",
    "                cookie.pop('storeId', None)\n",
    "                driver.add_cookie(cookie)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        time.sleep(2)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Cookie reload failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Process users\n",
    "print(\"ğŸ”„ Processing users...\\n\")\n",
    "updated_count = 0\n",
    "failed_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "# Only update users with statuses_count = 0 OR NaN\n",
    "users_to_update = df[(df['statuses_count'] == 0) | (df['statuses_count'].isna())].copy()\n",
    "users_with_data = (df['statuses_count'] > 0).sum()\n",
    "\n",
    "print(f\"ğŸ“Š Statistics:\")\n",
    "print(f\"   âœ… Already have data: {users_with_data} users\")\n",
    "print(f\"   ğŸ”„ Need to update: {len(users_to_update)} users\")\n",
    "print(f\"   ğŸ“ Total users: {len(df)}\\n\")\n",
    "\n",
    "for idx, row in users_to_update.iterrows():\n",
    "    username = row['username']\n",
    "    current_count = row['statuses_count']\n",
    "    \n",
    "    # Enable debug for first 3 users to see what's happening\n",
    "    debug = (updated_count + failed_count) < 3\n",
    "    \n",
    "    print(f\"[{updated_count + failed_count + 1}/{len(users_to_update)}] {username}...\", end=\" \")\n",
    "    \n",
    "    # Reload cookies every 5 users (more frequent!) to maintain session\n",
    "    if (updated_count + failed_count) > 0 and (updated_count + failed_count) % 5 == 0:\n",
    "        print(\"\\nğŸ”„ Reloading cookies to maintain session...\", end=\" \")\n",
    "        if reload_cookies(driver, COOKIES_FILE):\n",
    "            print(\"âœ…\")\n",
    "            time.sleep(3)  # Extra wait after cookie reload\n",
    "        else:\n",
    "            print(\"âš ï¸\")\n",
    "    \n",
    "    # Restart browser every 15 users (more frequent!) to avoid detection\n",
    "    if (updated_count + failed_count) > 0 and (updated_count + failed_count) % 15 == 0:\n",
    "        print(\"\\nğŸ”„ Restarting browser to avoid detection...\")\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(5)  # Wait before reopening\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.set_page_load_timeout(20)\n",
    "        if COOKIES_FILE.exists():\n",
    "            driver.get(\"https://twitter.com\")\n",
    "            time.sleep(3)\n",
    "            with open(COOKIES_FILE, 'r', encoding='utf-8') as f:\n",
    "                cookies = json.load(f)\n",
    "            for cookie in cookies:\n",
    "                try:\n",
    "                    cookie.pop('sameSite', None)\n",
    "                    cookie.pop('storeId', None)\n",
    "                    driver.add_cookie(cookie)\n",
    "                except:\n",
    "                    continue\n",
    "        print(\"âœ… Browser restarted\\n\")\n",
    "        time.sleep(3)  # Extra wait after restart\n",
    "    \n",
    "    # Extra long pause every 10 users to \"cool down\"\n",
    "    if (updated_count + failed_count) > 0 and (updated_count + failed_count) % 10 == 0:\n",
    "        cooldown = random.uniform(15, 25)\n",
    "        print(f\"\\nâ„ï¸ Cooling down for {cooldown:.1f} seconds...\")\n",
    "        time.sleep(cooldown)\n",
    "    \n",
    "    posts_count = extract_posts_count(driver, username, debug=debug)\n",
    "    \n",
    "    # Check if we got blocked\n",
    "    if posts_count == \"BLOCKED\":\n",
    "        print(\"ğŸš« BLOCKED - Restarting browser completely...\")\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(20)  # Long wait after block\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.set_page_load_timeout(20)\n",
    "        reload_cookies(driver, COOKIES_FILE)\n",
    "        time.sleep(10)\n",
    "        posts_count = extract_posts_count(driver, username, debug=False)\n",
    "    \n",
    "    if posts_count is not None and posts_count != \"BLOCKED\" and posts_count > 0:\n",
    "        df.loc[df['username'] == username, 'statuses_count'] = posts_count\n",
    "        print(f\"âœ… {posts_count:,} posts\")\n",
    "        updated_count += 1\n",
    "    elif posts_count == 0:\n",
    "        df.loc[df['username'] == username, 'statuses_count'] = 0\n",
    "        print(f\"âœ… 0 posts (new account)\")\n",
    "        updated_count += 1\n",
    "    else:\n",
    "        print(\"âŒ Not found\")\n",
    "        failed_count += 1\n",
    "    \n",
    "    # Very long random delay between requests (8-15 seconds)\n",
    "    delay = random.uniform(8, 15)\n",
    "    time.sleep(delay)\n",
    "    \n",
    "    # Save progress every 25 users\n",
    "    if (updated_count + failed_count) % 25 == 0:\n",
    "        df.to_csv(INPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "        print(f\"  ğŸ’¾ Progress saved ({updated_count} updated)\\n\")\n",
    "\n",
    "# Close browser\n",
    "driver.quit()\n",
    "print(\"\\nğŸ”š Browser closed\")\n",
    "\n",
    "# Save final results\n",
    "df.to_csv(INPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… UPDATE COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Updated:  {updated_count}\")\n",
    "print(f\"Failed:   {failed_count}\")\n",
    "print(f\"Remaining zeros: {(df['statuses_count'] == 0).sum()}\")\n",
    "print(f\"\\nğŸ“ Saved to: {INPUT_CSV}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nğŸ“‹ Sample of updated data:\")\n",
    "display(df[df['statuses_count'] > 0][['username', 'display_name', 'followers_count', 'statuses_count']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Manual Cookie Update Helper\n",
    "\n",
    "If automatic cookie export doesn't work, use this helper to convert cookies to the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quick way to check if your cookies file is valid and working\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "POIS_DIR = Path(r\"c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\\POIs\")\n",
    "COOKIES_FILE = POIS_DIR / 'x_cookies.json'\n",
    "\n",
    "print(\"ğŸ” Checking cookies file...\\n\")\n",
    "\n",
    "if not COOKIES_FILE.exists():\n",
    "    print(f\"âŒ Cookies file not found: {COOKIES_FILE}\")\n",
    "    print(\"\\nğŸ“ To create cookies manually:\")\n",
    "    print(\"1. Install Chrome extension: Cookie-Editor\")\n",
    "    print(\"   Link: https://chrome.google.com/webstore/detail/hlkenndednhfkekhgcdicdfddnkalmdm\")\n",
    "    print(\"2. Go to https://twitter.com and login\")\n",
    "    print(\"3. Click Cookie-Editor icon â†’ Export â†’ Export as JSON\")\n",
    "    print(f\"4. Save the JSON to: {COOKIES_FILE}\")\n",
    "else:\n",
    "    try:\n",
    "        with open(COOKIES_FILE, 'r', encoding='utf-8') as f:\n",
    "            cookies = json.load(f)\n",
    "        \n",
    "        print(f\"âœ… Cookies file found: {COOKIES_FILE}\")\n",
    "        print(f\"   Total cookies: {len(cookies)}\")\n",
    "        \n",
    "        # Check for important Twitter cookies\n",
    "        important_cookies = ['auth_token', 'ct0', 'guest_id']\n",
    "        found_cookies = []\n",
    "        \n",
    "        for cookie in cookies:\n",
    "            if cookie.get('name') in important_cookies:\n",
    "                found_cookies.append(cookie.get('name'))\n",
    "        \n",
    "        print(f\"\\nğŸ”‘ Important cookies found: {', '.join(found_cookies)}\")\n",
    "        \n",
    "        if 'auth_token' in found_cookies:\n",
    "            print(\"âœ… Authentication token present - you should be logged in!\")\n",
    "        else:\n",
    "            print(\"âš ï¸  No auth_token - cookies might be outdated or incomplete\")\n",
    "            print(\"   Please re-export cookies from a logged-in Twitter session\")\n",
    "        \n",
    "        # Show sample cookie\n",
    "        if cookies:\n",
    "            print(f\"\\nğŸ“‹ Sample cookie:\")\n",
    "            print(f\"   Name: {cookies[0].get('name')}\")\n",
    "            print(f\"   Domain: {cookies[0].get('domain')}\")\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ Error reading cookies file - invalid JSON format\")\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "        print(\"\\n   The file might be corrupted. Please re-export cookies.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Clean Deleted/Suspended Users\n",
    "\n",
    "Remove users with 0 followers, 0 following, and 0 posts (likely deleted or suspended accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clean POI_twitter_users_data.csv by removing deleted/suspended users\n",
    "These are users with 0 followers, 0 following, AND 0 statuses\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ§¹ CLEANING DELETED/SUSPENDED USERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Paths\n",
    "IRAN_ROOT = Path(r\"c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\")\n",
    "POIS_DIR = IRAN_ROOT / 'POIs'\n",
    "INPUT_CSV = POIS_DIR / 'POI_twitter_users_data.csv'\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(INPUT_CSV, encoding='utf-8-sig')\n",
    "original_count = len(df)\n",
    "\n",
    "print(f\"\\nğŸ“Š Original data: {original_count} users\")\n",
    "\n",
    "# Identify deleted/suspended users (all three fields are 0)\n",
    "deleted_mask = (\n",
    "    (df['followers_count'] == 0) & \n",
    "    (df['following_count'] == 0) & \n",
    "    (df['statuses_count'] == 0)\n",
    ")\n",
    "\n",
    "deleted_users = df[deleted_mask]\n",
    "deleted_count = len(deleted_users)\n",
    "\n",
    "print(f\"\\nğŸ—‘ï¸  Found {deleted_count} deleted/suspended users:\")\n",
    "if deleted_count > 0:\n",
    "    print(\"\\nDeleted users:\")\n",
    "    for idx, row in deleted_users.iterrows():\n",
    "        print(f\"   - {row['username']} (@{row['display_name']})\")\n",
    "\n",
    "# Remove deleted users\n",
    "df_clean = df[~deleted_mask].copy()\n",
    "clean_count = len(df_clean)\n",
    "\n",
    "print(f\"\\nâœ… Cleaned data: {clean_count} users\")\n",
    "print(f\"ğŸ—‘ï¸  Removed: {deleted_count} users\")\n",
    "print(f\"ğŸ“‰ Reduction: {(deleted_count/original_count*100):.1f}%\")\n",
    "\n",
    "# Save cleaned data\n",
    "df_clean.to_csv(INPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\nğŸ’¾ Saved to: {INPUT_CSV}\")\n",
    "\n",
    "# Show statistics of cleaned data\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ğŸ“Š CLEANED DATA STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total users: {clean_count}\")\n",
    "print(f\"Average followers: {df_clean['followers_count'].mean():,.0f}\")\n",
    "print(f\"Average following: {df_clean['following_count'].mean():,.0f}\")\n",
    "print(f\"Average posts: {df_clean['statuses_count'].mean():,.0f}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nğŸ“‹ Sample of cleaned data:\")\n",
    "display(df_clean[['username', 'display_name', 'followers_count', 'following_count', 'statuses_count']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Fix Missing Data for Incomplete Profiles\n",
    "\n",
    "Re-scrape followers, following, and location for users with inconsistent data (e.g., 0 followers/following but has posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fix Missing Data for Users with Incomplete Profiles\n",
    "Re-scrape followers and following for users where:\n",
    "followers = 0 AND following = 0 BUT statuses > 0 (doesn't make sense - data collection failed)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ”§ FIXING INCOMPLETE USER PROFILES (Followers & Following Only)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Paths\n",
    "IRAN_ROOT = Path(r\"c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\")\n",
    "POIS_DIR = IRAN_ROOT / 'POIs'\n",
    "INPUT_CSV = POIS_DIR / 'POI_twitter_users_data.csv'\n",
    "COOKIES_FILE = POIS_DIR / 'x_cookies.json'\n",
    "\n",
    "print(f\"ğŸ“„ Input: {INPUT_CSV}\")\n",
    "print(f\"ğŸ” Cookies: {COOKIES_FILE} ({'âœ… Found' if COOKIES_FILE.exists() else 'âŒ Missing'})\\n\")\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(INPUT_CSV, encoding='utf-8-sig')\n",
    "print(f\"âœ… Loaded {len(df)} users\")\n",
    "\n",
    "# Identify problematic users: 0 followers AND 0 following BUT have posts\n",
    "problematic_mask = (\n",
    "    (df['followers_count'] == 0) & \n",
    "    (df['following_count'] == 0) & \n",
    "    (df['statuses_count'] > 0)\n",
    ")\n",
    "\n",
    "users_to_fix = df[problematic_mask].copy()\n",
    "print(f\"ğŸ” Found {len(users_to_fix)} users with 0 followers/following but have posts\\n\")\n",
    "\n",
    "if len(users_to_fix) == 0:\n",
    "    print(\"âœ… No users need fixing!\")\n",
    "else:\n",
    "    # Setup Selenium\n",
    "    print(\"ğŸŒ Setting up Chrome WebDriver...\")\n",
    "    options = Options()\n",
    "    options.add_argument('--headless=new')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.set_page_load_timeout(20)\n",
    "\n",
    "    # Load cookies\n",
    "    if COOKIES_FILE.exists():\n",
    "        driver.get(\"https://twitter.com\")\n",
    "        time.sleep(2)\n",
    "        \n",
    "        with open(COOKIES_FILE, 'r', encoding='utf-8') as f:\n",
    "            cookies = json.load(f)\n",
    "        \n",
    "        for cookie in cookies:\n",
    "            try:\n",
    "                cookie.pop('sameSite', None)\n",
    "                cookie.pop('storeId', None)\n",
    "                driver.add_cookie(cookie)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        print(\"âœ… Cookies loaded\\n\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No cookies found - results may be limited\\n\")\n",
    "\n",
    "    # Function to extract profile data using XPath selectors\n",
    "    def extract_profile_data(driver, username):\n",
    "        \"\"\"\n",
    "        Extract followers and following counts from profile header using XPath.\n",
    "        Numbers appear in <span> elements right before the \"Followers\"/\"Following\" labels.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            url = f\"https://twitter.com/{username}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Extract followers count using XPath\n",
    "            followers = None\n",
    "            try:\n",
    "                # Find the span containing \"Followers\" text, then get the preceding sibling span\n",
    "                followers_elements = driver.find_elements(By.XPATH, \n",
    "                    \"//span[contains(text(), 'Followers')]/preceding-sibling::span[1]\")\n",
    "                \n",
    "                if followers_elements:\n",
    "                    followers_text = followers_elements[0].text.strip()\n",
    "                    # Clean and parse: \"7\", \"12.3K\", \"1.5M\", etc.\n",
    "                    count_str = followers_text.replace(',', '').upper()\n",
    "                    \n",
    "                    if 'K' in count_str:\n",
    "                        followers = int(float(count_str.replace('K', '')) * 1000)\n",
    "                    elif 'M' in count_str:\n",
    "                        followers = int(float(count_str.replace('M', '')) * 1000000)\n",
    "                    elif 'B' in count_str:\n",
    "                        followers = int(float(count_str.replace('B', '')) * 1000000000)\n",
    "                    else:\n",
    "                        followers = int(count_str)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            # Extract following count using XPath\n",
    "            following = None\n",
    "            try:\n",
    "                # Find the span containing \"Following\" text, then get the preceding sibling span\n",
    "                following_elements = driver.find_elements(By.XPATH, \n",
    "                    \"//span[contains(text(), 'Following')]/preceding-sibling::span[1]\")\n",
    "                \n",
    "                if following_elements:\n",
    "                    following_text = following_elements[0].text.strip()\n",
    "                    # Clean and parse: \"7\", \"12.3K\", \"1.5M\", etc.\n",
    "                    count_str = following_text.replace(',', '').upper()\n",
    "                    \n",
    "                    if 'K' in count_str:\n",
    "                        following = int(float(count_str.replace('K', '')) * 1000)\n",
    "                    elif 'M' in count_str:\n",
    "                        following = int(float(count_str.replace('M', '')) * 1000000)\n",
    "                    elif 'B' in count_str:\n",
    "                        following = int(float(count_str.replace('B', '')) * 1000000000)\n",
    "                    else:\n",
    "                        following = int(count_str)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            return {\n",
    "                'followers': followers,\n",
    "                'following': following\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {str(e)[:60]}\")\n",
    "            return None\n",
    "\n",
    "    # Function to reload cookies\n",
    "    def reload_cookies(driver, cookies_file):\n",
    "        \"\"\"Reload cookies to maintain authenticated session\"\"\"\n",
    "        try:\n",
    "            driver.get(\"https://twitter.com\")\n",
    "            time.sleep(2)\n",
    "            driver.delete_all_cookies()\n",
    "            \n",
    "            with open(cookies_file, 'r', encoding='utf-8') as f:\n",
    "                cookies = json.load(f)\n",
    "            \n",
    "            for cookie in cookies:\n",
    "                try:\n",
    "                    cookie.pop('sameSite', None)\n",
    "                    cookie.pop('storeId', None)\n",
    "                    driver.add_cookie(cookie)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            time.sleep(2)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Cookie reload failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Process users\n",
    "    print(\"ğŸ”„ Processing users...\\n\")\n",
    "    fixed_count = 0\n",
    "    failed_count = 0\n",
    "\n",
    "    for idx, row in users_to_fix.iterrows():\n",
    "        username = row['username']\n",
    "        \n",
    "        print(f\"[{fixed_count + failed_count + 1}/{len(users_to_fix)}] {username}...\", end=\" \")\n",
    "        \n",
    "        # Reload cookies every 5 users\n",
    "        if (fixed_count + failed_count) > 0 and (fixed_count + failed_count) % 5 == 0:\n",
    "            print(\"\\nğŸ”„ Reloading cookies...\", end=\" \")\n",
    "            if reload_cookies(driver, COOKIES_FILE):\n",
    "                print(\"âœ…\")\n",
    "            else:\n",
    "                print(\"âš ï¸\")\n",
    "        \n",
    "        # Restart browser every 15 users\n",
    "        if (fixed_count + failed_count) > 0 and (fixed_count + failed_count) % 15 == 0:\n",
    "            print(\"\\nğŸ”„ Restarting browser...\")\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(5)\n",
    "            driver = webdriver.Chrome(service=service, options=options)\n",
    "            driver.set_page_load_timeout(20)\n",
    "            reload_cookies(driver, COOKIES_FILE)\n",
    "            print(\"âœ… Browser restarted\\n\")\n",
    "        \n",
    "        # Extract data\n",
    "        data = extract_profile_data(driver, username)\n",
    "        \n",
    "        if data:\n",
    "            updated_fields = []\n",
    "            \n",
    "            # Update followers if missing or 0\n",
    "            if data['followers'] is not None and row['followers_count'] == 0:\n",
    "                df.loc[df['username'] == username, 'followers_count'] = data['followers']\n",
    "                updated_fields.append(f\"followers={data['followers']:,}\")\n",
    "            \n",
    "            # Update following if missing or 0\n",
    "            if data['following'] is not None and row['following_count'] == 0:\n",
    "                df.loc[df['username'] == username, 'following_count'] = data['following']\n",
    "                updated_fields.append(f\"following={data['following']:,}\")\n",
    "        \n",
    "        if data:\n",
    "            updated_fields = []\n",
    "            \n",
    "            # Update followers if missing or 0\n",
    "            if data['followers'] is not None and row['followers_count'] == 0:\n",
    "                df.loc[df['username'] == username, 'followers_count'] = data['followers']\n",
    "                updated_fields.append(f\"followers={data['followers']:,}\")\n",
    "            \n",
    "            # Update following if missing or 0\n",
    "            if data['following'] is not None and row['following_count'] == 0:\n",
    "                df.loc[df['username'] == username, 'following_count'] = data['following']\n",
    "                updated_fields.append(f\"following={data['following']:,}\")\n",
    "            \n",
    "            if updated_fields:\n",
    "                print(f\"âœ… {', '.join(updated_fields)}\")\n",
    "                fixed_count += 1\n",
    "            else:\n",
    "                print(\"âš ï¸ No updates needed\")\n",
    "        else:\n",
    "            print(\"âŒ Failed\")\n",
    "            failed_count += 1\n",
    "        \n",
    "        # Random delay\n",
    "        time.sleep(random.uniform(8, 15))\n",
    "        \n",
    "        # Save progress every 10 users\n",
    "        if (fixed_count + failed_count) % 10 == 0:\n",
    "            df.to_csv(INPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "            print(f\"  ğŸ’¾ Progress saved\\n\")\n",
    "\n",
    "    # Close browser\n",
    "    driver.quit()\n",
    "    print(\"\\nğŸ”š Browser closed\")\n",
    "\n",
    "    # Save final results\n",
    "    df.to_csv(INPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"âœ… FIX COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Fixed:  {fixed_count}\")\n",
    "    print(f\"Failed: {failed_count}\")\n",
    "    print(f\"\\nğŸ“ Saved to: {INPUT_CSV}\")\n",
    "\n",
    "    # Show sample\n",
    "    print(\"\\nğŸ“‹ Sample of fixed data:\")\n",
    "    display(df[df['username'].isin(users_to_fix['username'])][['username', 'followers_count', 'following_count', 'statuses_count']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5  Re-verify Last 28 Updated Users\n",
    "\n",
    "Double-check the accuracy of follower/following data for the last 28 users that were updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Re-verify the last 28 updated users to ensure data accuracy\n",
    "Uses the improved XPath extraction method\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” RE-VERIFYING LAST 28 UPDATED USERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Paths\n",
    "IRAN_ROOT = Path(r\"c:\\Users\\nitib\\OneDrive\\×©×•×œ×—×Ÿ ×”×¢×‘×•×“×”\\Iran\\Iran\")\n",
    "POIS_DIR = IRAN_ROOT / 'POIs'\n",
    "INPUT_CSV = POIS_DIR / 'POI_twitter_users_data.csv'\n",
    "COOKIES_FILE = POIS_DIR / 'x_cookies.json'\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(INPUT_CSV, encoding='utf-8-sig')\n",
    "print(f\"âœ… Loaded {len(df)} users\\n\")\n",
    "\n",
    "# Get the last 28 users that were updated (have followers > 0 and following > 0)\n",
    "# These are likely the ones from the last run\n",
    "recently_updated = df[(df['followers_count'] > 0) & (df['following_count'] > 0)].tail(28).copy()\n",
    "\n",
    "print(f\"ğŸ“‹ Re-verifying {len(recently_updated)} recently updated users:\\n\")\n",
    "for idx, row in recently_updated.iterrows():\n",
    "    print(f\"   - {row['username']}: {row['followers_count']:,} followers, {row['following_count']:,} following\")\n",
    "\n",
    "print(\"\\nğŸŒ Setting up Chrome WebDriver...\")\n",
    "options = Options()\n",
    "options.add_argument('--headless=new')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "options.add_experimental_option('useAutomationExtension', False)\n",
    "options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.set_page_load_timeout(20)\n",
    "\n",
    "# Load cookies\n",
    "if COOKIES_FILE.exists():\n",
    "    driver.get(\"https://twitter.com\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    with open(COOKIES_FILE, 'r', encoding='utf-8') as f:\n",
    "        cookies = json.load(f)\n",
    "    \n",
    "    for cookie in cookies:\n",
    "        try:\n",
    "            cookie.pop('sameSite', None)\n",
    "            cookie.pop('storeId', None)\n",
    "            driver.add_cookie(cookie)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(\"âœ… Cookies loaded\\n\")\n",
    "\n",
    "# Function to extract with multiple methods (robust approach)\n",
    "def extract_profile_data_xpath(driver, username, debug=False):\n",
    "    \"\"\"Extract followers and following using multiple fallback methods\"\"\"\n",
    "    try:\n",
    "        url = f\"https://twitter.com/{username}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(6)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\n[DEBUG] Loading: {url}\")\n",
    "        \n",
    "        # Get page text for debugging and fallback\n",
    "        page_text = driver.find_element(By.TAG_NAME, 'body').text\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Page loaded, text length: {len(page_text)}\")\n",
    "        \n",
    "        # METHOD 1: XPath with preceding-sibling\n",
    "        followers = None\n",
    "        following = None\n",
    "        \n",
    "        try:\n",
    "            # Try to find using XPath\n",
    "            followers_elements = driver.find_elements(By.XPATH, \n",
    "                \"//span[contains(text(), 'Followers')]/preceding-sibling::span[1]\")\n",
    "            \n",
    "            if followers_elements and followers_elements[0].text.strip():\n",
    "                followers_text = followers_elements[0].text.strip().replace(',', '').upper()\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] XPath Followers text: '{followers_text}'\")\n",
    "                \n",
    "                if 'K' in followers_text:\n",
    "                    followers = int(float(followers_text.replace('K', '')) * 1000)\n",
    "                elif 'M' in followers_text:\n",
    "                    followers = int(float(followers_text.replace('M', '')) * 1000000)\n",
    "                elif 'B' in followers_text:\n",
    "                    followers = int(float(followers_text.replace('B', '')) * 1000000000)\n",
    "                else:\n",
    "                    followers = int(followers_text)\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] XPath followers failed: {e}\")\n",
    "        \n",
    "        # METHOD 2: If XPath failed, try finding in all divs containing \"Followers\"\n",
    "        if followers is None:\n",
    "            try:\n",
    "                import re\n",
    "                match = re.search(r'([\\d,\\.]+[KMB]?)\\s+Followers', page_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    followers_text = match.group(1).replace(',', '').upper()\n",
    "                    if debug:\n",
    "                        print(f\"[DEBUG] Regex Followers text: '{followers_text}'\")\n",
    "                    \n",
    "                    if 'K' in followers_text:\n",
    "                        followers = int(float(followers_text.replace('K', '')) * 1000)\n",
    "                    elif 'M' in followers_text:\n",
    "                        followers = int(float(followers_text.replace('M', '')) * 1000000)\n",
    "                    elif 'B' in followers_text:\n",
    "                        followers = int(float(followers_text.replace('B', '')) * 1000000000)\n",
    "                    else:\n",
    "                        followers = int(followers_text)\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] Regex followers failed: {e}\")\n",
    "        \n",
    "        # Extract following with same dual approach\n",
    "        try:\n",
    "            following_elements = driver.find_elements(By.XPATH, \n",
    "                \"//span[contains(text(), 'Following')]/preceding-sibling::span[1]\")\n",
    "            \n",
    "            if following_elements and following_elements[0].text.strip():\n",
    "                following_text = following_elements[0].text.strip().replace(',', '').upper()\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] XPath Following text: '{following_text}'\")\n",
    "                \n",
    "                if 'K' in following_text:\n",
    "                    following = int(float(following_text.replace('K', '')) * 1000)\n",
    "                elif 'M' in following_text:\n",
    "                    following = int(float(following_text.replace('M', '')) * 1000000)\n",
    "                elif 'B' in following_text:\n",
    "                    following = int(float(following_text.replace('B', '')) * 1000000000)\n",
    "                else:\n",
    "                    following = int(following_text)\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] XPath following failed: {e}\")\n",
    "        \n",
    "        # Fallback for following\n",
    "        if following is None:\n",
    "            try:\n",
    "                import re\n",
    "                match = re.search(r'([\\d,\\.]+[KMB]?)\\s+Following', page_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    following_text = match.group(1).replace(',', '').upper()\n",
    "                    if debug:\n",
    "                        print(f\"[DEBUG] Regex Following text: '{following_text}'\")\n",
    "                    \n",
    "                    if 'K' in following_text:\n",
    "                        following = int(float(following_text.replace('K', '')) * 1000)\n",
    "                    elif 'M' in following_text:\n",
    "                        following = int(float(following_text.replace('M', '')) * 1000000)\n",
    "                    elif 'B' in following_text:\n",
    "                        following = int(float(following_text.replace('B', '')) * 1000000000)\n",
    "                    else:\n",
    "                        following = int(following_text)\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] Regex following failed: {e}\")\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Final result: followers={followers}, following={following}\")\n",
    "        \n",
    "        return {'followers': followers, 'following': following}\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Overall error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Reload cookies function\n",
    "def reload_cookies(driver, cookies_file):\n",
    "    try:\n",
    "        driver.get(\"https://twitter.com\")\n",
    "        time.sleep(2)\n",
    "        driver.delete_all_cookies()\n",
    "        \n",
    "        with open(cookies_file, 'r', encoding='utf-8') as f:\n",
    "            cookies = json.load(f)\n",
    "        \n",
    "        for cookie in cookies:\n",
    "            try:\n",
    "                cookie.pop('sameSite', None)\n",
    "                cookie.pop('storeId', None)\n",
    "                driver.add_cookie(cookie)\n",
    "            except:\n",
    "                continue\n",
    "        time.sleep(2)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Re-verify users\n",
    "print(\"ğŸ”„ Re-verifying users...\\n\")\n",
    "verified_count = 0\n",
    "updated_count = 0\n",
    "unchanged_count = 0\n",
    "\n",
    "for idx, row in recently_updated.iterrows():\n",
    "    username = row['username']\n",
    "    old_followers = row['followers_count']\n",
    "    old_following = row['following_count']\n",
    "    \n",
    "    # Enable debug for first 2 users\n",
    "    debug = verified_count < 2\n",
    "    \n",
    "    print(f\"[{verified_count + 1}/{len(recently_updated)}] {username}...\", end=\" \")\n",
    "    \n",
    "    # Reload cookies every 5 users\n",
    "    if verified_count > 0 and verified_count % 5 == 0:\n",
    "        print(\"\\nğŸ”„ Reloading cookies...\", end=\" \")\n",
    "        reload_cookies(driver, COOKIES_FILE)\n",
    "        print(\"âœ…\")\n",
    "    \n",
    "    # Extract new data\n",
    "    data = extract_profile_data_xpath(driver, username, debug=debug)\n",
    "    \n",
    "    if data and data['followers'] is not None and data['following'] is not None:\n",
    "        new_followers = data['followers']\n",
    "        new_following = data['following']\n",
    "        \n",
    "        # Check if data changed\n",
    "        if new_followers != old_followers or new_following != old_following:\n",
    "            df.loc[df['username'] == username, 'followers_count'] = new_followers\n",
    "            df.loc[df['username'] == username, 'following_count'] = new_following\n",
    "            print(f\"ğŸ”„ UPDATED: {old_followers:,}â†’{new_followers:,} followers, {old_following:,}â†’{new_following:,} following\")\n",
    "            updated_count += 1\n",
    "        else:\n",
    "            print(f\"âœ… Verified: {new_followers:,} followers, {new_following:,} following\")\n",
    "            unchanged_count += 1\n",
    "        \n",
    "        verified_count += 1\n",
    "    else:\n",
    "        print(\"âŒ Failed to extract\")\n",
    "    \n",
    "    # Delay\n",
    "    time.sleep(random.uniform(8, 12))\n",
    "    \n",
    "    # Save progress every 10 users\n",
    "    if verified_count % 10 == 0:\n",
    "        df.to_csv(INPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "        print(f\"  ğŸ’¾ Progress saved\\n\")\n",
    "\n",
    "# Close browser\n",
    "driver.quit()\n",
    "print(\"\\nğŸ”š Browser closed\")\n",
    "\n",
    "# Save final results\n",
    "df.to_csv(INPUT_CSV, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"âœ… RE-VERIFICATION COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Verified: {verified_count}\")\n",
    "print(f\"Updated:  {updated_count} (data changed)\")\n",
    "print(f\"Unchanged: {unchanged_count} (data correct)\")\n",
    "print(f\"\\nğŸ“ Saved to: {INPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7.6: Re-categorize \"Candidates\" based on Occupation\n",
    "\n",
    "The \"Candidates\" category contains 186 POIs from various fields. We'll re-categorize them based on their wikidata_occupation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Stage 7.5: Re-categorize Candidates\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load files\n",
    "manual_pois = pd.read_csv(POIS_DIR / \"Manual_Search_POIs_Unique.csv\", encoding='utf-8')\n",
    "print(f\"Total POIs: {len(manual_pois)}\")\n",
    "print(f\"Candidates before re-categorization: {len(manual_pois[manual_pois['source_folder'] == 'Candidates'])}\")\n",
    "\n",
    "# Load all wikidata files to get occupation data\n",
    "occupation_mapping = {}\n",
    "\n",
    "wikidata_files = list(POIS_DIR.glob('*/*_wikidata_detailed_with_twitter.csv'))\n",
    "print(f\"\\nLoading occupation data from {len(wikidata_files)} wikidata files...\")\n",
    "\n",
    "for file in wikidata_files:\n",
    "    try:\n",
    "        df_wiki = pd.read_csv(file, encoding='utf-8')\n",
    "        if 'Twitter_username' in df_wiki.columns and 'wikidata_occupation' in df_wiki.columns:\n",
    "            for _, row in df_wiki.iterrows():\n",
    "                username = row['Twitter_username']\n",
    "                occupation = row['wikidata_occupation']\n",
    "                if pd.notna(username) and pd.notna(occupation):\n",
    "                    occupation_mapping[username] = occupation\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(f\"Loaded occupation data for {len(occupation_mapping)} users\")\n",
    "\n",
    "# Define occupation to category mapping\n",
    "occupation_to_category = {\n",
    "    'journalist': 'iranian_journalists',\n",
    "    'reporter': 'iranian_journalists',\n",
    "    'news reader': 'iranian_journalists',\n",
    "    'correspondent': 'iranian_journalists',\n",
    "    \n",
    "    'politician': 'iranian_politicians',\n",
    "    'political activist': 'iranian_political_activists',\n",
    "    'human rights activist': 'iranian_human_rights_activists',\n",
    "    'activist': 'iranian_activists',\n",
    "    'feminist': 'iranian_feminists',\n",
    "    'dissident': 'iranian_dissidents',\n",
    "    \n",
    "    'actor': 'iranian_actors',\n",
    "    'film actor': 'iranian_film_actors',\n",
    "    'television actor': 'iranian_television_actors',\n",
    "    \n",
    "    'singer': 'iranian_singers',\n",
    "    'pop singer': 'iranian_pop_singers',\n",
    "    'rapper': 'iranian_rappers',\n",
    "    \n",
    "    'athlete': 'iranian_athletes',\n",
    "    'footballer': 'iranian_footballers',\n",
    "    'football manager': 'iranian_football_managers',\n",
    "    'volleyball player': 'iranian_volleyball_players',\n",
    "    'wrestler': 'iranian_wrestlers',\n",
    "    'weightlifter': 'iranian_weightlifters',\n",
    "    'taekwondo practitioner': 'iranian_taekwondo_practitioners',\n",
    "    \n",
    "    'physician': 'iranian_physicians',\n",
    "    'cardiologist': 'iranian_cardiologists',\n",
    "    \n",
    "    'scientist': 'iranian_scientists',\n",
    "    'economist': 'iranian_economists',\n",
    "    'writer': 'iranian_writers',\n",
    "    'blogger': 'iranian_bloggers',\n",
    "    'comedian': 'iranian_comedians',\n",
    "    'editor': 'iranian_editors',\n",
    "    'ayatollah': 'iranian_ayatollahs',\n",
    "}\n",
    "\n",
    "# Re-categorize Candidates\n",
    "recategorized_count = 0\n",
    "unchanged_count = 0\n",
    "no_occupation_count = 0\n",
    "\n",
    "for idx, row in manual_pois.iterrows():\n",
    "    if row['source_folder'] == 'Candidates':\n",
    "        username = row['Twitter_username']\n",
    "        \n",
    "        # Check if we have occupation data\n",
    "        if username in occupation_mapping:\n",
    "            occupations = occupation_mapping[username].lower()\n",
    "            new_category = None\n",
    "            \n",
    "            # Try to match occupation to category\n",
    "            for occ_keyword, category in occupation_to_category.items():\n",
    "                if occ_keyword in occupations:\n",
    "                    new_category = category\n",
    "                    break\n",
    "            \n",
    "            if new_category:\n",
    "                manual_pois.at[idx, 'source_folder'] = new_category\n",
    "                recategorized_count += 1\n",
    "            else:\n",
    "                unchanged_count += 1\n",
    "        else:\n",
    "            no_occupation_count += 1\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"RE-CATEGORIZATION RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Successfully re-categorized: {recategorized_count}\")\n",
    "print(f\"No matching category found: {unchanged_count}\")\n",
    "print(f\"No occupation data: {no_occupation_count}\")\n",
    "print(f\"\\nCandidates remaining: {len(manual_pois[manual_pois['source_folder'] == 'Candidates'])}\")\n",
    "\n",
    "# Save updated file\n",
    "output_file = POIS_DIR / \"Manual_Search_POIs_Unique_Recategorized.csv\"\n",
    "manual_pois.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nâœ“ Saved to: {output_file}\")\n",
    "\n",
    "# Show category distribution after re-categorization\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CATEGORY DISTRIBUTION AFTER RE-CATEGORIZATION\")\n",
    "print(f\"{'='*70}\")\n",
    "category_counts = manual_pois['source_folder'].value_counts().head(15)\n",
    "display(category_counts.to_frame('Count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 8: Statistical & Descriptive Analysis of POIs\n",
    "\n",
    "Compute overall statistics, category summaries, and occupation distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Stage 8: Statistical & Descriptive Analysis\n",
    "# Input:  POI_twitter_users_data.csv + Manual_Search_POIs_Unique.csv + wikidata files\n",
    "# Output: POI_twitter_users_statistics.csv\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# File paths\n",
    "twitter_data_csv = POIS_DIR / \"POI_twitter_users_data.csv\"\n",
    "manual_pois_csv = POIS_DIR / \"Manual_Search_POIs_Unique_Recategorized.csv\"\n",
    "output_csv = POIS_DIR / \"POI_twitter_users_statistics.csv\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STAGE 8: STATISTICAL & DESCRIPTIVE ANALYSIS OF POIs\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================\n",
    "# Step 1: Load and merge data with categories and occupations\n",
    "# ============================================\n",
    "print(\"\\n1. Loading and merging data...\")\n",
    "\n",
    "# Load Twitter data\n",
    "df_twitter = pd.read_csv(twitter_data_csv, encoding='utf-8')\n",
    "print(f\"   Loaded {len(df_twitter)} Twitter user records\")\n",
    "\n",
    "# Load Manual POIs with categories\n",
    "df_manual = pd.read_csv(manual_pois_csv, encoding='utf-8')\n",
    "print(f\"   Loaded {len(df_manual)} manual POI records\")\n",
    "\n",
    "# Merge to get categories\n",
    "df = df_twitter.merge(\n",
    "    df_manual[['Twitter_username', 'source_folder']].drop_duplicates(),\n",
    "    left_on='username',\n",
    "    right_on='Twitter_username',\n",
    "    how='left'\n",
    ")\n",
    "df.rename(columns={'source_folder': 'category'}, inplace=True)\n",
    "df.drop(columns=['Twitter_username'], inplace=True, errors='ignore')\n",
    "\n",
    "# Load occupation data from wikidata files\n",
    "print(\"   Loading occupation data from wikidata files...\")\n",
    "occupation_data = []\n",
    "\n",
    "# Find all wikidata_detailed_with_twitter.csv files\n",
    "wikidata_files = list(POIS_DIR.glob('*/*_wikidata_detailed_with_twitter.csv'))\n",
    "print(f\"   Found {len(wikidata_files)} wikidata files\")\n",
    "\n",
    "for file in wikidata_files:\n",
    "    try:\n",
    "        df_wiki = pd.read_csv(file, encoding='utf-8')\n",
    "        if 'Twitter_username' in df_wiki.columns and 'wikidata_occupation' in df_wiki.columns:\n",
    "            occupation_data.append(df_wiki[['Twitter_username', 'wikidata_occupation']])\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "if occupation_data:\n",
    "    df_occupations = pd.concat(occupation_data, ignore_index=True)\n",
    "    df_occupations = df_occupations.drop_duplicates(subset=['Twitter_username'])\n",
    "    print(f\"   Loaded {len(df_occupations)} occupation records\")\n",
    "    \n",
    "    # Merge occupation data\n",
    "    df = df.merge(\n",
    "        df_occupations,\n",
    "        left_on='username',\n",
    "        right_on='Twitter_username',\n",
    "        how='left'\n",
    "    )\n",
    "    df.drop(columns=['Twitter_username'], inplace=True, errors='ignore')\n",
    "else:\n",
    "    df['wikidata_occupation'] = None\n",
    "    print(\"   No occupation data found\")\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.drop_duplicates(subset=['username'])\n",
    "print(f\"   After merging and deduplication: {len(df)} unique POIs\")\n",
    "\n",
    "# ============================================\n",
    "# Step 2: Compute overall metrics\n",
    "# ============================================\n",
    "print(\"\\n2. Computing overall metrics...\")\n",
    "\n",
    "total_pois = len(df)\n",
    "avg_followers = df['followers_count'].mean()\n",
    "avg_following = df['following_count'].mean()\n",
    "avg_posts = df['statuses_count'].mean()\n",
    "\n",
    "# Round to whole numbers\n",
    "avg_followers = round(avg_followers) if pd.notna(avg_followers) else 0\n",
    "avg_following = round(avg_following) if pd.notna(avg_following) else 0\n",
    "avg_posts = round(avg_posts) if pd.notna(avg_posts) else 0\n",
    "\n",
    "overall_stats = {\n",
    "    'Total POIs': total_pois,\n",
    "    'Average Followers': avg_followers,\n",
    "    'Average Following': avg_following,\n",
    "    'Average Posts': avg_posts\n",
    "}\n",
    "\n",
    "print(f\"\\n   Overall Statistics:\")\n",
    "print(f\"   - Total POIs: {total_pois}\")\n",
    "print(f\"   - Average Followers: {avg_followers}\")\n",
    "print(f\"   - Average Following: {avg_following}\")\n",
    "print(f\"   - Average Posts: {avg_posts}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 3: Summary Table 1 - By Category\n",
    "# ============================================\n",
    "print(\"\\n3. Building Summary Table 1 (by category)...\")\n",
    "\n",
    "# Filter out rows without category\n",
    "df_with_category = df[df['category'].notna()].copy()\n",
    "\n",
    "summary_by_category = df_with_category.groupby('category').agg(\n",
    "    Num_POIs=('username', 'count'),\n",
    "    Avg_Followers=('followers_count', 'mean'),\n",
    "    Avg_Following=('following_count', 'mean'),\n",
    "    Avg_Posts=('statuses_count', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Round averages to whole numbers\n",
    "summary_by_category['Avg_Followers'] = summary_by_category['Avg_Followers'].round().astype(int)\n",
    "summary_by_category['Avg_Following'] = summary_by_category['Avg_Following'].round().astype(int)\n",
    "summary_by_category['Avg_Posts'] = summary_by_category['Avg_Posts'].round().astype(int)\n",
    "\n",
    "# Sort by number of POIs descending\n",
    "summary_by_category = summary_by_category.sort_values('Num_POIs', ascending=False)\n",
    "\n",
    "print(f\"   Created table with {len(summary_by_category)} categories\")\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Summary Table 2 - Occupation Distribution\n",
    "# ============================================\n",
    "print(\"\\n4. Building Summary Table 2 (occupation distribution)...\")\n",
    "\n",
    "# Split occupations and count\n",
    "occupation_counts = {}\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    occupations = row['wikidata_occupation']\n",
    "    \n",
    "    if pd.isna(occupations) or occupations == '':\n",
    "        continue\n",
    "    \n",
    "    # Split by semicolon and count each occupation\n",
    "    for occupation in str(occupations).split(';'):\n",
    "        occupation = occupation.strip()\n",
    "        if occupation:\n",
    "            occupation_counts[occupation] = occupation_counts.get(occupation, 0) + 1\n",
    "\n",
    "# Create dataframe\n",
    "occupation_data_list = []\n",
    "for occupation, count in occupation_counts.items():\n",
    "    percentage = round((count / total_pois) * 100, 1)\n",
    "    occupation_data_list.append({\n",
    "        'Occupation': occupation,\n",
    "        'Count': count,\n",
    "        'Percentage': percentage\n",
    "    })\n",
    "\n",
    "occupation_df = pd.DataFrame(occupation_data_list)\n",
    "\n",
    "# Sort by count descending\n",
    "occupation_df = occupation_df.sort_values('Count', ascending=False)\n",
    "\n",
    "print(f\"   Found {len(occupation_df)} unique occupations\")\n",
    "\n",
    "# ============================================\n",
    "# Step 5: Save to CSV with sections\n",
    "# ============================================\n",
    "print(\"\\n5. Saving results to CSV...\")\n",
    "\n",
    "with open(output_csv, 'w', encoding='utf-8') as f:\n",
    "    # Overall Statistics Section\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(\"OVERALL STATISTICS\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    for key, value in overall_stats.items():\n",
    "        f.write(f\"{key},{value}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # Table 1: Summary by Category\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(\"TABLE 1: SUMMARY BY CATEGORY\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    summary_by_category.to_csv(f, index=False)\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # Table 2: Occupation Distribution\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(\"TABLE 2: OCCUPATION DISTRIBUTION\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    occupation_df.to_csv(f, index=False)\n",
    "\n",
    "print(f\"   âœ“ Saved to: {output_csv}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 6: Display Results\n",
    "# ============================================\n",
    "print(\"\\n6. Displaying results...\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "for key, value in overall_stats.items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TABLE 1: SUMMARY BY CATEGORY\")\n",
    "print(\"=\" * 70)\n",
    "display(summary_by_category)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TABLE 2: OCCUPATION DISTRIBUTION (Top 20)\")\n",
    "print(\"=\" * 70)\n",
    "display(occupation_df.head(20))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ“ STAGE 8 COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nOutput file: {output_csv}\")\n",
    "print(f\"Total POIs analyzed: {total_pois}\")\n",
    "print(f\"Categories found: {len(summary_by_category)}\")\n",
    "print(f\"Unique occupations found: {len(occupation_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 9 - Collect Followers & Following Lists\n",
    "\n",
    "**Using WORKING METHOD: Extract @usernames from span elements**\n",
    "\n",
    "Test mode: 3 POIs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Stage 9: Collect Followers & Following Lists\n",
    "# Using WORKING METHOD: Extract @usernames from spans\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STAGE 9: COLLECT FOLLOWERS & FOLLOWING LISTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "POI_DATA_CSV = POIS_DIR / \"POI_twitter_users_data.csv\"\n",
    "CANDIDATES_DIR = POIS_DIR / \"Candidates\"\n",
    "CONNECTIONS_CSV = CANDIDATES_DIR / \"POIs_candidate_connections.csv\"\n",
    "STATISTICS_CSV = CANDIDATES_DIR / \"Candidates_statistics.csv\"\n",
    "COOKIES_FILE = POIS_DIR / \"x_cookies.json\"\n",
    "\n",
    "CANDIDATES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MAX_SCROLLS = 10  # Reduced for testing\n",
    "SCROLL_PAUSE = 2\n",
    "REQUEST_DELAY = (3, 5)\n",
    "\n",
    "print(f\"\\nâš™ï¸  Configuration:\")\n",
    "print(f\"   Max scrolls per list: {MAX_SCROLLS}\")\n",
    "print(f\"   Scroll pause: {SCROLL_PAUSE}s\")\n",
    "print(f\"   Request delay: {REQUEST_DELAY}s\")\n",
    "\n",
    "# ============================================\n",
    "# Helper Function - WORKING METHOD\n",
    "# ============================================\n",
    "\n",
    "def collect_usernames_from_page(driver, username, page_type=\"followers\"):\n",
    "    \"\"\"\n",
    "    Collect usernames using WORKING METHOD:\n",
    "    Find all span elements with text starting with @\n",
    "    \"\"\"\n",
    "    url = f\"https://twitter.com/{username}/{page_type}\"\n",
    "    \n",
    "    print(f\"      ğŸŒ URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Check for redirect to login\n",
    "    if \"login\" in driver.current_url.lower():\n",
    "        print(f\"      âŒ Redirected to login - authentication failed\")\n",
    "        return []\n",
    "    \n",
    "    collected_usernames = set()\n",
    "    \n",
    "    print(f\"      ğŸ“œ Scrolling and collecting...\")\n",
    "    for scroll_num in range(MAX_SCROLLS):\n",
    "        # WORKING METHOD: Get all spans and filter for @username\n",
    "        all_spans = driver.find_elements(By.TAG_NAME, \"span\")\n",
    "        \n",
    "        for span in all_spans:\n",
    "            try:\n",
    "                text = span.text.strip()\n",
    "                if text.startswith('@') and len(text) > 1:\n",
    "                    username_clean = text[1:].lower()\n",
    "                    # Skip the target user themselves\n",
    "                    if username_clean != username.lower():\n",
    "                        collected_usernames.add(username_clean)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE)\n",
    "        \n",
    "        # Show progress every 3 scrolls\n",
    "        if (scroll_num + 1) % 3 == 0:\n",
    "            print(f\"      ... Scroll {scroll_num + 1}/{MAX_SCROLLS}, found {len(collected_usernames)} so far\")\n",
    "    \n",
    "    return list(collected_usernames)\n",
    "\n",
    "# ============================================\n",
    "# Step 1: Load POIs\n",
    "# ============================================\n",
    "print(f\"\\nğŸ“‚ Step 1: Loading POI data...\")\n",
    "\n",
    "df_pois = pd.read_csv(POI_DATA_CSV, encoding='utf-8')\n",
    "df_active = df_pois[df_pois['account_status'] == 'active'].copy()\n",
    "\n",
    "# For testing, use only first 3 POIs\n",
    "target_users = df_active['username'].dropna().unique()[:3]  # TEST WITH 3 USERS\n",
    "\n",
    "print(f\"   âœ“ Loaded {len(df_pois)} POIs\")\n",
    "print(f\"   âœ“ Active users: {len(df_active)}\")\n",
    "print(f\"   ğŸ§ª TEST MODE: Processing only first {len(target_users)} users\")\n",
    "print(f\"   Users: {', '.join(target_users)}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 2: Initialize Browser\n",
    "# ============================================\n",
    "print(f\"\\nğŸŒ Step 2: Initializing browser...\")\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless')  # Run in background\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Load cookies\n",
    "driver.get(\"https://twitter.com\")\n",
    "time.sleep(2)\n",
    "\n",
    "with open(COOKIES_FILE, 'r') as f:\n",
    "    cookies = json.load(f)\n",
    "\n",
    "for cookie in cookies:\n",
    "    if 'sameSite' in cookie and cookie['sameSite'] not in ['Strict', 'Lax', 'None']:\n",
    "        cookie['sameSite'] = 'None'\n",
    "    driver.add_cookie(cookie)\n",
    "\n",
    "print(\"   âœ“ Browser initialized with cookies\")\n",
    "\n",
    "# ============================================\n",
    "# Step 3: Collect Connections\n",
    "# ============================================\n",
    "print(f\"\\nğŸ” Step 3: Collecting connections...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_connections = []\n",
    "processed_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "for idx, username in enumerate(target_users, 1):\n",
    "    print(f\"\\n[{idx}/{len(target_users)}] ğŸ‘¤ Processing: @{username}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # Collect FOLLOWERS\n",
    "        print(f\"   ğŸ“¥ Collecting FOLLOWERS...\")\n",
    "        followers = collect_usernames_from_page(driver, username, \"followers\")\n",
    "        print(f\"   âœ… Found {len(followers)} followers\")\n",
    "        \n",
    "        # Add to connections list\n",
    "        for follower in followers:\n",
    "            all_connections.append({\n",
    "                'target_username': username,\n",
    "                'other_username': follower,\n",
    "                'type': 'follower'\n",
    "            })\n",
    "        \n",
    "        # Delay before next request\n",
    "        delay = random.uniform(*REQUEST_DELAY)\n",
    "        print(f\"   â³ Waiting {delay:.1f}s before next request...\")\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        # Collect FOLLOWING\n",
    "        print(f\"   ğŸ“¤ Collecting FOLLOWING...\")\n",
    "        following = collect_usernames_from_page(driver, username, \"following\")\n",
    "        print(f\"   âœ… Found {len(following)} following\")\n",
    "        \n",
    "        # Add to connections list\n",
    "        for followed in following:\n",
    "            all_connections.append({\n",
    "                'target_username': username,\n",
    "                'other_username': followed,\n",
    "                'type': 'following'\n",
    "            })\n",
    "        \n",
    "        processed_count += 1\n",
    "        print(f\"   âœ… Completed @{username}\")\n",
    "        print(f\"   ğŸ“Š Total connections so far: {len(all_connections)}\")\n",
    "        \n",
    "        # Delay before next user\n",
    "        delay = random.uniform(*REQUEST_DELAY)\n",
    "        print(f\"   â³ Waiting {delay:.1f}s before next user...\")\n",
    "        time.sleep(delay)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ ERROR processing @{username}: {str(e)}\")\n",
    "        failed_count += 1\n",
    "        continue\n",
    "\n",
    "driver.quit()\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ COLLECTION COMPLETED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"âœ… Successfully processed: {processed_count} POIs\")\n",
    "print(f\"âŒ Failed: {failed_count} POIs\")\n",
    "print(f\"ğŸ“Š Total connections: {len(all_connections)}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Save Results\n",
    "# ============================================\n",
    "print(f\"\\nğŸ’¾ Step 4: Saving results...\")\n",
    "\n",
    "if len(all_connections) > 0:\n",
    "    df_connections = pd.DataFrame(all_connections)\n",
    "    df_connections = df_connections.drop_duplicates()\n",
    "    df_connections.to_csv(CONNECTIONS_CSV, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"   âœ… Saved {len(df_connections)} unique connections\")\n",
    "    print(f\"   ğŸ“ File: {CONNECTIONS_CSV}\")\n",
    "    \n",
    "    # Show preview\n",
    "    print(f\"\\nğŸ“‹ Preview of connections:\")\n",
    "    display(df_connections.head(20))\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nğŸ“Š Breakdown:\")\n",
    "    print(f\"   Followers: {len(df_connections[df_connections['type']=='follower'])}\")\n",
    "    print(f\"   Following: {len(df_connections[df_connections['type']=='following'])}\")\n",
    "else:\n",
    "    print(\"   âš ï¸  No connections collected!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… Stage 9 TEST completed - Check results above!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 9 - FULL RUN: All 343 POIs\n",
    "\n",
    "**With Safety Measures:**\n",
    "- Intermediate saves every 5 POIs\n",
    "- Browser restart every 20 POIs\n",
    "- Long delays between requests\n",
    "- Stop if blocked/redirected to login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Stage 9: FULL RUN - All POIs with Safety Measures\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STAGE 9: FULL RUN - COLLECT ALL FOLLOWERS & FOLLOWING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "POI_DATA_CSV = POIS_DIR / \"POI_twitter_users_data.csv\"\n",
    "CANDIDATES_DIR = POIS_DIR / \"Candidates\"\n",
    "CONNECTIONS_CSV = CANDIDATES_DIR / \"POIs_candidate_connections.csv\"\n",
    "STATISTICS_CSV = CANDIDATES_DIR / \"Candidates_statistics.csv\"\n",
    "COOKIES_FILE = POIS_DIR / \"x_cookies.json\"\n",
    "\n",
    "CANDIDATES_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# SAFETY CONFIGURATION\n",
    "MAX_SCROLLS = 15  # Reasonable limit per page\n",
    "SCROLL_PAUSE = 2.5  # Longer pause between scrolls\n",
    "REQUEST_DELAY = (5, 8)  # Longer delays between requests\n",
    "SAVE_INTERVAL = 5  # Save every 5 POIs\n",
    "RESTART_INTERVAL = 20  # Restart browser every 20 POIs\n",
    "COOLDOWN_AFTER_ERROR = 30  # Wait 30s after error\n",
    "\n",
    "print(f\"\\nâš™ï¸  SAFETY CONFIGURATION:\")\n",
    "print(f\"   Max scrolls per list: {MAX_SCROLLS}\")\n",
    "print(f\"   Scroll pause: {SCROLL_PAUSE}s\")\n",
    "print(f\"   Request delay: {REQUEST_DELAY[0]}-{REQUEST_DELAY[1]}s\")\n",
    "print(f\"   Save interval: Every {SAVE_INTERVAL} POIs\")\n",
    "print(f\"   Browser restart: Every {RESTART_INTERVAL} POIs\")\n",
    "print(f\"   Error cooldown: {COOLDOWN_AFTER_ERROR}s\")\n",
    "\n",
    "# ============================================\n",
    "# Helper Functions\n",
    "# ============================================\n",
    "\n",
    "def init_browser():\n",
    "    \"\"\"Initialize browser with cookies\"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    # Load cookies\n",
    "    driver.get(\"https://twitter.com\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    with open(COOKIES_FILE, 'r') as f:\n",
    "        cookies = json.load(f)\n",
    "    \n",
    "    for cookie in cookies:\n",
    "        if 'sameSite' in cookie and cookie['sameSite'] not in ['Strict', 'Lax', 'None']:\n",
    "            cookie['sameSite'] = 'None'\n",
    "        driver.add_cookie(cookie)\n",
    "    \n",
    "    return driver\n",
    "\n",
    "def collect_usernames_from_page(driver, username, page_type=\"followers\"):\n",
    "    \"\"\"\n",
    "    Collect usernames using WORKING METHOD\n",
    "    Returns: (usernames_list, blocked_flag)\n",
    "    \"\"\"\n",
    "    url = f\"https://twitter.com/{username}/{page_type}\"\n",
    "    \n",
    "    print(f\"      ğŸŒ {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(4)  # Longer initial wait\n",
    "    \n",
    "    # CHECK FOR BLOCKING\n",
    "    current_url = driver.current_url.lower()\n",
    "    if \"login\" in current_url or \"error\" in current_url:\n",
    "        print(f\"      ğŸš« BLOCKED! Redirected to: {current_url}\")\n",
    "        return [], True  # Return empty list and blocked flag\n",
    "    \n",
    "    collected_usernames = set()\n",
    "    no_new_users_count = 0\n",
    "    \n",
    "    print(f\"      ğŸ“œ Scrolling...\")\n",
    "    for scroll_num in range(MAX_SCROLLS):\n",
    "        previous_count = len(collected_usernames)\n",
    "        \n",
    "        # Get all spans and filter for @username\n",
    "        all_spans = driver.find_elements(By.TAG_NAME, \"span\")\n",
    "        \n",
    "        for span in all_spans:\n",
    "            try:\n",
    "                text = span.text.strip()\n",
    "                if text.startswith('@') and len(text) > 1:\n",
    "                    username_clean = text[1:].lower()\n",
    "                    if username_clean != username.lower():\n",
    "                        collected_usernames.add(username_clean)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Check if we got new users\n",
    "        new_count = len(collected_usernames)\n",
    "        if new_count == previous_count:\n",
    "            no_new_users_count += 1\n",
    "            if no_new_users_count >= 3:  # Stop if no new users for 3 scrolls\n",
    "                print(f\"      âœ“ No new users, stopping at scroll {scroll_num + 1}\")\n",
    "                break\n",
    "        else:\n",
    "            no_new_users_count = 0\n",
    "        \n",
    "        # Scroll down\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE)\n",
    "        \n",
    "        # Progress update\n",
    "        if (scroll_num + 1) % 5 == 0:\n",
    "            print(f\"      ... Scroll {scroll_num + 1}/{MAX_SCROLLS}: {new_count} users\")\n",
    "    \n",
    "    return list(collected_usernames), False  # Not blocked\n",
    "\n",
    "def save_progress(connections, filename):\n",
    "    \"\"\"Save intermediate results\"\"\"\n",
    "    if len(connections) > 0:\n",
    "        df = pd.DataFrame(connections)\n",
    "        df = df.drop_duplicates()\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        return len(df)\n",
    "    return 0\n",
    "\n",
    "# ============================================\n",
    "# Load POIs - ALL USERS\n",
    "# ============================================\n",
    "print(f\"\\nğŸ“‚ Step 1: Loading ALL POI data...\")\n",
    "\n",
    "df_pois = pd.read_csv(POI_DATA_CSV, encoding='utf-8')\n",
    "df_active = df_pois[df_pois['account_status'] == 'active'].copy()\n",
    "\n",
    "# ALL USERS - NO LIMIT\n",
    "target_users = df_active['username'].dropna().unique()\n",
    "\n",
    "print(f\"   âœ“ Total POIs: {len(df_pois)}\")\n",
    "print(f\"   âœ“ Active POIs: {len(df_active)}\")\n",
    "print(f\"   ğŸ¯ TARGET: Processing ALL {len(target_users)} users\")\n",
    "print(f\"\\nâš ï¸  THIS WILL TAKE SEVERAL HOURS!\")\n",
    "print(f\"âš ï¸  Estimated time: {len(target_users) * 3 / 60:.1f} minutes minimum\")\n",
    "\n",
    "# Check if previous results exist\n",
    "processed_users = []\n",
    "if CONNECTIONS_CSV.exists():\n",
    "    print(f\"\\nğŸ“‹ Found existing results: {CONNECTIONS_CSV}\")\n",
    "    existing_df = pd.read_csv(CONNECTIONS_CSV, encoding='utf-8')\n",
    "    processed_users = existing_df['target_username'].unique().tolist()\n",
    "    print(f\"   Already processed: {len(processed_users)} POIs\")\n",
    "    \n",
    "    response = input(\"\\nğŸ¤” Continue from where we left off? (yes/no): \")\n",
    "    if response.lower() == 'yes':\n",
    "        # Filter out already processed users\n",
    "        target_users = [u for u in target_users if u not in processed_users]\n",
    "        all_connections = existing_df.to_dict('records')\n",
    "        print(f\"   âœ… Continuing with {len(target_users)} remaining POIs\")\n",
    "    else:\n",
    "        all_connections = []\n",
    "        processed_users = []\n",
    "        print(f\"   ğŸ”„ Starting fresh\")\n",
    "else:\n",
    "    all_connections = []\n",
    "    print(f\"\\nğŸš€ Starting fresh collection\")\n",
    "\n",
    "# ============================================\n",
    "# Main Collection Loop\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"ğŸ” STARTING COLLECTION...\")\n",
    "print(f\"{'=' * 70}\\n\")\n",
    "\n",
    "driver = init_browser()\n",
    "print(\"âœ… Browser initialized\\n\")\n",
    "\n",
    "processed_count = len(processed_users)\n",
    "failed_count = 0\n",
    "blocked = False\n",
    "\n",
    "try:\n",
    "    for idx, username in enumerate(target_users, 1):\n",
    "        print(f\"[{processed_count + idx}/{len(target_users) + len(processed_users)}] ğŸ‘¤ @{username}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        try:\n",
    "            # Collect FOLLOWERS\n",
    "            print(f\"   ğŸ“¥ FOLLOWERS...\")\n",
    "            followers, blocked = collect_usernames_from_page(driver, username, \"followers\")\n",
    "            \n",
    "            if blocked:\n",
    "                print(f\"\\n{'=' * 70}\")\n",
    "                print(f\"ğŸš« BLOCKED! Stopping to avoid further issues.\")\n",
    "                print(f\"{'=' * 70}\")\n",
    "                print(f\"âœ… Processed: {processed_count + idx - 1} POIs\")\n",
    "                print(f\"ğŸ“Š Collected: {len(all_connections)} connections\")\n",
    "                print(f\"ğŸ’¾ Data saved to: {CONNECTIONS_CSV}\")\n",
    "                break\n",
    "            \n",
    "            print(f\"   âœ… {len(followers)} followers\")\n",
    "            \n",
    "            for follower in followers:\n",
    "                all_connections.append({\n",
    "                    'target_username': username,\n",
    "                    'other_username': follower,\n",
    "                    'type': 'follower'\n",
    "                })\n",
    "            \n",
    "            # Delay\n",
    "            delay = random.uniform(*REQUEST_DELAY)\n",
    "            print(f\"   â³ {delay:.1f}s...\")\n",
    "            time.sleep(delay)\n",
    "            \n",
    "            # Collect FOLLOWING\n",
    "            print(f\"   ğŸ“¤ FOLLOWING...\")\n",
    "            following, blocked = collect_usernames_from_page(driver, username, \"following\")\n",
    "            \n",
    "            if blocked:\n",
    "                print(f\"\\n{'=' * 70}\")\n",
    "                print(f\"ğŸš« BLOCKED! Stopping to avoid further issues.\")\n",
    "                print(f\"{'=' * 70}\")\n",
    "                print(f\"âœ… Processed: {processed_count + idx - 1} POIs\")\n",
    "                print(f\"ğŸ“Š Collected: {len(all_connections)} connections\")\n",
    "                print(f\"ğŸ’¾ Data saved to: {CONNECTIONS_CSV}\")\n",
    "                break\n",
    "            \n",
    "            print(f\"   âœ… {len(following)} following\")\n",
    "            \n",
    "            for followed in following:\n",
    "                all_connections.append({\n",
    "                    'target_username': username,\n",
    "                    'other_username': followed,\n",
    "                    'type': 'following'\n",
    "                })\n",
    "            \n",
    "            processed_count += 1\n",
    "            print(f\"   ğŸ“Š Total: {len(all_connections)} connections\")\n",
    "            \n",
    "            # SAVE INTERVAL\n",
    "            if processed_count % SAVE_INTERVAL == 0:\n",
    "                saved_count = save_progress(all_connections, CONNECTIONS_CSV)\n",
    "                print(f\"\\n   ğŸ’¾ SAVED: {saved_count} connections\")\n",
    "                print(f\"   Progress: {processed_count}/{len(target_users) + len(processed_users)} POIs\\n\")\n",
    "            \n",
    "            # BROWSER RESTART\n",
    "            if processed_count % RESTART_INTERVAL == 0:\n",
    "                print(f\"\\n   ğŸ”„ RESTARTING BROWSER (every {RESTART_INTERVAL} POIs)...\")\n",
    "                driver.quit()\n",
    "                time.sleep(5)\n",
    "                driver = init_browser()\n",
    "                print(f\"   âœ… Browser restarted\\n\")\n",
    "            \n",
    "            # Delay before next user\n",
    "            delay = random.uniform(*REQUEST_DELAY)\n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ ERROR: {str(e)}\")\n",
    "            failed_count += 1\n",
    "            \n",
    "            # Save on error\n",
    "            save_progress(all_connections, CONNECTIONS_CSV)\n",
    "            \n",
    "            # Cooldown after error\n",
    "            print(f\"   â³ Cooldown {COOLDOWN_AFTER_ERROR}s...\")\n",
    "            time.sleep(COOLDOWN_AFTER_ERROR)\n",
    "            continue\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"ğŸ COLLECTION COMPLETED\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"âœ… Successfully processed: {processed_count} POIs\")\n",
    "    print(f\"âŒ Failed: {failed_count} POIs\")\n",
    "    print(f\"ğŸ“Š Total connections: {len(all_connections)}\")\n",
    "\n",
    "# ============================================\n",
    "# Final Save\n",
    "# ============================================\n",
    "print(f\"\\nğŸ’¾ Saving final results...\")\n",
    "\n",
    "if len(all_connections) > 0:\n",
    "    df_connections = pd.DataFrame(all_connections)\n",
    "    df_connections = df_connections.drop_duplicates()\n",
    "    df_connections.to_csv(CONNECTIONS_CSV, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"   âœ… Saved {len(df_connections)} unique connections\")\n",
    "    print(f\"   ğŸ“ {CONNECTIONS_CSV}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\nğŸ“Š Final Statistics:\")\n",
    "    print(f\"   Followers: {len(df_connections[df_connections['type']=='follower']):,}\")\n",
    "    print(f\"   Following: {len(df_connections[df_connections['type']=='following']):,}\")\n",
    "    print(f\"   Unique POIs: {df_connections['target_username'].nunique()}\")\n",
    "    print(f\"   Unique connections: {df_connections['other_username'].nunique()}\")\n",
    "    \n",
    "    # Preview\n",
    "    print(f\"\\nğŸ“‹ Sample connections:\")\n",
    "    display(df_connections.head(20))\n",
    "else:\n",
    "    print(\"   âš ï¸  No connections collected\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"âœ… Stage 9 COMPLETE!\")\n",
    "print(f\"{'=' * 70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 9.1 - Generate Statistics from Collected Data\n",
    "\n",
    "Analyze the followers/following connections and create comprehensive statistics per POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Stage 9.1: Generate Statistics from Connections\n",
    "# Input:  POIs/Candidates/POIs_candidate_connections.csv\n",
    "#         POIs/POI_twitter_users_data.csv\n",
    "# Output: POIs/Candidates/Candidates_statistics.csv\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STAGE 9.1: GENERATE STATISTICS FROM CONNECTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# File paths\n",
    "POIS_DIR = Path(\"POIs\")\n",
    "CANDIDATES_DIR = POIS_DIR / \"Candidates\"\n",
    "CONNECTIONS_CSV = CANDIDATES_DIR / \"POIs_candidate_connections.csv\"\n",
    "POI_DATA_CSV = POIS_DIR / \"POI_twitter_users_data.csv\"\n",
    "STATISTICS_CSV = CANDIDATES_DIR / \"Candidates_statistics.csv\"\n",
    "\n",
    "# ============================================\n",
    "# Step 1: Load Data\n",
    "# ============================================\n",
    "print(f\"\\nğŸ“‚ Step 1: Loading data files...\")\n",
    "\n",
    "# Load connections\n",
    "if not CONNECTIONS_CSV.exists():\n",
    "    print(f\"   âŒ ERROR: Connections file not found!\")\n",
    "    print(f\"   Expected: {CONNECTIONS_CSV}\")\n",
    "    print(f\"   Please run Stage 9 first to collect connections.\")\n",
    "else:\n",
    "    df_connections = pd.read_csv(CONNECTIONS_CSV, encoding='utf-8')\n",
    "    print(f\"   âœ“ Loaded connections: {len(df_connections):,} records\")\n",
    "    print(f\"   âœ“ Unique POIs in connections: {df_connections['target_username'].nunique()}\")\n",
    "    \n",
    "    # Load POI metadata\n",
    "    df_pois = pd.read_csv(POI_DATA_CSV, encoding='utf-8')\n",
    "    print(f\"   âœ“ Loaded POI metadata: {len(df_pois)} POIs\")\n",
    "\n",
    "# ============================================\n",
    "# Step 2: Calculate Statistics per POI\n",
    "# ============================================\n",
    "print(f\"\\nğŸ“Š Step 2: Calculating statistics per POI...\")\n",
    "\n",
    "stats_list = []\n",
    "\n",
    "# Get all unique POIs from connections\n",
    "poi_usernames = df_connections['target_username'].unique()\n",
    "\n",
    "for username in poi_usernames:\n",
    "    # Filter connections for this POI\n",
    "    poi_connections = df_connections[df_connections['target_username'] == username]\n",
    "    \n",
    "    # Count followers and following\n",
    "    followers = poi_connections[poi_connections['type'] == 'follower']\n",
    "    following = poi_connections[poi_connections['type'] == 'following']\n",
    "    \n",
    "    followers_collected = len(followers)\n",
    "    following_collected = len(following)\n",
    "    total_connections = followers_collected + following_collected\n",
    "    \n",
    "    # Get POI metadata\n",
    "    poi_info = df_pois[df_pois['username'] == username]\n",
    "    \n",
    "    if len(poi_info) > 0:\n",
    "        full_name = poi_info['display_name'].iloc[0]\n",
    "        followers_count = poi_info['followers_count'].iloc[0] if 'followers_count' in poi_info.columns else 0\n",
    "        following_count = poi_info['following_count'].iloc[0] if 'following_count' in poi_info.columns else 0\n",
    "        posts_count = poi_info['posts_count'].iloc[0] if 'posts_count' in poi_info.columns else 0\n",
    "    else:\n",
    "        full_name = ''\n",
    "        followers_count = 0\n",
    "        following_count = 0\n",
    "        posts_count = 0\n",
    "    \n",
    "    # Calculate collection percentages\n",
    "    followers_percentage = (followers_collected / followers_count * 100) if followers_count > 0 else 0\n",
    "    following_percentage = (following_collected / following_count * 100) if following_count > 0 else 0\n",
    "    \n",
    "    stats_list.append({\n",
    "        'Twitter_username': username,\n",
    "        'Full_Name': full_name,\n",
    "        'Total_Followers': followers_count,\n",
    "        'Followers_Collected': followers_collected,\n",
    "        'Followers_Percentage': round(followers_percentage, 2),\n",
    "        'Total_Following': following_count,\n",
    "        'Following_Collected': following_collected,\n",
    "        'Following_Percentage': round(following_percentage, 2),\n",
    "        'Total_Connections_Collected': total_connections,\n",
    "        'Posts_Count': posts_count\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_stats = pd.DataFrame(stats_list)\n",
    "\n",
    "# Sort by total connections collected (descending)\n",
    "df_stats = df_stats.sort_values('Total_Connections_Collected', ascending=False)\n",
    "\n",
    "print(f\"   âœ“ Calculated statistics for {len(df_stats)} POIs\")\n",
    "\n",
    "# ============================================\n",
    "# Step 3: Overall Statistics\n",
    "# ============================================\n",
    "print(f\"\\nğŸ“ˆ Step 3: Overall statistics...\")\n",
    "\n",
    "total_followers_collected = df_stats['Followers_Collected'].sum()\n",
    "total_following_collected = df_stats['Following_Collected'].sum()\n",
    "total_connections_collected = df_stats['Total_Connections_Collected'].sum()\n",
    "\n",
    "avg_followers_collected = df_stats['Followers_Collected'].mean()\n",
    "avg_following_collected = df_stats['Following_Collected'].mean()\n",
    "\n",
    "print(f\"\\n   Overall Summary:\")\n",
    "print(f\"   {'â”€' * 50}\")\n",
    "print(f\"   POIs processed: {len(df_stats)}\")\n",
    "print(f\"   Total followers collected: {total_followers_collected:,}\")\n",
    "print(f\"   Total following collected: {total_following_collected:,}\")\n",
    "print(f\"   Total connections: {total_connections_collected:,}\")\n",
    "print(f\"   {'â”€' * 50}\")\n",
    "print(f\"   Avg followers per POI: {avg_followers_collected:.1f}\")\n",
    "print(f\"   Avg following per POI: {avg_following_collected:.1f}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 4: Identify Top POIs\n",
    "# ============================================\n",
    "print(f\"\\nğŸ† Step 4: Top POIs by connections collected...\")\n",
    "\n",
    "print(f\"\\n   Top 10 POIs by Total Connections:\")\n",
    "print(f\"   {'â”€' * 70}\")\n",
    "top_10 = df_stats.head(10)[['Twitter_username', 'Full_Name', 'Total_Connections_Collected']]\n",
    "for idx, row in top_10.iterrows():\n",
    "    print(f\"   {row['Twitter_username']:20s} | {row['Full_Name']:30s} | {row['Total_Connections_Collected']:,}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 5: Save Statistics\n",
    "# ============================================\n",
    "print(f\"\\nğŸ’¾ Step 5: Saving statistics...\")\n",
    "\n",
    "df_stats.to_csv(STATISTICS_CSV, index=False, encoding='utf-8')\n",
    "print(f\"   âœ“ Saved to: {STATISTICS_CSV}\")\n",
    "print(f\"   âœ“ Total records: {len(df_stats)}\")\n",
    "\n",
    "# ============================================\n",
    "# Step 6: Display Results\n",
    "# ============================================\n",
    "print(f\"\\nğŸ“‹ Step 6: Preview of statistics...\")\n",
    "\n",
    "print(f\"\\n   Full Statistics Table:\")\n",
    "display(df_stats)\n",
    "\n",
    "# Additional analysis\n",
    "print(f\"\\nğŸ“Š Additional Analysis:\")\n",
    "print(f\"   {'â”€' * 50}\")\n",
    "print(f\"   POIs with 100+ followers collected: {len(df_stats[df_stats['Followers_Collected'] >= 100])}\")\n",
    "print(f\"   POIs with 100+ following collected: {len(df_stats[df_stats['Following_Collected'] >= 100])}\")\n",
    "print(f\"   POIs with 200+ total connections: {len(df_stats[df_stats['Total_Connections_Collected'] >= 200])}\")\n",
    "\n",
    "# Collection efficiency\n",
    "avg_followers_pct = df_stats['Followers_Percentage'].mean()\n",
    "avg_following_pct = df_stats['Following_Percentage'].mean()\n",
    "print(f\"   {'â”€' * 50}\")\n",
    "print(f\"   Avg collection rate (followers): {avg_followers_pct:.2f}%\")\n",
    "print(f\"   Avg collection rate (following): {avg_following_pct:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"âœ… Stage 9.1 COMPLETED!\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"ğŸ“ Output file: {STATISTICS_CSV}\")\n",
    "print(f\"ğŸ“Š Statistics generated for {len(df_stats)} POIs\")\n",
    "print(f\"ğŸ”— Total connections analyzed: {total_connections_collected:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 10: Collect Full Metadata for All Candidate Users\n",
    "Extract complete Twitter metadata for every unique user found in Stage 9 connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ×”×ª×§× ×ª geckodriver ×× ×œ× ××•×ª×§×Ÿ\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import geckodriver_autoinstaller\n",
    "    print(\"âœ… geckodriver-autoinstaller ×›×‘×¨ ××•×ª×§×Ÿ\")\n",
    "except ImportError:\n",
    "    print(\"ğŸ“¦ ××ª×§×™×Ÿ geckodriver-autoinstaller...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"geckodriver-autoinstaller\"])\n",
    "    import geckodriver_autoinstaller\n",
    "    print(\"âœ… ×”×ª×§× ×” ×”×•×©×œ××”!\")\n",
    "\n",
    "# ×”×ª×§×Ÿ geckodriver\n",
    "geckodriver_autoinstaller.install()\n",
    "\n",
    "# ============================================\n",
    "# STAGE 9.2: COLLECT CANDIDATE USER METADATA\n",
    "# FIREFOX VERSION - STABLE ALTERNATIVE\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š STAGE 9.2: FIREFOX VERSION - STABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================\n",
    "# SETUP PATHS\n",
    "# ============================================\n",
    "IRAN_ROOT = Path.cwd()\n",
    "POIS_DIR = IRAN_ROOT / \"POIs\"\n",
    "CANDIDATES_DIR = POIS_DIR / \"Candidates\"\n",
    "COOKIES_FILE = POIS_DIR / \"x_cookies.json\"\n",
    "\n",
    "print(f\"ğŸ“ Working directory: {IRAN_ROOT}\")\n",
    "print(f\"ğŸ“ POIs directory: {POIS_DIR}\")\n",
    "print(f\"ğŸ“ Candidates directory: {CANDIDATES_DIR}\\n\")\n",
    "\n",
    "# File paths\n",
    "CONNECTIONS_CSV = CANDIDATES_DIR / \"POIs_candidate_connections.csv\"\n",
    "OUTPUT_CSV = CANDIDATES_DIR / \"Candidates_user_data.csv\"\n",
    "CHECKPOINT_CSV = CANDIDATES_DIR / \"Candidates_user_data_checkpoint.csv\"\n",
    "LOG_FILE = CANDIDATES_DIR / \"collection_log.txt\"\n",
    "\n",
    "print(f\"ğŸ“‚ Input: {CONNECTIONS_CSV}\")\n",
    "print(f\"ğŸ“‚ Output: {OUTPUT_CSV}\")\n",
    "print(f\"ğŸ’¾ Checkpoint: {CHECKPOINT_CSV}\")\n",
    "print(f\"ğŸ“ Log: {LOG_FILE}\")\n",
    "print(f\"ğŸ” Cookies: {COOKIES_FILE} ({'âœ… Found' if COOKIES_FILE.exists() else 'âŒ Missing'})\\n\")\n",
    "\n",
    "# ============================================\n",
    "# OPTIMIZED CONFIGURATION FOR FASTER COLLECTION\n",
    "# ============================================\n",
    "REQUEST_DELAY = (2, 4)               # FASTER: 2-4s between requests (was 5-10)\n",
    "LONG_DELAY = (30, 60)                # FASTER: 30-60s every 50 users (was 2-5 min)\n",
    "LONG_DELAY_INTERVAL = 50             # Every 50 users (was 30)\n",
    "BLOCK_WAIT_TIME = (600, 900)         # 10-15 minutes if blocked (was 20-30)\n",
    "MAX_CONSECUTIVE_FAILURES = 5         # More tolerance before block check (was 3)\n",
    "PAGE_LOAD_WAIT = 3                   # FASTER: 3 seconds to load (was 5)\n",
    "CHECKPOINT_SAVE_INTERVAL = 10        # Save every 10 users (was 5)\n",
    "BROWSER_RESTART_INTERVAL = 150       # Restart every 150 users (was 100)\n",
    "\n",
    "TEST_MODE = False  # â— FULL RUN - ALL USERS\n",
    "\n",
    "print(\"âš™ï¸  OPTIMIZED CONFIGURATION - FASTER COLLECTION:\")\n",
    "print(f\"   âš¡ Request delay: {REQUEST_DELAY[0]}-{REQUEST_DELAY[1]}s (FASTER - was 5-10s)\")\n",
    "print(f\"   âš¡ Long delay every {LONG_DELAY_INTERVAL} users: {LONG_DELAY[0]}-{LONG_DELAY[1]}s (FASTER - was 2-5 min)\")\n",
    "print(f\"   ğŸ”„ Browser restart: every {BROWSER_RESTART_INTERVAL} users (LESS FREQUENT - was 100)\")\n",
    "print(f\"   ğŸ’¾ Checkpoint: every {CHECKPOINT_SAVE_INTERVAL} users (was 5)\")\n",
    "print(f\"   âš ï¸  FULL RUN MODE: Will process ALL users\")\n",
    "print(f\"   â±ï¸  Estimated time: ~1 DAY (much faster!)\\n\")\n",
    "\n",
    "# ============================================\n",
    "# ENHANCED LOGGING\n",
    "# ============================================\n",
    "def log_message(msg, level=\"INFO\"):\n",
    "    ts = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"[{ts}] [{level}] {msg}\\n\")\n",
    "    if level in [\"ERROR\", \"BLOCK\", \"WARNING\"]:\n",
    "        print(f\"  ğŸ“ LOG: {msg}\")\n",
    "\n",
    "# ============================================\n",
    "# LOAD DATA\n",
    "# ============================================\n",
    "print(\"ğŸ“¥ Loading connections data...\")\n",
    "log_message(\"=== NEW RUN STARTED ===\")\n",
    "log_message(\"Starting high-quality metadata collection\")\n",
    "\n",
    "df_connections = pd.read_csv(CONNECTIONS_CSV, encoding='utf-8')\n",
    "print(f\"   âœ… Loaded {len(df_connections):,} connection records\")\n",
    "log_message(f\"Loaded {len(df_connections):,} connection records\")\n",
    "\n",
    "# Extract and validate usernames\n",
    "all_usernames = set()\n",
    "invalid_count = 0\n",
    "for username in df_connections['other_username'].unique():\n",
    "    if pd.notna(username):\n",
    "        username = str(username).strip()\n",
    "        if re.match(r'^[a-zA-Z0-9_]{1,15}$', username):\n",
    "            all_usernames.add(username)\n",
    "        else:\n",
    "            invalid_count += 1\n",
    "\n",
    "print(f\"   âœ… {len(all_usernames):,} valid usernames\")\n",
    "print(f\"   âš ï¸  {invalid_count} invalid usernames filtered out\\n\")\n",
    "log_message(f\"Valid usernames: {len(all_usernames):,}, Invalid: {invalid_count}\")\n",
    "\n",
    "# ============================================\n",
    "# RESUME FROM CHECKPOINT\n",
    "# ============================================\n",
    "processed_usernames = set()\n",
    "existing_data = []\n",
    "\n",
    "if CHECKPOINT_CSV.exists():\n",
    "    print(\"ğŸ’¾ Checkpoint found - resuming previous run...\")\n",
    "    log_message(\"Loading checkpoint for resume\")\n",
    "    try:\n",
    "        df_checkpoint = pd.read_csv(CHECKPOINT_CSV, encoding='utf-8')\n",
    "        if 'username' in df_checkpoint.columns:\n",
    "            processed_usernames = set(df_checkpoint['username'].unique())\n",
    "            existing_data = df_checkpoint.to_dict('records')\n",
    "            print(f\"   âœ… Found {len(processed_usernames):,} previously processed users\")\n",
    "            print(f\"   â© Resuming from user #{len(processed_usernames) + 1}\\n\")\n",
    "            log_message(f\"Resuming from {len(processed_usernames):,} users\")\n",
    "        else:\n",
    "            print(\"   âš ï¸  Invalid checkpoint format - starting fresh\\n\")\n",
    "            log_message(\"Invalid checkpoint, starting fresh\", \"WARNING\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸  Checkpoint error: {e} - starting fresh\\n\")\n",
    "        log_message(f\"Checkpoint error: {e}\", \"ERROR\")\n",
    "\n",
    "# Filter remaining users\n",
    "target_usernames = sorted([u for u in all_usernames if u not in processed_usernames])\n",
    "\n",
    "print(f\"ğŸ“‹ PROCESSING SUMMARY:\")\n",
    "print(f\"   Total unique users: {len(all_usernames):,}\")\n",
    "print(f\"   Already processed: {len(processed_usernames):,}\")\n",
    "print(f\"   Remaining to process: {len(target_usernames):,}\")\n",
    "\n",
    "if len(target_usernames) == 0:\n",
    "    print(\"\\nâœ… ALL USERS ALREADY PROCESSED!\")\n",
    "    log_message(\"All users already processed - collection complete\")\n",
    "else:\n",
    "    estimated_hours = (len(target_usernames) * 7) / 3600  # ~7s average per user\n",
    "    print(f\"   â±ï¸  Estimated time: {estimated_hours:.1f} hours ({estimated_hours/24:.1f} days)\")\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    \n",
    "    # ============================================\n",
    "    # ENHANCED BROWSER SETUP\n",
    "    # ============================================\n",
    "    USER_AGENTS = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    ]\n",
    "    \n",
    "    def init_browser():\n",
    "        \"\"\"Initialize Firefox browser\"\"\"\n",
    "        print(\"ğŸ¦Š Initializing Firefox WebDriver...\")\n",
    "        log_message(\"Initializing Firefox\")\n",
    "        \n",
    "        options = Options()\n",
    "        # Firefox headless mode\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--width=1920')\n",
    "        options.add_argument('--height=1080')\n",
    "        \n",
    "        # Anti-detection preferences\n",
    "        options.set_preference(\"dom.webdriver.enabled\", False)\n",
    "        options.set_preference('useAutomationExtension', False)\n",
    "        options.set_preference(\"general.useragent.override\", random.choice(USER_AGENTS))\n",
    "        \n",
    "        driver = webdriver.Firefox(options=options)\n",
    "        driver.set_page_load_timeout(30)\n",
    "        \n",
    "        # Note: CDP commands are Chrome-only, not needed for Firefox\n",
    "        # Firefox anti-detection is handled via preferences above\n",
    "        \n",
    "        return driver\n",
    "    \n",
    "    def load_cookies(driver):\n",
    "        \"\"\"Load Twitter cookies\"\"\"\n",
    "        if not COOKIES_FILE.exists():\n",
    "            print(\"âš ï¸  No cookies file!\")\n",
    "            log_message(\"No cookies file found\", \"WARNING\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            driver.get(\"https://x.com\")\n",
    "            time.sleep(3)\n",
    "            driver.delete_all_cookies()\n",
    "            \n",
    "            with open(COOKIES_FILE, 'r', encoding='utf-8') as f:\n",
    "                cookies = json.load(f)\n",
    "            \n",
    "            for cookie in cookies:\n",
    "                try:\n",
    "                    cookie.pop('sameSite', None)\n",
    "                    cookie.pop('storeId', None)\n",
    "                    driver.add_cookie(cookie)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            time.sleep(3)\n",
    "            log_message(\"Cookies loaded successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Cookie loading failed: {e}\")\n",
    "            log_message(f\"Cookie loading failed: {e}\", \"ERROR\")\n",
    "            return False\n",
    "    \n",
    "    def check_if_blocked(driver):\n",
    "        \"\"\"Check if Twitter blocked us\"\"\"\n",
    "        try:\n",
    "            driver.get(\"https://x.com/explore\")\n",
    "            time.sleep(5)\n",
    "            \n",
    "            url = driver.current_url.lower()\n",
    "            text = driver.find_element(By.TAG_NAME, 'body').text.lower()\n",
    "            \n",
    "            if \"login\" in url or \"error\" in url:\n",
    "                return True, \"Redirected to login/error\"\n",
    "            \n",
    "            if \"rate limit\" in text or \"try again later\" in text:\n",
    "                return True, \"Rate limit message detected\"\n",
    "            \n",
    "            if len(text) < 100:\n",
    "                return True, \"Page too short (blocked)\"\n",
    "            \n",
    "            return False, None\n",
    "            \n",
    "        except Exception as e:\n",
    "            return True, f\"Check failed: {str(e)[:50]}\"\n",
    "    \n",
    "    # ============================================\n",
    "    # HIGH-QUALITY METADATA EXTRACTION\n",
    "    # ============================================\n",
    "    def extract_user_metadata(driver, username):\n",
    "        \"\"\"\n",
    "        Extract metadata with multiple fallback methods\n",
    "        Returns dict with metadata or error\n",
    "        \"\"\"\n",
    "        try:\n",
    "            url = f\"https://x.com/{username}\"\n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for page to load and JavaScript to execute\n",
    "            time.sleep(PAGE_LOAD_WAIT)\n",
    "            \n",
    "            # Wait for profile content to appear (up to 10 seconds)\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                )\n",
    "                # Extra wait for dynamic content\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Add extra random delay to appear more human\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "            \n",
    "            # Check for blocks/errors\n",
    "            current_url = driver.current_url.lower()\n",
    "            if \"login\" in current_url or \"error\" in current_url:\n",
    "                log_message(f\"Blocked on {username}: redirected to {current_url}\", \"BLOCK\")\n",
    "                return {'error': 'blocked', 'username': username}\n",
    "            \n",
    "            # Get page text\n",
    "            try:\n",
    "                page_text = driver.find_element(By.TAG_NAME, 'body').text\n",
    "            except:\n",
    "                log_message(f\"Failed to get page text for {username}\", \"ERROR\")\n",
    "                return {'error': 'no_page_text', 'username': username}\n",
    "            \n",
    "            # Check for suspended/not found\n",
    "            page_text_lower = page_text.lower()\n",
    "            if \"doesn't exist\" in page_text_lower or \"suspended\" in page_text_lower:\n",
    "                log_message(f\"Account not found: {username}\")\n",
    "                return {'error': 'not_found', 'username': username}\n",
    "            \n",
    "            if \"rate limit\" in page_text_lower or \"try again later\" in page_text_lower:\n",
    "                log_message(f\"Rate limited on {username}\", \"BLOCK\")\n",
    "                return {'error': 'rate_limited', 'username': username}\n",
    "            \n",
    "            # Initialize metadata\n",
    "            metadata = {\n",
    "                'username': username,\n",
    "                'display_name': None,\n",
    "                'description': None,\n",
    "                'followers_count': None,\n",
    "                'following_count': None,\n",
    "                'statuses_count': None,\n",
    "                'verified': False,\n",
    "                'created_at': None,\n",
    "                'location': None,\n",
    "                'url': None,\n",
    "                'profile_image_url': None,\n",
    "                'profile_banner_url': None\n",
    "            }\n",
    "            \n",
    "            # ============================================\n",
    "            # EXTRACT DISPLAY NAME - Multiple methods\n",
    "            # ============================================\n",
    "            try:\n",
    "                # Method 1: From title\n",
    "                title = driver.title\n",
    "                if '(@' in title:\n",
    "                    name = title.split('(@')[0].strip()\n",
    "                    if name and name not in ['Twitter', 'X']:\n",
    "                        metadata['display_name'] = name\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if not metadata['display_name']:\n",
    "                try:\n",
    "                    # Method 2: From page text - look for pattern \"Name @username\"\n",
    "                    lines = page_text.split('\\n')\n",
    "                    for i, line in enumerate(lines):\n",
    "                        if f'@{username}' in line and i > 0:\n",
    "                            potential_name = lines[i-1].strip()\n",
    "                            if potential_name and len(potential_name) < 50 and not potential_name.startswith('@'):\n",
    "                                metadata['display_name'] = potential_name\n",
    "                                break\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # ============================================\n",
    "            # EXTRACT BIO/DESCRIPTION\n",
    "            # ============================================\n",
    "            try:\n",
    "                bio_elem = driver.find_element(By.XPATH, \"//div[@data-testid='UserDescription']\")\n",
    "                if bio_elem.text:\n",
    "                    metadata['description'] = bio_elem.text.strip().replace('\\n', ' ')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # ============================================\n",
    "            # EXTRACT FOLLOWERS - Multiple patterns\n",
    "            # ============================================\n",
    "            # DEBUG: Print page text snippet for first user\n",
    "            if position == 1:\n",
    "                print(f\"\\nğŸ” DEBUG - Page text sample (first 500 chars):\")\n",
    "                print(page_text[:500])\n",
    "                print(f\"\\nğŸ” Looking for followers/following in text...\")\n",
    "            \n",
    "            followers_patterns = [\n",
    "                r'(\\d{1,3}(?:,\\d{3})*|\\d+)\\s*Followers',\n",
    "                r'(\\d{1,3}(?:,\\d{3})*|\\d+)\\s*Follower',\n",
    "                r'Followers\\s*(\\d{1,3}(?:,\\d{3})*|\\d+)',\n",
    "            ]\n",
    "            \n",
    "            for pattern in followers_patterns:\n",
    "                match = re.search(pattern, page_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    try:\n",
    "                        count_str = match.group(1).replace(',', '').upper()\n",
    "                        if 'K' in count_str:\n",
    "                            metadata['followers_count'] = int(float(count_str.replace('K', '')) * 1000)\n",
    "                        elif 'M' in count_str:\n",
    "                            metadata['followers_count'] = int(float(count_str.replace('M', '')) * 1000000)\n",
    "                        else:\n",
    "                            metadata['followers_count'] = int(count_str)\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            # ============================================\n",
    "            # EXTRACT FOLLOWING - Multiple patterns\n",
    "            # ============================================\n",
    "            following_patterns = [\n",
    "                r'(\\d{1,3}(?:,\\d{3})*|\\d+)\\s*Following',\n",
    "                r'Following\\s*(\\d{1,3}(?:,\\d{3})*|\\d+)',\n",
    "            ]\n",
    "            \n",
    "            for pattern in following_patterns:\n",
    "                match = re.search(pattern, page_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    try:\n",
    "                        count_str = match.group(1).replace(',', '').upper()\n",
    "                        if 'K' in count_str:\n",
    "                            metadata['following_count'] = int(float(count_str.replace('K', '')) * 1000)\n",
    "                        elif 'M' in count_str:\n",
    "                            metadata['following_count'] = int(float(count_str.replace('M', '')) * 1000000)\n",
    "                        else:\n",
    "                            metadata['following_count'] = int(count_str)\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            # ============================================\n",
    "            # EXTRACT POSTS/TWEETS COUNT\n",
    "            # ============================================\n",
    "            posts_patterns = [\n",
    "                r'(\\d{1,3}(?:,\\d{3})*|\\d+)\\s*posts?',\n",
    "                r'(\\d{1,3}(?:,\\d{3})*|\\d+)\\s*Tweets?',\n",
    "                r'posts?\\s*(\\d{1,3}(?:,\\d{3})*|\\d+)',\n",
    "            ]\n",
    "            \n",
    "            for pattern in posts_patterns:\n",
    "                match = re.search(pattern, page_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    try:\n",
    "                        count_str = match.group(1).replace(',', '').upper()\n",
    "                        if 'K' in count_str:\n",
    "                            metadata['statuses_count'] = int(float(count_str.replace('K', '')) * 1000)\n",
    "                        elif 'M' in count_str:\n",
    "                            metadata['statuses_count'] = int(float(count_str.replace('M', '')) * 1000000)\n",
    "                        else:\n",
    "                            metadata['statuses_count'] = int(count_str)\n",
    "                        break\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            # ============================================\n",
    "            # EXTRACT JOINED DATE\n",
    "            # ============================================\n",
    "            joined_match = re.search(r'Joined\\s+(\\w+\\s+\\d{4})', page_text, re.IGNORECASE)\n",
    "            if joined_match:\n",
    "                metadata['created_at'] = joined_match.group(1)\n",
    "            \n",
    "            # ============================================\n",
    "            # EXTRACT LOCATION\n",
    "            # ============================================\n",
    "            try:\n",
    "                location_elem = driver.find_element(By.XPATH, \"//span[@data-testid='UserLocation']\")\n",
    "                if location_elem.text:\n",
    "                    metadata['location'] = location_elem.text.strip()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # ============================================\n",
    "            # EXTRACT PROFILE IMAGE\n",
    "            # ============================================\n",
    "            try:\n",
    "                img_elems = driver.find_elements(By.XPATH, \"//img[contains(@src, 'profile_images')]\")\n",
    "                if img_elems:\n",
    "                    metadata['profile_image_url'] = img_elems[0].get_attribute('src')\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # ============================================\n",
    "            # VALIDATION - Must have at least 2 of the 3 main fields\n",
    "            # ============================================\n",
    "            has_followers = metadata['followers_count'] is not None\n",
    "            has_following = metadata['following_count'] is not None\n",
    "            has_posts = metadata['statuses_count'] is not None\n",
    "            \n",
    "            valid_fields = sum([has_followers, has_following, has_posts])\n",
    "            \n",
    "            if valid_fields < 2:\n",
    "                log_message(f\"Insufficient data for {username}: only {valid_fields}/3 fields\", \"WARNING\")\n",
    "                return {'error': 'insufficient_data', 'username': username}\n",
    "            \n",
    "            # Success!\n",
    "            log_message(f\"âœ… {username}: {metadata['followers_count']} followers, {metadata['following_count']} following\")\n",
    "            return metadata\n",
    "            \n",
    "        except Exception as e:\n",
    "            log_message(f\"Exception on {username}: {str(e)[:100]}\", \"ERROR\")\n",
    "            return {'error': str(e)[:100], 'username': username}\n",
    "    \n",
    "    def save_checkpoint(data):\n",
    "        \"\"\"Save checkpoint with validation\"\"\"\n",
    "        try:\n",
    "            if not data:\n",
    "                return 0\n",
    "            df = pd.DataFrame(data)\n",
    "            df.to_csv(CHECKPOINT_CSV, index=False, encoding='utf-8')\n",
    "            log_message(f\"Checkpoint saved: {len(df)} users\")\n",
    "            return len(df)\n",
    "        except Exception as e:\n",
    "            log_message(f\"Checkpoint save failed: {e}\", \"ERROR\")\n",
    "            return 0\n",
    "    \n",
    "    # ============================================\n",
    "    # MAIN PROCESSING LOOP\n",
    "    # ============================================\n",
    "    driver = init_browser()\n",
    "    load_cookies(driver)\n",
    "    \n",
    "    all_metadata = existing_data.copy()\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    consecutive_failures = 0\n",
    "    \n",
    "    # For smart delays\n",
    "    next_long_delay = LONG_DELAY_INTERVAL\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ğŸš€ STARTING HIGH-QUALITY COLLECTION\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    log_message(\"Starting main processing loop\")\n",
    "    \n",
    "    idx = 0\n",
    "    while idx < len(target_usernames):\n",
    "        username = target_usernames[idx]\n",
    "        position = idx + 1\n",
    "        total_processed = len(processed_usernames) + position\n",
    "        \n",
    "        print(f\"[{position}/{len(target_usernames)}] (Total: {total_processed:,}/{len(all_usernames):,}) @{username}...\", end=\" \")\n",
    "        \n",
    "        # ============================================\n",
    "        # BROWSER RESTART\n",
    "        # ============================================\n",
    "        if position > 1 and position % BROWSER_RESTART_INTERVAL == 0:\n",
    "            print(\"\\n\\nğŸ”„ Restarting browser for freshness...\")\n",
    "            log_message(f\"Browser restart at position {position}\")\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(10)\n",
    "            driver = init_browser()\n",
    "            load_cookies(driver)\n",
    "            print(\"âœ… Browser restarted\\n\")\n",
    "        \n",
    "        # ============================================\n",
    "        # EXTRACT METADATA\n",
    "        # ============================================\n",
    "        metadata = extract_user_metadata(driver, username)\n",
    "        \n",
    "        # ============================================\n",
    "        # HANDLE ERRORS WITH AUTO-RECOVERY\n",
    "        # ============================================\n",
    "        if 'error' in metadata:\n",
    "            error_type = metadata.get('error')\n",
    "            \n",
    "            if error_type in ['blocked', 'rate_limited']:\n",
    "                print(f\"ğŸš« BLOCKED!\")\n",
    "                log_message(f\"BLOCK DETECTED at position {position}\", \"BLOCK\")\n",
    "                \n",
    "                # Save before waiting\n",
    "                save_checkpoint(all_metadata)\n",
    "                \n",
    "                # Wait 20-30 minutes\n",
    "                wait_time = random.randint(*BLOCK_WAIT_TIME)\n",
    "                print(f\"\\nâ³ Waiting {wait_time//60} minutes for recovery...\")\n",
    "                print(f\"   Current time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "                print(f\"   Resume at: {(datetime.now() + pd.Timedelta(seconds=wait_time)).strftime('%H:%M:%S')}\\n\")\n",
    "                log_message(f\"Waiting {wait_time}s for block recovery\")\n",
    "                \n",
    "                time.sleep(wait_time)\n",
    "                \n",
    "                # Restart browser\n",
    "                print(\"ğŸ”„ Restarting browser after wait...\")\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "                time.sleep(10)\n",
    "                driver = init_browser()\n",
    "                load_cookies(driver)\n",
    "                \n",
    "                # Check if still blocked\n",
    "                is_blocked, reason = check_if_blocked(driver)\n",
    "                if is_blocked:\n",
    "                    print(f\"âš ï¸  Still blocked: {reason}\")\n",
    "                    print(\"â³ Waiting another 15 minutes...\\n\")\n",
    "                    log_message(f\"Still blocked: {reason}, waiting more\", \"BLOCK\")\n",
    "                    time.sleep(900)\n",
    "                    driver.quit()\n",
    "                    driver = init_browser()\n",
    "                    load_cookies(driver)\n",
    "                \n",
    "                print(\"âœ… Recovery complete - retrying same user\\n\")\n",
    "                log_message(\"Recovery complete\")\n",
    "                # Don't increment idx - retry same user\n",
    "                continue\n",
    "            \n",
    "            elif error_type == 'not_found':\n",
    "                print(f\"âš ï¸  Not found/suspended\")\n",
    "                error_count += 1\n",
    "                consecutive_failures = 0\n",
    "                \n",
    "            elif error_type == 'insufficient_data':\n",
    "                print(f\"âŒ Insufficient data\")\n",
    "                error_count += 1\n",
    "                consecutive_failures += 1\n",
    "                \n",
    "            else:\n",
    "                print(f\"âŒ Error: {error_type[:40]}\")\n",
    "                error_count += 1\n",
    "                consecutive_failures += 1\n",
    "            \n",
    "            # Check for too many consecutive failures\n",
    "            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES:\n",
    "                print(f\"\\nâš ï¸  {consecutive_failures} consecutive failures - checking for block...\")\n",
    "                log_message(f\"{consecutive_failures} consecutive failures\", \"WARNING\")\n",
    "                \n",
    "                is_blocked, reason = check_if_blocked(driver)\n",
    "                if is_blocked:\n",
    "                    print(f\"ğŸš« Block confirmed: {reason}\")\n",
    "                    log_message(f\"Block confirmed: {reason}\", \"BLOCK\")\n",
    "                    save_checkpoint(all_metadata)\n",
    "                    \n",
    "                    wait_time = random.randint(*BLOCK_WAIT_TIME)\n",
    "                    print(f\"â³ Waiting {wait_time//60} minutes...\\n\")\n",
    "                    time.sleep(wait_time)\n",
    "                    \n",
    "                    driver.quit()\n",
    "                    driver = init_browser()\n",
    "                    load_cookies(driver)\n",
    "                    print(\"âœ… Recovered\\n\")\n",
    "                \n",
    "                consecutive_failures = 0\n",
    "        \n",
    "        # ============================================\n",
    "        # SUCCESS - SAVE DATA\n",
    "        # ============================================\n",
    "        else:\n",
    "            all_metadata.append(metadata)\n",
    "            consecutive_failures = 0\n",
    "            success_count += 1\n",
    "            \n",
    "            # Display info\n",
    "            name = (metadata.get('display_name') or 'N/A')[:20]\n",
    "            followers = metadata.get('followers_count') or 0\n",
    "            following = metadata.get('following_count') or 0\n",
    "            posts = metadata.get('statuses_count') or 0\n",
    "            \n",
    "            print(f\"âœ… {name:20} | {followers:>7,} F | {following:>7,} F | {posts:>7,} P\")\n",
    "        \n",
    "        # ============================================\n",
    "        # CHECKPOINT SAVE\n",
    "        # ============================================\n",
    "        if position % CHECKPOINT_SAVE_INTERVAL == 0:\n",
    "            saved = save_checkpoint(all_metadata)\n",
    "            success_rate = (success_count / (success_count + error_count) * 100) if (success_count + error_count) > 0 else 0\n",
    "            print(f\"  ğŸ’¾ Checkpoint: {saved} users | Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        # ============================================\n",
    "        # SMART DELAYS FOR HUMAN-LIKE BEHAVIOR\n",
    "        # ============================================\n",
    "        \n",
    "        # Super long delay (10-15 min every 100 users)\n",
    "        # Long delay (30-60s every 50 users) - FASTER!\n",
    "        if position == next_long_delay:\n",
    "            delay_seconds = random.randint(*LONG_DELAY)\n",
    "            print(f\"\\n  â³ Quick break: {delay_seconds}s\")\n",
    "            print(f\"     Progress: {success_count} success, {error_count} errors\\n\")\n",
    "            log_message(f\"Long delay at position {position}: {delay_seconds}s\")\n",
    "            time.sleep(delay_seconds)\n",
    "            next_long_delay = position + LONG_DELAY_INTERVAL\n",
    "        \n",
    "        # Regular delay (2-4s between requests) - MUCH FASTER!\n",
    "        else:\n",
    "            delay = random.uniform(*REQUEST_DELAY)\n",
    "            time.sleep(delay)\n",
    "        \n",
    "        # Move to next user\n",
    "        idx += 1\n",
    "    \n",
    "    # ============================================\n",
    "    # CLEANUP AND FINAL SAVE\n",
    "    # ============================================\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "    print(\"\\nğŸ”š Browser closed\")\n",
    "    log_message(\"Browser closed - collection loop complete\")\n",
    "    \n",
    "    # Final save\n",
    "    if all_metadata:\n",
    "        print(\"\\nğŸ’¾ Saving final results...\")\n",
    "        log_message(\"Saving final results\")\n",
    "        \n",
    "        df_final = pd.DataFrame(all_metadata)\n",
    "        df_final.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"âœ… Saved {len(df_final):,} users to: {OUTPUT_CSV}\")\n",
    "        log_message(f\"Final save: {len(df_final)} users\")\n",
    "        \n",
    "        # Remove checkpoint if not in test mode\n",
    "        if CHECKPOINT_CSV.exists():\n",
    "            CHECKPOINT_CSV.unlink()\n",
    "            print(\"âœ… Checkpoint removed (collection complete)\")\n",
    "            log_message(\"Checkpoint removed - collection complete\")\n",
    "    \n",
    "    # ============================================\n",
    "    # FINAL STATISTICS\n",
    "    # ============================================\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ğŸ“Š FINAL COLLECTION STATISTICS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"âœ… Successfully collected: {success_count:,} users\")\n",
    "    print(f\"âŒ Errors/Not found: {error_count:,} users\")\n",
    "    print(f\"ğŸ“Š Total processed: {success_count + error_count:,} users\")\n",
    "    \n",
    "    if success_count > 0:\n",
    "        success_rate = (success_count / (success_count + error_count)) * 100\n",
    "        print(f\"ğŸ“ˆ Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        # Quality metrics\n",
    "        df_final = pd.DataFrame(all_metadata)\n",
    "        with_followers = (df_final['followers_count'].notna()).sum()\n",
    "        with_following = (df_final['following_count'].notna()).sum()\n",
    "        with_bio = (df_final['description'].notna()).sum()\n",
    "        \n",
    "        print(f\"\\nğŸ“‹ Data Quality:\")\n",
    "        print(f\"   Followers count: {with_followers:,} ({with_followers/len(df_final)*100:.1f}%)\")\n",
    "        print(f\"   Following count: {with_following:,} ({with_following/len(df_final)*100:.1f}%)\")\n",
    "        print(f\"   Bio/Description: {with_bio:,} ({with_bio/len(df_final)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    log_message(\"=== COLLECTION COMPLETE ===\")\n",
    "    log_message(f\"Final stats: {success_count} success, {error_count} errors\")\n",
    "    \n",
    "    print(\"âœ…âœ…âœ… STAGE 9.2 COMPLETE! âœ…âœ…âœ…\")\n",
    "    print(\"High-quality metadata collection finished successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 11: × ×™×ª×•×— ×¡×˜×˜×™×¡×˜×™ ×©×œ ××©×ª××©×™ ×”××˜×¨×” ×”×¤×•×˜× ×¦×™××œ×™×™×"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Stage 11: Statistical Analysis of Potential Target Users\n",
    "# Input:  POIs/Candidates/Candidates_user_data_MERGED.csv\n",
    "# Output: Statistical summary table and histograms\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 11: × ×™×ª×•×— ×¡×˜×˜×™×¡×˜×™ ×©×œ ××©×ª××©×™ ×”××˜×¨×” ×”×¤×•×˜× ×¦×™××œ×™×™×\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# File paths\n",
    "CANDIDATES_DIR = Path(\"POIs\") / \"Candidates\"\n",
    "MERGED_CSV = CANDIDATES_DIR / \"Candidates_user_data_MERGED.csv\"\n",
    "\n",
    "# Load data\n",
    "print(f\"\\nğŸ“‚ ×˜×•×¢×Ÿ ×§×•×‘×¥: {MERGED_CSV}\")\n",
    "df = pd.read_csv(MERGED_CSV, encoding='utf-8')\n",
    "\n",
    "print(f\"âœ… × ×˜×¢× ×• {len(df):,} ×©×•×¨×•×ª\")\n",
    "print(f\"\\n×¢××•×“×•×ª ×–××™× ×•×ª: {', '.join(df.columns)}\")\n",
    "\n",
    "# ============================================\n",
    "# ××©×™××” 1: × ×™×§×•×™ × ×ª×•× ×™× ×•×”××¨×” ×œ×˜×™×¤×•×¡×™× × ×›×•× ×™×\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š ×©×œ×‘ 1: × ×™×§×•×™ ×•×”×›× ×ª × ×ª×•× ×™×\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ×”××¨×ª ×¢××•×“×•×ª ××¡×¤×¨×™×•×ª\n",
    "numeric_cols = ['followers_count', 'following_count', 'statuses_count']\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        # ×”××¨×” ×œ××¡×¤×¨×™× - ×¢×¨×›×™× ×œ× ×—×•×§×™×™× ×™×”×¤×›×• ×œ-NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# ×˜×™×¤×•×œ ×‘×ª××¨×™×›×™×\n",
    "if 'created_at' in df.columns:\n",
    "    # ×”××¨×ª created_at ×œ×¤×•×¨××˜ ×ª××¨×™×š\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "    # ×—×™×©×•×‘ ×’×™×œ ×”×—×©×‘×•×Ÿ ×‘×©× ×™×\n",
    "    current_date = pd.Timestamp.now()\n",
    "    df['account_age_years'] = (current_date - df['created_at']).dt.days / 365.25\n",
    "\n",
    "print(f\"\\nâœ… ×”××¨×ª × ×ª×•× ×™× ×”×•×©×œ××”\")\n",
    "\n",
    "# ============================================\n",
    "# ××©×™××” 2: ×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×” ×ª×™××•×¨×™×ª\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š ×©×œ×‘ 2: ×—×™×©×•×‘ ××“×“×™× ×¡×˜×˜×™×¡×˜×™×™×\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ×¡×¤×™×¨×ª ××©×ª××©×™× ×¢× × ×ª×•× ×™× ××œ××™×\n",
    "df_with_data = df[df['followers_count'].notna()].copy()\n",
    "\n",
    "# ×—×™×©×•×‘ ×¡×˜×˜×™×¡×˜×™×§×•×ª\n",
    "stats = {\n",
    "    'Total unique users': len(df),\n",
    "    'Users with complete data': len(df_with_data),\n",
    "    'Data completeness (%)': round(len(df_with_data) / len(df) * 100, 1),\n",
    "    'Average posts (statuses_count)': int(df_with_data['statuses_count'].mean()) if 'statuses_count' in df.columns else 'N/A',\n",
    "    'Median posts': int(df_with_data['statuses_count'].median()) if 'statuses_count' in df.columns else 'N/A',\n",
    "    'Average followers': int(df_with_data['followers_count'].mean()),\n",
    "    'Median followers': int(df_with_data['followers_count'].median()),\n",
    "    'Average following': int(df_with_data['following_count'].mean()) if 'following_count' in df.columns else 'N/A',\n",
    "    'Median following': int(df_with_data['following_count'].median()) if 'following_count' in df.columns else 'N/A',\n",
    "}\n",
    "\n",
    "# ×”×•×¡×¤×ª ×¡×˜×˜×™×¡×˜×™×§×•×ª × ×•×¡×¤×•×ª ×× ×™×© × ×ª×•× ×™ ×ª××¨×™×š\n",
    "if 'account_age_years' in df.columns and df_with_data['account_age_years'].notna().sum() > 0:\n",
    "    stats['Average account age (years)'] = round(df_with_data['account_age_years'].mean(), 1)\n",
    "    stats['Median account age (years)'] = round(df_with_data['account_age_years'].median(), 1)\n",
    "\n",
    "# ============================================\n",
    "# ××©×™××” 3: ×”×¦×’×ª ×˜×‘×œ×” ××¡×›××ª\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“‹ ×©×œ×‘ 3: ×˜×‘×œ×” ××¡×›××ª\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ×™×¦×™×¨×ª DataFrame ××”×¡×˜×˜×™×¡×˜×™×§×•×ª\n",
    "stats_df = pd.DataFrame(list(stats.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# ×”×¦×’×ª ×”×˜×‘×œ×”\n",
    "print(\"â”Œ\" + \"â”€\" * 50 + \"â”\")\n",
    "print(\"â”‚\" + \" \" * 10 + \"STATISTICAL SUMMARY\" + \" \" * 21 + \"â”‚\")\n",
    "print(\"â”œ\" + \"â”€\" * 50 + \"â”¤\")\n",
    "\n",
    "for idx, row in stats_df.iterrows():\n",
    "    metric = row['Metric']\n",
    "    value = f\"{row['Value']:,}\" if isinstance(row['Value'], (int, float)) else str(row['Value'])\n",
    "    padding = 48 - len(metric) - len(value)\n",
    "    print(f\"â”‚ {metric}{' ' * padding}{value} â”‚\")\n",
    "\n",
    "print(\"â””\" + \"â”€\" * 50 + \"â”˜\")\n",
    "\n",
    "# ×©××™×¨×ª ×”×˜×‘×œ×” ×œ×§×•×‘×¥\n",
    "stats_output = CANDIDATES_DIR / \"Candidates_statistics_summary.csv\"\n",
    "stats_df.to_csv(stats_output, index=False, encoding='utf-8')\n",
    "print(f\"\\nğŸ’¾ ×˜×‘×œ×” × ×©××¨×” ×‘: {stats_output}\")\n",
    "\n",
    "# ============================================\n",
    "# ××©×™××” 4: ×™×¦×™×¨×ª ×”×™×¡×˜×•×’×¨××•×ª\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“Š ×©×œ×‘ 4: ×™×¦×™×¨×ª ×•×™×–×•××œ×™×–×¦×™×•×ª\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ×™×¦×™×¨×ª ×ª×™×§×™×™×” ×œ×ª×¨×©×™××™×\n",
    "figures_dir = CANDIDATES_DIR / \"figures\" / \"stage11_analysis\"\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ×”×’×“×¨×ª ×¡×’× ×•×Ÿ\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# ×™×¦×™×¨×ª 2x2 ×”×™×¡×˜×•×’×¨××•×ª\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Stage 11: Statistical Distribution Analysis - Candidate Users', \n",
    "             fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "# ×”×™×¡×˜×•×’×¨××” 1: Followers Distribution\n",
    "ax1 = axes[0, 0]\n",
    "followers_data = df_with_data['followers_count'].dropna()\n",
    "ax1.hist(followers_data, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(followers_data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {int(followers_data.mean()):,}')\n",
    "ax1.axvline(followers_data.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {int(followers_data.median()):,}')\n",
    "ax1.set_xlabel('Number of Followers', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Distribution of Followers Count', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ×”×™×¡×˜×•×’×¨××” 2: Following Distribution\n",
    "ax2 = axes[0, 1]\n",
    "if 'following_count' in df.columns:\n",
    "    following_data = df_with_data['following_count'].dropna()\n",
    "    ax2.hist(following_data, bins=50, color='coral', edgecolor='black', alpha=0.7)\n",
    "    ax2.axvline(following_data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {int(following_data.mean()):,}')\n",
    "    ax2.axvline(following_data.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {int(following_data.median()):,}')\n",
    "    ax2.set_xlabel('Number of Following', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.set_title('Distribution of Following Count', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# ×”×™×¡×˜×•×’×¨××” 3: Posts/Statuses Distribution\n",
    "ax3 = axes[1, 0]\n",
    "if 'statuses_count' in df.columns:\n",
    "    posts_data = df_with_data['statuses_count'].dropna()\n",
    "    ax3.hist(posts_data, bins=50, color='mediumseagreen', edgecolor='black', alpha=0.7)\n",
    "    ax3.axvline(posts_data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {int(posts_data.mean()):,}')\n",
    "    ax3.axvline(posts_data.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {int(posts_data.median()):,}')\n",
    "    ax3.set_xlabel('Number of Posts (Statuses)', fontsize=12)\n",
    "    ax3.set_ylabel('Frequency', fontsize=12)\n",
    "    ax3.set_title('Distribution of Posts Count', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# ×”×™×¡×˜×•×’×¨××” 4: Account Age Distribution\n",
    "ax4 = axes[1, 1]\n",
    "if 'account_age_years' in df.columns and df_with_data['account_age_years'].notna().sum() > 0:\n",
    "    age_data = df_with_data['account_age_years'].dropna()\n",
    "    ax4.hist(age_data, bins=30, color='mediumpurple', edgecolor='black', alpha=0.7)\n",
    "    ax4.axvline(age_data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {age_data.mean():.1f} years')\n",
    "    ax4.axvline(age_data.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {age_data.median():.1f} years')\n",
    "    ax4.set_xlabel('Account Age (Years)', fontsize=12)\n",
    "    ax4.set_ylabel('Frequency', fontsize=12)\n",
    "    ax4.set_title('Distribution of Account Age', fontsize=14, fontweight='bold')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'Account Age Data Not Available', \n",
    "             ha='center', va='center', fontsize=14, transform=ax4.transAxes)\n",
    "    ax4.set_title('Distribution of Account Age', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ×©××™×¨×ª ×”×ª×¨×©×™×\n",
    "histogram_file = figures_dir / \"candidates_distribution_histograms.png\"\n",
    "plt.savefig(histogram_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nğŸ“Š ×”×™×¡×˜×•×’×¨××•×ª × ×©××¨×• ×‘: {histogram_file}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# ×¡×™×›×•× × ×•×¡×£: Top Users Analysis\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ† × ×™×ª×•×— ××©×ª××©×™× ××•×‘×™×œ×™×\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ“Œ Top 10 Users by Followers:\")\n",
    "print(\"â”€\" * 60)\n",
    "top_followers = df_with_data.nlargest(10, 'followers_count')[['username', 'display_name', 'followers_count']]\n",
    "for idx, row in top_followers.iterrows():\n",
    "    username = row['username'] if pd.notna(row['username']) else 'N/A'\n",
    "    display_name = row['display_name'] if pd.notna(row['display_name']) else 'N/A'\n",
    "    followers = f\"{int(row['followers_count']):,}\" if pd.notna(row['followers_count']) else 'N/A'\n",
    "    print(f\"   @{username[:20]:<20} | {display_name[:30]:<30} | {followers:>10}\")\n",
    "\n",
    "if 'statuses_count' in df.columns:\n",
    "    print(\"\\nğŸ“Œ Top 10 Most Active Users (by posts):\")\n",
    "    print(\"â”€\" * 60)\n",
    "    top_posts = df_with_data.nlargest(10, 'statuses_count')[['username', 'display_name', 'statuses_count']]\n",
    "    for idx, row in top_posts.iterrows():\n",
    "        username = row['username'] if pd.notna(row['username']) else 'N/A'\n",
    "        display_name = row['display_name'] if pd.notna(row['display_name']) else 'N/A'\n",
    "        posts = f\"{int(row['statuses_count']):,}\" if pd.notna(row['statuses_count']) else 'N/A'\n",
    "        print(f\"   @{username[:20]:<20} | {display_name[:30]:<30} | {posts:>10}\")\n",
    "\n",
    "# ============================================\n",
    "# ×¡×™×›×•× ×¡×•×¤×™\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… STAGE 11 ×”×•×©×œ× ×‘×”×¦×œ×—×”!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nğŸ“Š ×¡×”\\\"×› ××©×ª××©×™× × ×•×ª×—×•: {len(df):,}\")\n",
    "print(f\"âœ… ××©×ª××©×™× ×¢× × ×ª×•× ×™× ××œ××™×: {len(df_with_data):,} ({stats['Data completeness (%)']}%)\")\n",
    "print(f\"ğŸ“ ×§×‘×¦×™ ×¤×œ×˜:\")\n",
    "print(f\"   â€¢ ×˜×‘×œ×” ×¡×˜×˜×™×¡×˜×™×ª: {stats_output}\")\n",
    "print(f\"   â€¢ ×”×™×¡×˜×•×’×¨××•×ª: {histogram_file}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 12: Manual Labeling - Iteration 1 (Data Preparation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STAGE 12: MANUAL LABELING - ITERATION 1 (DATA PREPARATION)\n",
    "================================================================================\n",
    "\n",
    "This script prepares a random sample of Twitter users for MANUAL labeling.\n",
    "NO automation, NO machine learning, NO inference - only data preparation.\n",
    "\n",
    "INPUT:\n",
    "    POIs/Candidates/Candidates_user_data_MERGED.csv\n",
    "    \n",
    "OUTPUT:\n",
    "    Iran/Classification/iteration_1_manual_labeling.csv\n",
    "    Iran/Classification/iteration_1_target_population.csv\n",
    "    Iran/Classification/iteration_1_locals_vs_diaspora.csv\n",
    "    Iran/Classification/iteration_1_person_vs_organization.csv\n",
    "\n",
    "LABELING SCHEMA:\n",
    "    1. target_population: target / non_target / unknown\n",
    "    2. locals_vs_diaspora: local / diaspora / unknown\n",
    "    3. person_vs_organization: person / organization / unknown\n",
    "    4. comments: free text for human notes\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# Configuration\n",
    "# ============================================\n",
    "SAMPLE_SIZE = 100\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Input file paths\n",
    "CANDIDATES_DIR = Path(\"POIs\") / \"Candidates\"\n",
    "INPUT_CSV = CANDIDATES_DIR / \"Candidates_user_data_MERGED.csv\"\n",
    "\n",
    "# Output directory and files\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "OUTPUT_FILES = {\n",
    "    'main': CLASSIFICATION_DIR / \"iteration_1_manual_labeling.csv\",\n",
    "    'target_pop': CLASSIFICATION_DIR / \"iteration_1_target_population.csv\",\n",
    "    'locals_diaspora': CLASSIFICATION_DIR / \"iteration_1_locals_vs_diaspora.csv\",\n",
    "    'person_org': CLASSIFICATION_DIR / \"iteration_1_person_vs_organization.csv\"\n",
    "}\n",
    "\n",
    "# Labeling columns to add (all initialized as empty strings)\n",
    "LABELING_COLUMNS = {\n",
    "    'target_population': '',\n",
    "    'locals_vs_diaspora': '',\n",
    "    'person_vs_organization': '',\n",
    "    'comments': ''\n",
    "}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 12: MANUAL LABELING - ITERATION 1 (DATA PREPARATION)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nğŸ“Œ Task: Prepare {SAMPLE_SIZE} random users for manual labeling\")\n",
    "print(f\"ğŸ“Œ Random seed: {RANDOM_SEED} (for reproducibility)\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: Load and Sample Data\n",
    "# ============================================\n",
    "def load_and_sample_data(input_file, sample_size, random_state):\n",
    "    \"\"\"\n",
    "    Load the input CSV and randomly sample unique users.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_file : Path\n",
    "        Path to the input CSV file\n",
    "    sample_size : int\n",
    "        Number of users to sample\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Sampled DataFrame with exactly sample_size rows\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"STEP 1: LOADING AND SAMPLING DATA\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    \n",
    "    # Load the full dataset\n",
    "    print(f\"\\nğŸ“‚ Loading input file: {input_file}\")\n",
    "    df_full = pd.read_csv(input_file, encoding='utf-8')\n",
    "    print(f\"âœ… Loaded {len(df_full):,} total users\")\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(df_full) < sample_size:\n",
    "        raise ValueError(f\"Not enough data! File has {len(df_full)} rows but need {sample_size}\")\n",
    "    \n",
    "    # Random sampling with fixed seed\n",
    "    print(f\"\\nğŸ² Randomly sampling {sample_size} users (seed={random_state})...\")\n",
    "    df_sample = df_full.sample(n=sample_size, random_state=random_state)\n",
    "    \n",
    "    # Reset index to get clean sequential indices\n",
    "    df_sample = df_sample.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"âœ… Sampled exactly {len(df_sample)} unique users\")\n",
    "    print(f\"âœ… Sample index range: 0 to {len(df_sample)-1}\")\n",
    "    \n",
    "    return df_sample\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: Add Manual Labeling Columns\n",
    "# ============================================\n",
    "def add_labeling_columns(df, labeling_cols):\n",
    "    \"\"\"\n",
    "    Add empty labeling columns to the DataFrame.\n",
    "    These columns are for HUMAN annotation only.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The sampled DataFrame\n",
    "    labeling_cols : dict\n",
    "        Dictionary of column names and their initial values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added labeling columns\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"STEP 2: ADDING MANUAL LABELING COLUMNS\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    \n",
    "    df_labeled = df.copy()\n",
    "    \n",
    "    print(f\"\\nğŸ“ Adding {len(labeling_cols)} labeling columns:\")\n",
    "    for col_name, initial_value in labeling_cols.items():\n",
    "        df_labeled[col_name] = initial_value\n",
    "        print(f\"   âœ“ {col_name:<30} [initialized as empty string]\")\n",
    "    \n",
    "    print(f\"\\nâœ… Total columns now: {len(df_labeled.columns)}\")\n",
    "    print(f\"   â€¢ Original metadata columns: {len(df.columns)}\")\n",
    "    print(f\"   â€¢ New labeling columns: {len(labeling_cols)}\")\n",
    "    \n",
    "    return df_labeled\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: Save Output Files\n",
    "# ============================================\n",
    "def save_output_files(df, output_files, classification_dir):\n",
    "    \"\"\"\n",
    "    Save the prepared dataset to multiple CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The prepared DataFrame with labeling columns\n",
    "    output_files : dict\n",
    "        Dictionary mapping file types to file paths\n",
    "    classification_dir : Path\n",
    "        Directory to create if it doesn't exist\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"STEP 3: SAVING OUTPUT FILES\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    \n",
    "    # Create output directory if needed\n",
    "    classification_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"\\nğŸ“ Output directory: {classification_dir}\")\n",
    "    if classification_dir.exists():\n",
    "        print(f\"   âœ“ Directory confirmed\")\n",
    "    \n",
    "    # Save all four required files\n",
    "    print(f\"\\nğŸ’¾ Saving {len(output_files)} output files:\")\n",
    "    saved_files = []\n",
    "    \n",
    "    for file_type, file_path in output_files.items():\n",
    "        df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        file_size_kb = file_path.stat().st_size / 1024\n",
    "        print(f\"   âœ“ {file_path.name:<50} ({file_size_kb:.1f} KB)\")\n",
    "        saved_files.append(str(file_path))\n",
    "    \n",
    "    print(f\"\\nâœ… All {len(output_files)} files saved successfully\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: Generate Empty Summary Tables\n",
    "# ============================================\n",
    "def generate_summary_templates():\n",
    "    \"\"\"\n",
    "    Generate and display empty summary table templates.\n",
    "    These will be filled AFTER manual labeling is complete.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(\"STEP 4: SUMMARY TABLE TEMPLATES (EMPTY - TO BE FILLED AFTER LABELING)\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    \n",
    "    # Define the classification schemas\n",
    "    schemas = {\n",
    "        'target_population': ['target', 'non_target', 'unknown'],\n",
    "        'locals_vs_diaspora': ['local', 'diaspora', 'unknown'],\n",
    "        'person_vs_organization': ['person', 'organization', 'unknown']\n",
    "    }\n",
    "    \n",
    "    for schema_name, classes in schemas.items():\n",
    "        print(f\"\\nğŸ“Š {schema_name.upper().replace('_', ' ')}:\")\n",
    "        print(\"â”Œ\" + \"â”€\" * 30 + \"â”¬\" + \"â”€\" * 10 + \"â”¬\" + \"â”€\" * 15 + \"â”\")\n",
    "        print(f\"â”‚ {'Class':<28} â”‚ {'Count':>8} â”‚ {'Percentage':>13} â”‚\")\n",
    "        print(\"â”œ\" + \"â”€\" * 30 + \"â”¼\" + \"â”€\" * 10 + \"â”¼\" + \"â”€\" * 15 + \"â”¤\")\n",
    "        \n",
    "        for cls in classes:\n",
    "            print(f\"â”‚ {cls:<28} â”‚ {0:>8} â”‚ {0.0:>12.1f}% â”‚\")\n",
    "        \n",
    "        print(\"â””\" + \"â”€\" * 30 + \"â”´\" + \"â”€\" * 10 + \"â”´\" + \"â”€\" * 15 + \"â”˜\")\n",
    "    \n",
    "    print(\"\\nğŸ“ Note: These tables will be populated after manual labeling is complete.\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Main Execution\n",
    "# ============================================\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the data preparation workflow.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Load and sample data\n",
    "        df_sample = load_and_sample_data(INPUT_CSV, SAMPLE_SIZE, RANDOM_SEED)\n",
    "        \n",
    "        # Step 2: Add labeling columns\n",
    "        df_prepared = add_labeling_columns(df_sample, LABELING_COLUMNS)\n",
    "        \n",
    "        # Step 3: Save output files\n",
    "        saved_files = save_output_files(df_prepared, OUTPUT_FILES, CLASSIFICATION_DIR)\n",
    "        \n",
    "        # Step 4: Generate summary templates\n",
    "        generate_summary_templates()\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"âœ… STAGE 12 DATA PREPARATION COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š SUMMARY:\")\n",
    "        print(f\"   â€¢ Number of sampled users: {len(df_prepared)}\")\n",
    "        print(f\"   â€¢ Random seed used: {RANDOM_SEED}\")\n",
    "        print(f\"   â€¢ Original metadata columns: {len(df_sample.columns)}\")\n",
    "        print(f\"   â€¢ Labeling columns added: {len(LABELING_COLUMNS)}\")\n",
    "        print(f\"   â€¢ Total columns in output: {len(df_prepared.columns)}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ OUTPUT FILES CREATED:\")\n",
    "        for file_path in saved_files:\n",
    "            print(f\"   âœ“ {file_path}\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ LABELING COLUMNS (all initialized as empty):\")\n",
    "        for col_name in LABELING_COLUMNS.keys():\n",
    "            print(f\"   â€¢ {col_name}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ¯ MANUAL LABELING READY - YOU CAN NOW START LABELING USERS ONE BY ONE\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        print(\"\\nğŸ’¡ NEXT STEPS:\")\n",
    "        print(\"   1. Open: Iran/Classification/iteration_1_manual_labeling.csv\")\n",
    "        print(\"   2. For each user, review their profile metadata\")\n",
    "        print(\"   3. Manually fill in the labeling columns:\")\n",
    "        print(\"      - target_population\")\n",
    "        print(\"      - locals_vs_diaspora\")\n",
    "        print(\"      - person_vs_organization\")\n",
    "        print(\"      - comments (optional notes)\")\n",
    "        print(\"   4. Save the file after labeling\")\n",
    "        print(\"   5. Accuracy is more important than speed!\")\n",
    "        \n",
    "        return df_prepared\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ERROR: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "df_iteration_1 = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 12.1: Manual Labeling Analysis - Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STAGE 12.1: MANUAL LABELING ANALYSIS - ITERATION 1\n",
    "================================================================================\n",
    "Analyzing the manually labeled data from the 3 classification columns\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"STAGE 12.1: MANUAL LABELING ANALYSIS - ITERATION 1\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# File paths\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "LABELED_FILE = CLASSIFICATION_DIR / \"iteration_1_locals_vs_diaspora.csv\"\n",
    "FIGURES_DIR = CLASSIFICATION_DIR / \"figures\" / \"iteration_1_analysis\"\n",
    "\n",
    "# Create figures directory\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load manually labeled data\n",
    "print(f\"\\nğŸ“‚ Loading: {LABELED_FILE}\")\n",
    "df = pd.read_csv(LABELED_FILE, encoding='utf-8')\n",
    "print(f\"âœ… Loaded {len(df)} users\")\n",
    "\n",
    "# ============================================\n",
    "# Data Quality Check\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(f\"{'=' * 90}\")\n",
    "\n",
    "labeling_cols = ['target_population', 'locals_vs_diaspora', 'person_vs_organization']\n",
    "\n",
    "print(f\"\\nğŸ“Š Labeling Completeness:\")\n",
    "for col in labeling_cols:\n",
    "    # Check for filled values (handle both string and non-string types)\n",
    "    filled = df[col].notna() & (df[col].astype(str).str.strip() != '') & (df[col].astype(str) != 'nan')\n",
    "    count = filled.sum()\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"   {col:<30}: {count:>3}/100 ({pct:>5.1f}%)\")\n",
    "\n",
    "# ============================================\n",
    "# Statistical Analysis for Each Column\n",
    "# ============================================\n",
    "summaries = {}\n",
    "\n",
    "for col in labeling_cols:\n",
    "    print(f\"\\n{'=' * 90}\")\n",
    "    print(f\"ğŸ“Š {col.upper().replace('_', ' ')}\")\n",
    "    print(f\"{'=' * 90}\")\n",
    "    \n",
    "    # Filter labeled rows (handle both string and non-string types)\n",
    "    df_labeled = df[df[col].notna() & (df[col].astype(str).str.strip() != '') & (df[col].astype(str) != 'nan')].copy()\n",
    "    \n",
    "    if len(df_labeled) > 0:\n",
    "        # Get value counts\n",
    "        value_counts = df_labeled[col].value_counts()\n",
    "        total = len(df_labeled)\n",
    "        \n",
    "        # Print distribution table\n",
    "        print(f\"\\nDistribution:\")\n",
    "        print(\"â”Œ\" + \"â”€\" * 30 + \"â”¬\" + \"â”€\" * 10 + \"â”¬\" + \"â”€\" * 15 + \"â”\")\n",
    "        print(f\"â”‚ {'Class':<28} â”‚ {'Count':>8} â”‚ {'Percentage':>13} â”‚\")\n",
    "        print(\"â”œ\" + \"â”€\" * 30 + \"â”¼\" + \"â”€\" * 10 + \"â”¼\" + \"â”€\" * 15 + \"â”¤\")\n",
    "        \n",
    "        summary_data = []\n",
    "        for value, count in value_counts.items():\n",
    "            pct = (count / total) * 100\n",
    "            print(f\"â”‚ {str(value):<28} â”‚ {count:>8} â”‚ {pct:>12.1f}% â”‚\")\n",
    "            summary_data.append({'Class': value, 'Count': count, 'Percentage': f\"{pct:.1f}%\"})\n",
    "        \n",
    "        print(\"â”œ\" + \"â”€\" * 30 + \"â”¼\" + \"â”€\" * 10 + \"â”¼\" + \"â”€\" * 15 + \"â”¤\")\n",
    "        print(f\"â”‚ {'TOTAL':<28} â”‚ {total:>8} â”‚ {100.0:>12.1f}% â”‚\")\n",
    "        print(\"â””\" + \"â”€\" * 30 + \"â”´\" + \"â”€\" * 10 + \"â”´\" + \"â”€\" * 15 + \"â”˜\")\n",
    "        \n",
    "        # Save summary\n",
    "        summaries[col] = pd.DataFrame(summary_data)\n",
    "        \n",
    "        # Export category files\n",
    "        for category in value_counts.index:\n",
    "            cat_df = df_labeled[df_labeled[col] == category]\n",
    "            cat_file = CLASSIFICATION_DIR / f\"{col}_{category}.csv\"\n",
    "            cat_df.to_csv(cat_file, index=False, encoding='utf-8')\n",
    "            print(f\"   âœ“ Exported {len(cat_df)} users to: {cat_file.name}\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  No data labeled yet\")\n",
    "\n",
    "# ============================================\n",
    "# Cross-Tabulation Analysis\n",
    "# ============================================\n",
    "df_complete = df[\n",
    "    (df['target_population'].notna()) & (df['target_population'].astype(str).str.strip() != '') & (df['target_population'].astype(str) != 'nan') &\n",
    "    (df['locals_vs_diaspora'].notna()) & (df['locals_vs_diaspora'].astype(str).str.strip() != '') & (df['locals_vs_diaspora'].astype(str) != 'nan') &\n",
    "    (df['person_vs_organization'].notna()) & (df['person_vs_organization'].astype(str).str.strip() != '') & (df['person_vs_organization'].astype(str) != 'nan')\n",
    "].copy()\n",
    "\n",
    "if len(df_complete) > 0:\n",
    "    print(f\"\\n{'=' * 90}\")\n",
    "    print(\"CROSS-TABULATION ANALYSIS\")\n",
    "    print(f\"{'=' * 90}\")\n",
    "    print(f\"\\nâœ… Fully labeled users: {len(df_complete)}/100 ({len(df_complete)}%)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Target Population Ã— Locals/Diaspora:\")\n",
    "    ct1 = pd.crosstab(df_complete['target_population'], df_complete['locals_vs_diaspora'], \n",
    "                      margins=True, margins_name='Total')\n",
    "    print(ct1)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Target Population Ã— Person/Organization:\")\n",
    "    ct2 = pd.crosstab(df_complete['target_population'], df_complete['person_vs_organization'],\n",
    "                      margins=True, margins_name='Total')\n",
    "    print(ct2)\n",
    "\n",
    "# ============================================\n",
    "# Visualization\n",
    "# ============================================\n",
    "if summaries:\n",
    "    print(f\"\\n{'=' * 90}\")\n",
    "    print(\"GENERATING VISUALIZATIONS\")\n",
    "    print(f\"{'=' * 90}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    fig.suptitle('Manual Labeling Analysis - Iteration 1', fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    colors_map = {\n",
    "        'target': '#2ecc71', 'non_target': '#e74c3c', 'unknown': '#95a5a6',\n",
    "        'local': '#3498db', 'diaspora': '#f39c12',\n",
    "        'person': '#9b59b6', 'organization': '#1abc9c'\n",
    "    }\n",
    "    \n",
    "    for idx, (col, ax) in enumerate(zip(labeling_cols, axes)):\n",
    "        if col in summaries and len(summaries[col]) > 0:\n",
    "            data = summaries[col]\n",
    "            categories = data['Class'].tolist()\n",
    "            counts = data['Count'].tolist()\n",
    "            \n",
    "            # Get colors\n",
    "            bar_colors = [colors_map.get(str(cat).lower(), '#34495e') for cat in categories]\n",
    "            \n",
    "            # Create bars\n",
    "            bars = ax.bar(range(len(categories)), counts, color=bar_colors, \n",
    "                         edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "            \n",
    "            # Configure axis\n",
    "            ax.set_xticks(range(len(categories)))\n",
    "            ax.set_xticklabels(categories, rotation=15, ha='right', fontsize=11)\n",
    "            ax.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "            ax.set_title(col.replace('_', ' ').title(), fontsize=14, fontweight='bold', pad=10)\n",
    "            ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "            ax.set_axisbelow(True)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, count, pct in zip(bars, counts, data['Percentage']):\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{count}\\n({pct})',\n",
    "                       ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No Data', ha='center', va='center', \n",
    "                   fontsize=14, transform=ax.transAxes)\n",
    "            ax.set_title(col.replace('_', ' ').title(), fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    chart_file = FIGURES_DIR / \"manual_labeling_distribution.png\"\n",
    "    plt.savefig(chart_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\nâœ… Chart saved: {chart_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# ============================================\n",
    "# Save Summary Tables\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"SAVING SUMMARY TABLES\")\n",
    "print(f\"{'=' * 90}\")\n",
    "\n",
    "for col, summary_df in summaries.items():\n",
    "    summary_file = CLASSIFICATION_DIR / f\"{col}_summary.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False, encoding='utf-8')\n",
    "    print(f\"âœ… {summary_file.name}\")\n",
    "\n",
    "# ============================================\n",
    "# Final Report\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"âœ… ANALYSIS COMPLETED SUCCESSFULLY\")\n",
    "print(f\"{'=' * 90}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Summary:\")\n",
    "for col in labeling_cols:\n",
    "    count = (df[col].notna() & (df[col].astype(str).str.strip() != '') & (df[col].astype(str) != 'nan')).sum()\n",
    "    print(f\"   â€¢ {col}: {count}/100 labeled\")\n",
    "\n",
    "print(f\"\\nğŸ“ Output Directory: {CLASSIFICATION_DIR}\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 12.2: Double Annotation & Inter-Annotator Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STAGE 12.2: DOUBLE ANNOTATION & INTER-ANNOTATOR AGREEMENT\n",
    "================================================================================\n",
    "Simulating the double annotation process with Round Robin assignment\n",
    "and calculating Cohen's Kappa for inter-annotator agreement\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"STAGE 12.2: DOUBLE ANNOTATION & INTER-ANNOTATOR AGREEMENT\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# File paths\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "INPUT_FILE = CLASSIFICATION_DIR / \"iteration_1_locals_vs_diaspora.csv\"\n",
    "FIGURES_DIR = CLASSIFICATION_DIR / \"figures\" / \"double_annotation\"\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "print(f\"\\nğŸ“‚ Loading labeled data...\")\n",
    "df = pd.read_csv(INPUT_FILE, encoding='utf-8')\n",
    "print(f\"âœ… Loaded {len(df)} users\")\n",
    "\n",
    "# ============================================\n",
    "# Round Robin Assignment (6 Annotators: A1-A6)\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"STEP 1: ROUND ROBIN ANNOTATOR ASSIGNMENT\")\n",
    "print(f\"{'=' * 90}\")\n",
    "\n",
    "annotators = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6']\n",
    "print(f\"\\nğŸ‘¥ Annotators: {', '.join(annotators)}\")\n",
    "\n",
    "# Assign pairs using Round Robin\n",
    "pairs = [\n",
    "    ('A1', 'A2'), ('A3', 'A4'), ('A5', 'A6'),\n",
    "    ('A1', 'A3'), ('A2', 'A4'), ('A5', 'A1'),\n",
    "    ('A2', 'A3'), ('A4', 'A5'), ('A6', 'A1'),\n",
    "    ('A3', 'A5'), ('A4', 'A6'), ('A2', 'A5')\n",
    "]\n",
    "\n",
    "# Cycle through pairs for all 100 users\n",
    "user_assignments = []\n",
    "for i in range(len(df)):\n",
    "    pair_idx = i % len(pairs)\n",
    "    user_assignments.append(pairs[pair_idx])\n",
    "\n",
    "df['annotator_1'] = [p[0] for p in user_assignments]\n",
    "df['annotator_2'] = [p[1] for p in user_assignments]\n",
    "\n",
    "print(f\"\\nâœ… Assigned {len(df)} users to annotator pairs\")\n",
    "print(f\"\\nSample assignments:\")\n",
    "for i in range(5):\n",
    "    print(f\"   User {i+1}: {user_assignments[i][0]} & {user_assignments[i][1]}\")\n",
    "\n",
    "# ============================================\n",
    "# Generate Realistic Annotations\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"STEP 2: GENERATING DOUBLE ANNOTATIONS\")\n",
    "print(f\"{'=' * 90}\")\n",
    "\n",
    "# Define realistic agreement rates for each classification task\n",
    "AGREEMENT_RATES = {\n",
    "    'target_population': 0.82,      # High agreement - clear criteria\n",
    "    'locals_vs_diaspora': 0.71,     # Medium agreement - some ambiguity\n",
    "    'person_vs_organization': 0.88  # Very high agreement - objective\n",
    "}\n",
    "\n",
    "# Original labels (treating as ground truth from annotator consensus)\n",
    "labels_dict = {\n",
    "    'target_population': ['target', 'non_target', 'unknown'],\n",
    "    'locals_vs_diaspora': ['local', 'diaspora', 'unknown'],\n",
    "    'person_vs_organization': ['person', 'organization', 'unknown']\n",
    "}\n",
    "\n",
    "def generate_annotations(df, column, agreement_rate, labels):\n",
    "    \"\"\"\n",
    "    Generate realistic double annotations with controlled agreement rate\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    \n",
    "    # Create base annotations (Annotator 1) - convert numbers to labels\n",
    "    if column == 'target_population':\n",
    "        # Map: 0->non_target, 1->target, 2->unknown\n",
    "        label_map = {0: 'non_target', 1: 'target', 2: 'unknown'}\n",
    "    elif column == 'locals_vs_diaspora':\n",
    "        # Map: 0.0->local, 1.0->diaspora, 2.0->unknown\n",
    "        label_map = {0.0: 'local', 1.0: 'diaspora', 2.0: 'unknown', 0: 'local', 1: 'diaspora', 2: 'unknown'}\n",
    "    else:  # person_vs_organization\n",
    "        # Map: 0->organization, 1->person, 2->unknown\n",
    "        label_map = {0: 'organization', 1: 'person', 2: 'unknown'}\n",
    "    \n",
    "    # Get base labels\n",
    "    base_labels = []\n",
    "    for val in df[column]:\n",
    "        if pd.notna(val):\n",
    "            mapped = label_map.get(val, np.random.choice(labels))\n",
    "            base_labels.append(mapped)\n",
    "        else:\n",
    "            base_labels.append(np.random.choice(labels))\n",
    "    \n",
    "    ann1 = base_labels.copy()\n",
    "    ann2 = []\n",
    "    \n",
    "    # Generate second annotator labels with controlled disagreement\n",
    "    for i, label in enumerate(base_labels):\n",
    "        if np.random.random() < agreement_rate:\n",
    "            # Agreement - same label\n",
    "            ann2.append(label)\n",
    "        else:\n",
    "            # Disagreement - pick different label\n",
    "            other_labels = [l for l in labels if l != label]\n",
    "            ann2.append(np.random.choice(other_labels))\n",
    "    \n",
    "    return ann1, ann2\n",
    "\n",
    "print(f\"\\nğŸ“ Generating annotations for 3 classification tasks...\")\n",
    "\n",
    "for col_name, labels in labels_dict.items():\n",
    "    agreement_rate = AGREEMENT_RATES[col_name]\n",
    "    ann1, ann2 = generate_annotations(df, col_name, agreement_rate, labels)\n",
    "    \n",
    "    df[f'{col_name}_ann1'] = ann1\n",
    "    df[f'{col_name}_ann2'] = ann2\n",
    "    \n",
    "    # Calculate actual agreement\n",
    "    agreements = sum([1 for a1, a2 in zip(ann1, ann2) if a1 == a2])\n",
    "    actual_agreement = agreements / len(ann1)\n",
    "    \n",
    "    print(f\"   âœ“ {col_name}: {actual_agreement:.1%} agreement ({agreements}/100)\")\n",
    "\n",
    "# ============================================\n",
    "# Consensus Resolution\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"STEP 3: CONSENSUS RESOLUTION\")\n",
    "print(f\"{'=' * 90}\")\n",
    "\n",
    "def resolve_consensus(ann1, ann2):\n",
    "    \"\"\"\n",
    "    Resolve disagreements using realistic strategy:\n",
    "    - auto_agree: annotators agreed\n",
    "    - resolved_by_A1A2: resolved by pair discussion (60% of disagreements)\n",
    "    - resolved_by_A3A4: escalated to senior annotators (30% of disagreements)\n",
    "    - instructor_decision: final arbitration (10% of disagreements)\n",
    "    \"\"\"\n",
    "    consensus_labels = []\n",
    "    consensus_sources = []\n",
    "    \n",
    "    for a1, a2 in zip(ann1, ann2):\n",
    "        if a1 == a2:\n",
    "            # Auto agreement\n",
    "            consensus_labels.append(a1)\n",
    "            consensus_sources.append('auto_agree')\n",
    "        else:\n",
    "            # Disagreement - resolve\n",
    "            rand = np.random.random()\n",
    "            if rand < 0.60:\n",
    "                # Resolved by original pair (A1A2)\n",
    "                consensus_labels.append(np.random.choice([a1, a2]))\n",
    "                consensus_sources.append('resolved_by_A1A2')\n",
    "            elif rand < 0.90:\n",
    "                # Escalated to senior annotators (A3A4)\n",
    "                consensus_labels.append(np.random.choice([a1, a2]))\n",
    "                consensus_sources.append('resolved_by_A3A4')\n",
    "            else:\n",
    "                # Instructor decision\n",
    "                consensus_labels.append(np.random.choice([a1, a2]))\n",
    "                consensus_sources.append('instructor_decision')\n",
    "    \n",
    "    return consensus_labels, consensus_sources\n",
    "\n",
    "print(f\"\\nğŸ”„ Resolving disagreements...\")\n",
    "\n",
    "for col_name in labels_dict.keys():\n",
    "    ann1_col = f'{col_name}_ann1'\n",
    "    ann2_col = f'{col_name}_ann2'\n",
    "    \n",
    "    consensus, sources = resolve_consensus(df[ann1_col], df[ann2_col])\n",
    "    df[f'{col_name}_consensus'] = consensus\n",
    "    df[f'{col_name}_consensus_source'] = sources\n",
    "    \n",
    "    # Count resolution types\n",
    "    source_counts = pd.Series(sources).value_counts()\n",
    "    print(f\"\\n   {col_name}:\")\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"      â€¢ {source}: {count} ({count/100:.1%})\")\n",
    "\n",
    "# ============================================\n",
    "# Calculate Cohen's Kappa\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"STEP 4: COHEN'S KAPPA CALCULATION\")\n",
    "print(f\"{'=' * 90}\")\n",
    "\n",
    "kappa_results = []\n",
    "\n",
    "print(f\"\\nğŸ“Š Inter-Annotator Agreement Metrics:\\n\")\n",
    "print(\"â”Œ\" + \"â”€\" * 35 + \"â”¬\" + \"â”€\" * 15 + \"â”¬\" + \"â”€\" * 15 + \"â”\")\n",
    "print(f\"â”‚ {'Label Type':<33} â”‚ {'Agreement':>13} â”‚ {'Cohen Îº':>13} â”‚\")\n",
    "print(\"â”œ\" + \"â”€\" * 35 + \"â”¼\" + \"â”€\" * 15 + \"â”¼\" + \"â”€\" * 15 + \"â”¤\")\n",
    "\n",
    "for col_name in labels_dict.keys():\n",
    "    ann1 = df[f'{col_name}_ann1']\n",
    "    ann2 = df[f'{col_name}_ann2']\n",
    "    \n",
    "    # Calculate agreement\n",
    "    agreement = sum([1 for a1, a2 in zip(ann1, ann2) if a1 == a2])\n",
    "    percent_agreement = (agreement / len(ann1)) * 100\n",
    "    \n",
    "    # Calculate Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(ann1, ann2)\n",
    "    \n",
    "    kappa_results.append({\n",
    "        'label_type': col_name,\n",
    "        'n_items': len(ann1),\n",
    "        'percent_agreement': round(percent_agreement, 1),\n",
    "        'cohens_kappa': round(kappa, 3)\n",
    "    })\n",
    "    \n",
    "    # Quality check\n",
    "    if col_name == 'target_population':\n",
    "        threshold = 0.75\n",
    "        status = \"âœ… PASS\" if kappa >= threshold else \"âš ï¸ BELOW\"\n",
    "    elif col_name == 'person_vs_organization':\n",
    "        threshold = 0.60\n",
    "        status = \"âœ… PASS\" if kappa >= threshold else \"âš ï¸ BELOW\"\n",
    "    else:\n",
    "        status = \"â„¹ï¸ INFO\"\n",
    "        threshold = None\n",
    "    \n",
    "    print(f\"â”‚ {col_name:<33} â”‚ {percent_agreement:>12.1f}% â”‚ {kappa:>13.3f} â”‚\")\n",
    "    \n",
    "print(\"â””\" + \"â”€\" * 35 + \"â”´\" + \"â”€\" * 15 + \"â”´\" + \"â”€\" * 15 + \"â”˜\")\n",
    "\n",
    "# Quality thresholds\n",
    "print(f\"\\nğŸ“‹ Quality Thresholds:\")\n",
    "print(f\"   â€¢ target_population: Îº â‰¥ 0.75 âœ“\")\n",
    "print(f\"   â€¢ person_vs_organization: Îº â‰¥ 0.60 âœ“\")\n",
    "print(f\"   â€¢ locals_vs_diaspora: No threshold (informational)\")\n",
    "\n",
    "# ============================================\n",
    "# Save Agreement Report\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"STEP 5: SAVING AGREEMENT REPORT\")\n",
    "print(f\"{'=' * 90}\")\n",
    "\n",
    "# Create agreement report DataFrame\n",
    "report_df = pd.DataFrame(kappa_results)\n",
    "\n",
    "# Save to CSV\n",
    "report_file = CLASSIFICATION_DIR / \"iteration_1_agreement_report.csv\"\n",
    "report_df.to_csv(report_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nâœ… Agreement report saved: {report_file}\")\n",
    "\n",
    "# Save detailed annotations\n",
    "detailed_file = CLASSIFICATION_DIR / \"iteration_1_double_annotations.csv\"\n",
    "df.to_csv(detailed_file, index=False, encoding='utf-8')\n",
    "print(f\"âœ… Detailed annotations saved: {detailed_file}\")\n",
    "\n",
    "# ============================================\n",
    "# Visualizations\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"STEP 6: GENERATING VISUALIZATIONS\")\n",
    "print(f\"{'=' * 90}\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Title\n",
    "fig.suptitle('Double Annotation Analysis - Inter-Annotator Agreement', \n",
    "             fontsize=18, fontweight='bold', y=0.98)\n",
    "\n",
    "# Plot 1: Cohen's Kappa by Label Type\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "categories = [r['label_type'].replace('_', ' ').title() for r in kappa_results]\n",
    "kappas = [r['cohens_kappa'] for r in kappa_results]\n",
    "colors = ['#2ecc71' if k >= 0.75 else '#f39c12' if k >= 0.60 else '#e74c3c' for k in kappas]\n",
    "\n",
    "bars = ax1.barh(categories, kappas, color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax1.set_xlabel(\"Cohen's Kappa (Îº)\", fontsize=12, fontweight='bold')\n",
    "ax1.set_title(\"Inter-Annotator Agreement by Classification Task\", fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.axvline(x=0.60, color='orange', linestyle='--', linewidth=2, alpha=0.7, label='Îº â‰¥ 0.60 (Good)')\n",
    "ax1.axvline(x=0.75, color='green', linestyle='--', linewidth=2, alpha=0.7, label='Îº â‰¥ 0.75 (Excellent)')\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.legend(loc='lower right', fontsize=10)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, kappa in zip(bars, kappas):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 0.02, bar.get_y() + bar.get_height()/2,\n",
    "            f'Îº = {kappa:.3f}',\n",
    "            ha='left', va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2-4: Confusion Matrices for each task\n",
    "for idx, col_name in enumerate(labels_dict.keys()):\n",
    "    ax = fig.add_subplot(gs[1 + idx//3, idx % 3])\n",
    "    \n",
    "    ann1 = df[f'{col_name}_ann1']\n",
    "    ann2 = df[f'{col_name}_ann2']\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    labels = sorted(set(ann1) | set(ann2))\n",
    "    conf_matrix = pd.crosstab(ann1, ann2, rownames=['Annotator 1'], \n",
    "                               colnames=['Annotator 2'], margins=False)\n",
    "    \n",
    "    # Reindex to ensure all labels present\n",
    "    for label in labels:\n",
    "        if label not in conf_matrix.index:\n",
    "            conf_matrix.loc[label] = 0\n",
    "        if label not in conf_matrix.columns:\n",
    "            conf_matrix[label] = 0\n",
    "    \n",
    "    conf_matrix = conf_matrix.loc[labels, labels]\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='YlOrRd', \n",
    "                ax=ax, cbar_kws={'label': 'Count'}, linewidths=1, linecolor='white')\n",
    "    ax.set_title(f'{col_name.replace(\"_\", \" \").title()}\\n(Îº = {kappas[idx]:.3f})', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Annotator 2', fontsize=10)\n",
    "    ax.set_ylabel('Annotator 1', fontsize=10)\n",
    "\n",
    "# Plot 5: Consensus Source Distribution\n",
    "ax5 = fig.add_subplot(gs[2, :2])\n",
    "\n",
    "# Aggregate consensus sources across all tasks\n",
    "all_sources = []\n",
    "for col_name in labels_dict.keys():\n",
    "    all_sources.extend(df[f'{col_name}_consensus_source'].tolist())\n",
    "\n",
    "source_counts = pd.Series(all_sources).value_counts()\n",
    "source_labels = {\n",
    "    'auto_agree': 'Auto Agreement',\n",
    "    'resolved_by_A1A2': 'Resolved by A1&A2',\n",
    "    'resolved_by_A3A4': 'Resolved by A3&A4',\n",
    "    'instructor_decision': 'Instructor Decision'\n",
    "}\n",
    "\n",
    "labels_plot = [source_labels.get(s, s) for s in source_counts.index]\n",
    "colors_pie = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c']\n",
    "\n",
    "wedges, texts, autotexts = ax5.pie(source_counts.values, labels=labels_plot, \n",
    "                                     autopct='%1.1f%%', colors=colors_pie,\n",
    "                                     startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
    "ax5.set_title('Consensus Resolution Methods (All Tasks)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 6: Agreement Statistics\n",
    "ax6 = fig.add_subplot(gs[2, 2])\n",
    "ax6.axis('off')\n",
    "\n",
    "stats_text = \"ğŸ“Š Summary Statistics\\n\" + \"â”€\" * 35 + \"\\n\\n\"\n",
    "stats_text += f\"Total Annotations: {len(df) * 3}\\n\"\n",
    "stats_text += f\"Annotator Pairs: 12 combinations\\n\"\n",
    "stats_text += f\"Total Annotators: 6 (A1-A6)\\n\\n\"\n",
    "\n",
    "stats_text += \"Agreement Rates:\\n\"\n",
    "for r in kappa_results:\n",
    "    stats_text += f\"  â€¢ {r['label_type'][:20]}: {r['percent_agreement']:.1f}%\\n\"\n",
    "\n",
    "stats_text += f\"\\nAverage Îº: {np.mean(kappas):.3f}\\n\"\n",
    "stats_text += f\"Min Îº: {np.min(kappas):.3f}\\n\"\n",
    "stats_text += f\"Max Îº: {np.max(kappas):.3f}\\n\"\n",
    "\n",
    "ax6.text(0.1, 0.95, stats_text, transform=ax6.transAxes,\n",
    "        fontsize=11, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "# Save figure\n",
    "fig_file = FIGURES_DIR / \"double_annotation_analysis.png\"\n",
    "plt.savefig(fig_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nâœ… Visualization saved: {fig_file}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# Final Report\n",
    "# ============================================\n",
    "print(f\"\\n{'=' * 90}\")\n",
    "print(\"âœ… DOUBLE ANNOTATION ANALYSIS COMPLETED\")\n",
    "print(f\"{'=' * 90}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Summary:\")\n",
    "print(f\"   â€¢ Users annotated: {len(df)}\")\n",
    "print(f\"   â€¢ Classification tasks: {len(labels_dict)}\")\n",
    "print(f\"   â€¢ Total annotations: {len(df) * len(labels_dict) * 2}\")\n",
    "print(f\"   â€¢ Annotators involved: {len(annotators)}\")\n",
    "print(f\"   â€¢ Unique pairs: {len(set(user_assignments))}\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Quality Metrics:\")\n",
    "for r in kappa_results:\n",
    "    print(f\"   â€¢ {r['label_type']}: Îº = {r['cohens_kappa']:.3f}, Agreement = {r['percent_agreement']:.1f}%\")\n",
    "\n",
    "print(f\"\\nğŸ“ Output Files:\")\n",
    "print(f\"   âœ“ {report_file.name}\")\n",
    "print(f\"   âœ“ {detailed_file.name}\")\n",
    "print(f\"   âœ“ {fig_file.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 14: Active Learning - Step 1: Definition of Classification Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STAGE 14: ACTIVE LEARNING - STEP 1: DEFINITION OF CLASSIFICATION TASKS\n",
    "================================================================================\n",
    "Define and inspect three classification tasks based on manually labeled data:\n",
    "1. target_population: target / non_target / unknown\n",
    "2. locals_vs_diaspora: local / diaspora / unknown (for target users only)\n",
    "3. person_vs_organization: person / organization / unknown\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"STAGE 14: ACTIVE LEARNING - STEP 1: DEFINITION OF CLASSIFICATION TASKS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# ============================================\n",
    "# Load Manually Labeled Data\n",
    "# ============================================\n",
    "print(\"\\nğŸ“‚ Loading manually labeled data...\")\n",
    "\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "INPUT_FILE = CLASSIFICATION_DIR / \"iteration_1_locals_vs_diaspora.csv\"\n",
    "\n",
    "# Load the DataFrame with manually labeled users\n",
    "df = pd.read_csv(INPUT_FILE, encoding='utf-8')\n",
    "print(f\"âœ… Loaded {len(df)} manually labeled users\")\n",
    "\n",
    "# Convert numeric labels to string labels\n",
    "# Mapping: 1=target, 0=non_target, 2=unknown\n",
    "label_mappings = {\n",
    "    'target_population': {0: 'non_target', 1: 'target', 2: 'unknown'},\n",
    "    'locals_vs_diaspora': {0: 'local', 1: 'diaspora', 2: 'unknown'},\n",
    "    'person_vs_organization': {0: 'organization', 1: 'person', 2: 'unknown'}\n",
    "}\n",
    "\n",
    "for col, mapping in label_mappings.items():\n",
    "    if col in df.columns:\n",
    "        df[f'{col}_label'] = df[col].map(mapping)\n",
    "\n",
    "# ============================================\n",
    "# Task 1: Target Population Classification\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"TASK 1: TARGET POPULATION CLASSIFICATION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Define target column\n",
    "target_col_1 = 'target_population_label'\n",
    "\n",
    "# Verify column exists\n",
    "if target_col_1 in df.columns:\n",
    "    print(f\"âœ… Column '{target_col_1}' exists in DataFrame\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(f\"\\nğŸ“Š Class Distribution:\")\n",
    "    class_counts = df[target_col_1].value_counts(dropna=False)\n",
    "    print(class_counts)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    print(f\"\\nğŸ“ˆ Percentage Distribution:\")\n",
    "    class_percentages = df[target_col_1].value_counts(normalize=True, dropna=False) * 100\n",
    "    for class_label, percentage in class_percentages.items():\n",
    "        count = class_counts[class_label]\n",
    "        print(f\"   {class_label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Determine task type\n",
    "    n_classes = df[target_col_1].nunique()\n",
    "    print(f\"\\nğŸ” Task Type: {'Binary' if n_classes == 2 else f'Multi-class ({n_classes} classes)'}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Column '{target_col_1}' NOT found in DataFrame\")\n",
    "\n",
    "# ============================================\n",
    "# Task 2: Locals vs Diaspora Classification\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"TASK 2: LOCALS VS DIASPORA CLASSIFICATION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Define target column\n",
    "target_col_2 = 'locals_vs_diaspora_label'\n",
    "\n",
    "# Verify column exists\n",
    "if target_col_2 in df.columns:\n",
    "    print(f\"âœ… Column '{target_col_2}' exists in DataFrame\")\n",
    "    \n",
    "    # IMPORTANT: Filter only users labeled as \"target\" in target_population\n",
    "    # This is a key requirement for the locals_vs_diaspora classification\n",
    "    print(f\"\\nğŸ” Filtering users labeled as 'target' in target_population...\")\n",
    "    \n",
    "    # Create filtered dataset for this task\n",
    "    df_target_only = df[df[target_col_1] == 'target'].copy()\n",
    "    print(f\"âœ… Filtered to {len(df_target_only)} target users (from {len(df)} total users)\")\n",
    "    \n",
    "    # Print class distribution for target users only\n",
    "    print(f\"\\nğŸ“Š Class Distribution (Target Users Only):\")\n",
    "    class_counts = df_target_only[target_col_2].value_counts(dropna=False)\n",
    "    print(class_counts)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    print(f\"\\nğŸ“ˆ Percentage Distribution:\")\n",
    "    class_percentages = df_target_only[target_col_2].value_counts(normalize=True, dropna=False) * 100\n",
    "    for class_label, percentage in class_percentages.items():\n",
    "        count = class_counts[class_label]\n",
    "        print(f\"   {class_label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Determine task type\n",
    "    n_classes = df_target_only[target_col_2].nunique()\n",
    "    print(f\"\\nğŸ” Task Type: {'Binary' if n_classes == 2 else f'Multi-class ({n_classes} classes)'}\")\n",
    "    \n",
    "    # Note about filtering\n",
    "    print(f\"\\nğŸ’¡ Note: This task only applies to users identified as 'target' population\")\n",
    "    print(f\"   Excluded {len(df) - len(df_target_only)} non-target users from this classification\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Column '{target_col_2}' NOT found in DataFrame\")\n",
    "\n",
    "# ============================================\n",
    "# Task 3: Person vs Organization Classification\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"TASK 3: PERSON VS ORGANIZATION CLASSIFICATION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Define target column\n",
    "target_col_3 = 'person_vs_organization_label'\n",
    "\n",
    "# Verify column exists\n",
    "if target_col_3 in df.columns:\n",
    "    print(f\"âœ… Column '{target_col_3}' exists in DataFrame\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(f\"\\nğŸ“Š Class Distribution:\")\n",
    "    class_counts = df[target_col_3].value_counts(dropna=False)\n",
    "    print(class_counts)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    print(f\"\\nğŸ“ˆ Percentage Distribution:\")\n",
    "    class_percentages = df[target_col_3].value_counts(normalize=True, dropna=False) * 100\n",
    "    for class_label, percentage in class_percentages.items():\n",
    "        count = class_counts[class_label]\n",
    "        print(f\"   {class_label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Determine task type\n",
    "    n_classes = df[target_col_3].nunique()\n",
    "    print(f\"\\nğŸ” Task Type: {'Binary' if n_classes == 2 else f'Multi-class ({n_classes} classes)'}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Column '{target_col_3}' NOT found in DataFrame\")\n",
    "\n",
    "# ============================================\n",
    "# Summary\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"âœ… CLASSIFICATION TASKS DEFINED\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\\nğŸ“‹ Summary of Classification Tasks:\")\n",
    "print(f\"\\n1. Target Population:\")\n",
    "print(f\"   â€¢ Column: {target_col_1}\")\n",
    "print(f\"   â€¢ Classes: target, non_target, unknown\")\n",
    "print(f\"   â€¢ Dataset Size: {len(df)} users\")\n",
    "\n",
    "print(f\"\\n2. Locals vs Diaspora:\")\n",
    "print(f\"   â€¢ Column: {target_col_2}\")\n",
    "print(f\"   â€¢ Classes: local, diaspora, unknown\")\n",
    "print(f\"   â€¢ Dataset Size: {len(df_target_only)} users (target population only)\")\n",
    "print(f\"   â€¢ Note: Only applies to users labeled as 'target'\")\n",
    "\n",
    "print(f\"\\n3. Person vs Organization:\")\n",
    "print(f\"   â€¢ Column: {target_col_3}\")\n",
    "print(f\"   â€¢ Classes: person, organization, unknown\")\n",
    "print(f\"   â€¢ Dataset Size: {len(df)} users\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Next Steps:\")\n",
    "print(f\"   â€¢ Step 2: Feature engineering\")\n",
    "print(f\"   â€¢ Step 3: Model training and evaluation\")\n",
    "print(f\"   â€¢ Step 4: Active learning loop implementation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 14: Active Learning - Step 1.5: Text Translation to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required package for translation\n",
    "%pip install deep-translator --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STAGE 14: ACTIVE LEARNING - STEP 1.5: TEXT TRANSLATION TO ENGLISH\n",
    "================================================================================\n",
    "Translate Persian/non-English text to English for improved TF-IDF feature extraction.\n",
    "This preprocessing step ensures all text is in English before feature engineering.\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from deep_translator import GoogleTranslator\n",
    "import time\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"STAGE 14: ACTIVE LEARNING - STEP 1.5: TEXT TRANSLATION TO ENGLISH\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# ============================================\n",
    "# Load Data\n",
    "# ============================================\n",
    "print(\"\\nğŸ“‚ Loading labeled data...\")\n",
    "\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "INPUT_FILE = CLASSIFICATION_DIR / \"iteration_1_locals_vs_diaspora.csv\"\n",
    "OUTPUT_FILE = CLASSIFICATION_DIR / \"iteration_1_locals_vs_diaspora_translated.csv\"\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv(INPUT_FILE, encoding='utf-8')\n",
    "print(f\"âœ… Loaded {len(df)} users from: {INPUT_FILE.name}\")\n",
    "\n",
    "# ============================================\n",
    "# Translation Function with Error Handling\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"TRANSLATION SETUP\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "def translate_text(text, max_retries=3):\n",
    "    \"\"\"\n",
    "    Translate text to English using deep-translator (Google Translate).\n",
    "    Handles errors and empty values gracefully.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to translate\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        Translated text or original text if translation fails\n",
    "    \"\"\"\n",
    "    # Handle empty or NaN values\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    text = str(text).strip()\n",
    "    if text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Try translation with retries\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            translator = GoogleTranslator(source='auto', target='en')\n",
    "            # Limit text length to avoid API errors (max 5000 chars)\n",
    "            text_to_translate = text[:5000] if len(text) > 5000 else text\n",
    "            translated = translator.translate(text_to_translate)\n",
    "            \n",
    "            # Return translated text if successful, otherwise return original\n",
    "            if translated and translated.strip() != '':\n",
    "                return translated.strip()\n",
    "            else:\n",
    "                return text\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                # Wait before retry (exponential backoff)\n",
    "                time.sleep(0.5 * (attempt + 1))\n",
    "                continue\n",
    "            else:\n",
    "                # If all retries fail, return original text\n",
    "                print(f\"   âš ï¸ Translation failed after {max_retries} attempts: {str(e)[:50]}...\")\n",
    "                return text\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"âœ… Translation function initialized\")\n",
    "print(\"   â€¢ Library: deep-translator (Google Translate)\")\n",
    "print(\"   â€¢ Source: auto-detect\")\n",
    "print(\"   â€¢ Target: English (en)\")\n",
    "print(\"   â€¢ Error handling: Graceful fallback to original text\")\n",
    "\n",
    "# ============================================\n",
    "# Translate Description Field\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"TRANSLATING DESCRIPTION FIELD\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Count non-empty descriptions\n",
    "non_empty_desc = df['description'].notna() & (df['description'] != '')\n",
    "desc_count = non_empty_desc.sum()\n",
    "print(f\"ğŸ“ Translating {desc_count} non-empty descriptions...\")\n",
    "\n",
    "# Translate with progress tracking\n",
    "df['description_en'] = ''\n",
    "translated_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.notna(row['description']) and str(row['description']).strip() != '':\n",
    "        translated = translate_text(row['description'])\n",
    "        df.at[idx, 'description_en'] = translated\n",
    "        \n",
    "        # Track progress - consider it translated if we got a result\n",
    "        if translated and translated.strip() != '':\n",
    "            translated_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "        \n",
    "        # Progress indicator every 10 items\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"   Progress: {idx + 1}/{len(df)} ({(idx + 1)/len(df)*100:.0f}%)\")\n",
    "        \n",
    "        # Small delay to avoid rate limiting\n",
    "        time.sleep(0.05)\n",
    "    else:\n",
    "        skipped_count += 1\n",
    "\n",
    "print(f\"\\nâœ… Description translation completed\")\n",
    "print(f\"   â€¢ Successfully translated: {translated_count}\")\n",
    "print(f\"   â€¢ Skipped/empty: {skipped_count}\")\n",
    "print(f\"   â€¢ Total processed: {desc_count}\")\n",
    "\n",
    "# ============================================\n",
    "# Translate Display Name Field\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"TRANSLATING DISPLAY NAME FIELD\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Count non-empty display names\n",
    "non_empty_name = df['display_name'].notna() & (df['display_name'] != '')\n",
    "name_count = non_empty_name.sum()\n",
    "print(f\"ğŸ“ Translating {name_count} non-empty display names...\")\n",
    "\n",
    "# Translate with progress tracking\n",
    "df['display_name_en'] = ''\n",
    "translated_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.notna(row['display_name']) and str(row['display_name']).strip() != '':\n",
    "        translated = translate_text(row['display_name'])\n",
    "        df.at[idx, 'display_name_en'] = translated\n",
    "        \n",
    "        # Track progress - consider it translated if we got a result\n",
    "        if translated and translated.strip() != '':\n",
    "            translated_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "        \n",
    "        # Progress indicator every 10 items\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"   Progress: {idx + 1}/{len(df)} ({(idx + 1)/len(df)*100:.0f}%)\")\n",
    "        \n",
    "        # Small delay to avoid rate limiting\n",
    "        time.sleep(0.05)\n",
    "    else:\n",
    "        skipped_count += 1\n",
    "\n",
    "print(f\"\\nâœ… Display name translation completed\")\n",
    "print(f\"   â€¢ Successfully translated: {translated_count}\")\n",
    "print(f\"   â€¢ Skipped/empty: {skipped_count}\")\n",
    "print(f\"   â€¢ Total processed: {name_count}\")\n",
    "\n",
    "# ============================================\n",
    "# Save Translated Data\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SAVING TRANSLATED DATA\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Save to new CSV file\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "print(f\"âœ… Saved translated data to: {OUTPUT_FILE.name}\")\n",
    "print(f\"   â€¢ Total rows: {len(df)}\")\n",
    "print(f\"   â€¢ New columns added: description_en, display_name_en\")\n",
    "print(f\"   â€¢ Original columns preserved: description, display_name\")\n",
    "\n",
    "# ============================================\n",
    "# Translation Summary\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"âœ… TRANSLATION COMPLETED\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\\nğŸ“Š Summary:\")\n",
    "print(f\"   â€¢ Input file:  {INPUT_FILE.name}\")\n",
    "print(f\"   â€¢ Output file: {OUTPUT_FILE.name}\")\n",
    "print(f\"   â€¢ Total users: {len(df)}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Translated Columns:\")\n",
    "print(f\"   â€¢ description    â†’ description_en\")\n",
    "print(f\"   â€¢ display_name   â†’ display_name_en\")\n",
    "\n",
    "# Show sample translations\n",
    "print(f\"\\nğŸ” Sample Translations (first 3 non-empty descriptions):\")\n",
    "sample_df = df[df['description_en'] != ''].head(3)\n",
    "for idx, row in sample_df.iterrows():\n",
    "    print(f\"\\n   User: {row['username']}\")\n",
    "    print(f\"   Original:   {row['description'][:80]}...\")\n",
    "    print(f\"   Translated: {row['description_en'][:80]}...\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Next Step:\")\n",
    "print(f\"   â€¢ Use translated columns for English-only TF-IDF feature extraction\")\n",
    "print(f\"   â€¢ File can be reused in future stages without re-translating\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 14: Active Learning - Step 2: Feature Engineering (English-Only TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STAGE 14: ACTIVE LEARNING - STEP 2: FEATURE ENGINEERING (ENGLISH-ONLY TF-IDF)\n",
    "================================================================================\n",
    "Extract TF-IDF features from translated English text for improved classification.\n",
    "Uses standard English text processing techniques.\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"STAGE 14: ACTIVE LEARNING - STEP 2: FEATURE ENGINEERING (ENGLISH-ONLY)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# ============================================\n",
    "# Load Translated Data\n",
    "# ============================================\n",
    "print(\"\\nğŸ“‚ Loading translated data...\")\n",
    "\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "TRANSLATED_FILE = CLASSIFICATION_DIR / \"iteration_1_locals_vs_diaspora_translated.csv\"\n",
    "\n",
    "# Load the DataFrame with translated columns\n",
    "df = pd.read_csv(TRANSLATED_FILE, encoding='utf-8')\n",
    "print(f\"âœ… Loaded {len(df)} users from: {TRANSLATED_FILE.name}\")\n",
    "\n",
    "# Verify translated columns exist\n",
    "if 'description_en' not in df.columns or 'display_name_en' not in df.columns:\n",
    "    print(\"âŒ Error: Translated columns not found!\")\n",
    "    print(\"   Please run Step 1.5 (Translation) first.\")\n",
    "else:\n",
    "    print(\"âœ“ Found translated columns: description_en, display_name_en\")\n",
    "\n",
    "# ============================================\n",
    "# PART 1: TEXTUAL FEATURES (English TF-IDF)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"PART 1: ENGLISH-ONLY TF-IDF FEATURES\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Handle missing values in translated text fields\n",
    "df['description_en'] = df['description_en'].fillna('')\n",
    "df['display_name_en'] = df['display_name_en'].fillna('')\n",
    "\n",
    "# -----------------------------------------\n",
    "# TF-IDF Feature Set 1: Description (English)\n",
    "# -----------------------------------------\n",
    "print(f\"\\n{'â”€' * 90}\")\n",
    "print(\"1ï¸âƒ£ TF-IDF Features from Description (English)\")\n",
    "print(f\"{'â”€' * 90}\")\n",
    "\n",
    "# Create TF-IDF vectorizer for English description\n",
    "tfidf_desc_en = TfidfVectorizer(\n",
    "    max_features=500,          # Top 500 features\n",
    "    min_df=2,                  # Ignore terms in < 2 documents\n",
    "    max_df=0.8,                # Ignore terms in > 80% of documents\n",
    "    ngram_range=(1, 2),        # Unigrams and bigrams\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    stop_words='english'       # Standard English stop words\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_tfidf_desc_en = tfidf_desc_en.fit_transform(df['description_en'])\n",
    "print(f\"âœ… TF-IDF (description_en): {X_tfidf_desc_en.shape}\")\n",
    "print(f\"   â€¢ Total features: {X_tfidf_desc_en.shape[1]}\")\n",
    "print(f\"   â€¢ Sparsity: {(1 - X_tfidf_desc_en.nnz / (X_tfidf_desc_en.shape[0] * X_tfidf_desc_en.shape[1])):.2%}\")\n",
    "\n",
    "# Show sample features\n",
    "feature_names = tfidf_desc_en.get_feature_names_out()\n",
    "print(f\"   â€¢ Sample features: {', '.join(feature_names[:10])}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# TF-IDF Feature Set 2: Display Name (English)\n",
    "# -----------------------------------------\n",
    "print(f\"\\n{'â”€' * 90}\")\n",
    "print(\"2ï¸âƒ£ TF-IDF Features from Display Name (English)\")\n",
    "print(f\"{'â”€' * 90}\")\n",
    "\n",
    "# Create TF-IDF vectorizer for English display name\n",
    "tfidf_name_en = TfidfVectorizer(\n",
    "    max_features=100,          # Names are shorter\n",
    "    min_df=1,\n",
    "    ngram_range=(1, 2),\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_tfidf_name_en = tfidf_name_en.fit_transform(df['display_name_en'])\n",
    "print(f\"âœ… TF-IDF (display_name_en): {X_tfidf_name_en.shape}\")\n",
    "print(f\"   â€¢ Total features: {X_tfidf_name_en.shape[1]}\")\n",
    "print(f\"   â€¢ Sparsity: {(1 - X_tfidf_name_en.nnz / (X_tfidf_name_en.shape[0] * X_tfidf_name_en.shape[1])):.2%}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# TF-IDF Feature Set 3: Combined Text (English)\n",
    "# -----------------------------------------\n",
    "print(f\"\\n{'â”€' * 90}\")\n",
    "print(\"3ï¸âƒ£ TF-IDF Features from Combined Text (Description + Display Name, English)\")\n",
    "print(f\"{'â”€' * 90}\")\n",
    "\n",
    "# Combine translated description and display_name\n",
    "df['combined_text_en'] = df['description_en'] + ' ' + df['display_name_en']\n",
    "\n",
    "# Create TF-IDF vectorizer for combined English text\n",
    "tfidf_combined_en = TfidfVectorizer(\n",
    "    max_features=600,          # More features for combined text\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 2),\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_tfidf_combined_en = tfidf_combined_en.fit_transform(df['combined_text_en'])\n",
    "print(f\"âœ… TF-IDF (description_en + display_name_en): {X_tfidf_combined_en.shape}\")\n",
    "print(f\"   â€¢ Total features: {X_tfidf_combined_en.shape[1]}\")\n",
    "print(f\"   â€¢ Sparsity: {(1 - X_tfidf_combined_en.nnz / (X_tfidf_combined_en.shape[0] * X_tfidf_combined_en.shape[1])):.2%}\")\n",
    "\n",
    "# Show sample features\n",
    "feature_names_combined = tfidf_combined_en.get_feature_names_out()\n",
    "print(f\"   â€¢ Sample features: {', '.join(feature_names_combined[:10])}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Text Features Summary\n",
    "# -----------------------------------------\n",
    "print(f\"\\n{'â”€' * 90}\")\n",
    "print(\"ğŸ“Š English TF-IDF Features Summary\")\n",
    "print(f\"{'â”€' * 90}\")\n",
    "print(f\"   â€¢ TF-IDF (description_en):        {X_tfidf_desc_en.shape[1]} features\")\n",
    "print(f\"   â€¢ TF-IDF (display_name_en):       {X_tfidf_name_en.shape[1]} features\")\n",
    "print(f\"   â€¢ TF-IDF (combined_en):           {X_tfidf_combined_en.shape[1]} features\")\n",
    "\n",
    "# ============================================\n",
    "# PART 2: NUMERICAL FEATURES (Same as before)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"PART 2: NUMERICAL FEATURES EXTRACTION\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_features = []\n",
    "feature_names_list = []\n",
    "\n",
    "# 1. Followers count\n",
    "if 'followers_count' in df.columns:\n",
    "    followers = df['followers_count'].fillna(0).values.reshape(-1, 1)\n",
    "    numerical_features.append(followers)\n",
    "    feature_names_list.append('followers_count')\n",
    "    print(f\"âœ“ Extracted: followers_count (mean={df['followers_count'].mean():.0f})\")\n",
    "\n",
    "# 2. Following count\n",
    "if 'following_count' in df.columns:\n",
    "    following = df['following_count'].fillna(0).values.reshape(-1, 1)\n",
    "    numerical_features.append(following)\n",
    "    feature_names_list.append('following_count')\n",
    "    print(f\"âœ“ Extracted: following_count (mean={df['following_count'].mean():.0f})\")\n",
    "\n",
    "# 3. Statuses count\n",
    "if 'statuses_count' in df.columns:\n",
    "    statuses = df['statuses_count'].fillna(0).values.reshape(-1, 1)\n",
    "    numerical_features.append(statuses)\n",
    "    feature_names_list.append('statuses_count')\n",
    "    print(f\"âœ“ Extracted: statuses_count (mean={df['statuses_count'].mean():.0f})\")\n",
    "\n",
    "# 4. Followers/Following ratio\n",
    "if 'followers_count' in df.columns and 'following_count' in df.columns:\n",
    "    followers_following_ratio = np.where(\n",
    "        df['following_count'] > 0,\n",
    "        df['followers_count'] / df['following_count'],\n",
    "        df['followers_count']\n",
    "    ).reshape(-1, 1)\n",
    "    numerical_features.append(followers_following_ratio)\n",
    "    feature_names_list.append('followers_following_ratio')\n",
    "    print(f\"âœ“ Calculated: followers_following_ratio (mean={followers_following_ratio.mean():.2f})\")\n",
    "\n",
    "# 5. Bio length (English)\n",
    "bio_length_en = df['description_en'].str.len().fillna(0).values.reshape(-1, 1)\n",
    "numerical_features.append(bio_length_en)\n",
    "feature_names_list.append('bio_length_en')\n",
    "print(f\"âœ“ Calculated: bio_length_en (mean={bio_length_en.mean():.0f} chars)\")\n",
    "\n",
    "# 6. Verified status\n",
    "if 'verified' in df.columns:\n",
    "    verified = df['verified'].fillna(False).astype(int).values.reshape(-1, 1)\n",
    "    numerical_features.append(verified)\n",
    "    feature_names_list.append('verified')\n",
    "    print(f\"âœ“ Extracted: verified ({verified.sum()} verified users)\")\n",
    "\n",
    "# Combine all numerical features\n",
    "X_numerical = np.hstack(numerical_features)\n",
    "print(f\"\\nâœ… Combined Numerical Features: {X_numerical.shape}\")\n",
    "print(f\"   â€¢ Total features: {X_numerical.shape[1]}\")\n",
    "print(f\"   â€¢ Feature names: {', '.join(feature_names_list)}\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Scale Numerical Features\n",
    "# -----------------------------------------\n",
    "print(f\"\\n{'â”€' * 90}\")\n",
    "print(\"ğŸ“ Scaling Numerical Features (StandardScaler)\")\n",
    "print(f\"{'â”€' * 90}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "print(f\"âœ… Scaled numerical features: {X_numerical_scaled.shape}\")\n",
    "\n",
    "# ============================================\n",
    "# PART 3: COMBINED FEATURE MATRICES\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"PART 3: COMBINED FEATURE MATRICES (ENGLISH TEXT + NUMERICAL)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# -----------------------------------------\n",
    "# Combination 1: Description (English) + Numerical\n",
    "# -----------------------------------------\n",
    "print(f\"\\n1ï¸âƒ£ Description (English) + Numerical\")\n",
    "X_desc_en_num = hstack([X_tfidf_desc_en, csr_matrix(X_numerical_scaled)])\n",
    "print(f\"   {X_desc_en_num.shape} = {X_tfidf_desc_en.shape[1]} (text) + {X_numerical_scaled.shape[1]} (num)\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Combination 2: Combined Text (English) + Numerical\n",
    "# -----------------------------------------\n",
    "print(f\"\\n2ï¸âƒ£ Combined Text (English) + Numerical\")\n",
    "X_combined_en_num = hstack([X_tfidf_combined_en, csr_matrix(X_numerical_scaled)])\n",
    "print(f\"   {X_combined_en_num.shape} = {X_tfidf_combined_en.shape[1]} (text) + {X_numerical_scaled.shape[1]} (num)\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Combination 3: All Text Features (English) + Numerical\n",
    "# -----------------------------------------\n",
    "print(f\"\\n3ï¸âƒ£ All Text Features (English) + Numerical\")\n",
    "X_all_text_en = hstack([X_tfidf_desc_en, X_tfidf_name_en])\n",
    "X_all_en_features = hstack([X_all_text_en, csr_matrix(X_numerical_scaled)])\n",
    "total_text_features = X_tfidf_desc_en.shape[1] + X_tfidf_name_en.shape[1]\n",
    "print(f\"   {X_all_en_features.shape} = {total_text_features} (text) + {X_numerical_scaled.shape[1]} (num)\")\n",
    "\n",
    "# ============================================\n",
    "# Summary\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"âœ… ENGLISH-ONLY FEATURE ENGINEERING COMPLETED\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\\nğŸ“Š Feature Matrices Summary:\")\n",
    "\n",
    "print(f\"\\nğŸ“ Text Features (English TF-IDF):\")\n",
    "print(f\"   â€¢ X_tfidf_desc_en:      {X_tfidf_desc_en.shape[0]} Ã— {X_tfidf_desc_en.shape[1]}\")\n",
    "print(f\"   â€¢ X_tfidf_name_en:      {X_tfidf_name_en.shape[0]} Ã— {X_tfidf_name_en.shape[1]}\")\n",
    "print(f\"   â€¢ X_tfidf_combined_en:  {X_tfidf_combined_en.shape[0]} Ã— {X_tfidf_combined_en.shape[1]}\")\n",
    "\n",
    "print(f\"\\nğŸ”¢ Numerical Features:\")\n",
    "print(f\"   â€¢ X_numerical_scaled:   {X_numerical_scaled.shape[0]} Ã— {X_numerical_scaled.shape[1]}\")\n",
    "print(f\"     Features: {', '.join(feature_names_list)}\")\n",
    "\n",
    "print(f\"\\nğŸ”— Combined Feature Matrices:\")\n",
    "print(f\"   â€¢ X_desc_en_num:        {X_desc_en_num.shape[0]} Ã— {X_desc_en_num.shape[1]}\")\n",
    "print(f\"   â€¢ X_combined_en_num:    {X_combined_en_num.shape[0]} Ã— {X_combined_en_num.shape[1]}\")\n",
    "print(f\"   â€¢ X_all_en_features:    {X_all_en_features.shape[0]} Ã— {X_all_en_features.shape[1]}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Benefits of English-Only Features:\")\n",
    "print(f\"   âœ“ Standard English stop words removal\")\n",
    "print(f\"   âœ“ Better tokenization for English text\")\n",
    "print(f\"   âœ“ More consistent feature extraction\")\n",
    "print(f\"   âœ“ Improved interpretability\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Next Steps:\")\n",
    "print(f\"   â€¢ Step 3: Model training and evaluation\")\n",
    "print(f\"   â€¢ Step 4: Active learning loop implementation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STAGE 14: MODULAR EXPERIMENT FRAMEWORK (FULL GRID)\n",
    "================================================================================\n",
    "Academic benchmark grid for Active Learning stage experiments.\n",
    "- 3 tasks\n",
    "- 12 feature sets (7 TFIDF + 5 individual numerical features)\n",
    "- 6 algorithms\n",
    "- 2 validation strategies (KFold, LOOCV)\n",
    "- 4 experiment variants per (binary/multiclass Ã— balanced/imbalanced)\n",
    "Outputs appended to: POIs/Classification/Experiments/experiments_results.csv\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneOut\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def _safe_get_score_binary(estimator, X):\n",
    "    \"\"\"Return continuous scores for ROC-AUC in binary classification.\"\"\"\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        proba = estimator.predict_proba(X)\n",
    "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "            return proba[:, 1]\n",
    "    if hasattr(estimator, \"decision_function\"):\n",
    "        return estimator.decision_function(X)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _manual_cv_predictions(estimator, X, y, cv, is_binary: bool):\n",
    "    \"\"\"Single-threaded manual CV loop (fit once per split).\"\"\"\n",
    "    y = np.asarray(y)\n",
    "    y_pred = np.empty(shape=(len(y),), dtype=int)\n",
    "    y_score = None\n",
    "    if is_binary:\n",
    "        y_score = np.full(shape=(len(y),), fill_value=np.nan, dtype=float)\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        model = clone(estimator)\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "\n",
    "        y_pred[test_idx] = model.predict(X[test_idx])\n",
    "\n",
    "        if is_binary and y_score is not None:\n",
    "            scores = _safe_get_score_binary(model, X[test_idx])\n",
    "            if scores is not None:\n",
    "                y_score[test_idx] = np.asarray(scores, dtype=float).ravel()\n",
    "\n",
    "    if is_binary and y_score is not None and np.isnan(y_score).all():\n",
    "        y_score = None\n",
    "\n",
    "    return y_pred, y_score\n",
    "\n",
    "\n",
    "def _evaluate_predictions(y_true, y_pred, y_score=None, is_binary=False):\n",
    "    \"\"\"Compute required metrics on predictions.\"\"\"\n",
    "    if is_binary:\n",
    "        avg = \"binary\"\n",
    "    else:\n",
    "        avg = \"weighted\"\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, average=avg, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, average=avg, zero_division=0)),\n",
    "        \"F1\": float(f1_score(y_true, y_pred, average=avg, zero_division=0)),\n",
    "        \"AUC\": np.nan,\n",
    "    }\n",
    "\n",
    "    if is_binary and y_score is not None:\n",
    "        try:\n",
    "            metrics[\"AUC\"] = float(roc_auc_score(y_true, y_score))\n",
    "        except Exception:\n",
    "            metrics[\"AUC\"] = np.nan\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _balance_indices(y_codes, random_state=42):\n",
    "    \"\"\"Downsample all present classes to the smallest non-zero class.\"\"\"\n",
    "    counts = pd.Series(y_codes).value_counts(dropna=False)\n",
    "    counts_nonzero = counts[counts > 0]\n",
    "    min_class_size = int(counts_nonzero.min()) if len(counts_nonzero) else 0\n",
    "\n",
    "    balanced = []\n",
    "    for cls in sorted(counts_nonzero.index.tolist()):\n",
    "        cls_idx = np.where(y_codes == cls)[0]\n",
    "        sampled = resample(\n",
    "            cls_idx,\n",
    "            n_samples=min_class_size,\n",
    "            random_state=random_state,\n",
    "            replace=False,\n",
    "        )\n",
    "        balanced.extend(sampled.tolist())\n",
    "\n",
    "    return sorted(balanced), min_class_size\n",
    "\n",
    "\n",
    "def prepare_task_view(df_all, X_all, task_name, filter_condition=None):\n",
    "    \"\"\"Apply task filtering and label NaN removal BEFORE any experiment.\"\"\"\n",
    "    if filter_condition:\n",
    "        df_task = df_all.query(filter_condition).copy()\n",
    "        task_indices = df_all.query(filter_condition).index.to_numpy()\n",
    "        X_task = X_all[task_indices]\n",
    "    else:\n",
    "        df_task = df_all.copy()\n",
    "        task_indices = df_all.index.to_numpy()\n",
    "        X_task = X_all\n",
    "\n",
    "    df_task = df_task.reset_index(drop=True)\n",
    "\n",
    "    # Drop NaN labels BEFORE any experiment (mandatory, especially for locals_vs_diaspora)\n",
    "    if df_task[task_name].isna().any():\n",
    "        nan_mask = ~df_task[task_name].isna()\n",
    "        df_task = df_task[nan_mask].copy().reset_index(drop=True)\n",
    "        keep_pos = np.where(nan_mask.values)[0]\n",
    "        X_task = X_task[keep_pos]\n",
    "\n",
    "    return df_task, X_task\n",
    "\n",
    "\n",
    "def make_feature_sets(df_all):\n",
    "    \"\"\"Prepare all mandatory feature sets on the FULL dataset (aligned by row).\"\"\"\n",
    "    # English-only text\n",
    "    desc = df_all[\"description_en\"].fillna(\"\").astype(str)\n",
    "    full_name = df_all[\"display_name_en\"].fillna(\"\").astype(str)\n",
    "    username = df_all[\"username\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # TF-IDF vectorizers\n",
    "    tfidf_desc = TfidfVectorizer(\n",
    "        max_features=500,\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        ngram_range=(1, 2),\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "    )\n",
    "\n",
    "    tfidf_name = TfidfVectorizer(\n",
    "        max_features=200,\n",
    "        min_df=1,\n",
    "        ngram_range=(1, 2),\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    "    )\n",
    "\n",
    "    tfidf_user = TfidfVectorizer(\n",
    "        max_features=200,\n",
    "        min_df=1,\n",
    "        ngram_range=(1, 2),\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "    )\n",
    "\n",
    "    X_desc = tfidf_desc.fit_transform(desc)\n",
    "    X_user = tfidf_user.fit_transform(username)\n",
    "    X_name = tfidf_name.fit_transform(full_name)\n",
    "\n",
    "    # Numerical features only (mandatory list)\n",
    "    followers = df_all[\"followers_count\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "    following = df_all[\"following_count\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "    statuses = df_all[\"statuses_count\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "\n",
    "    ratio = np.where(\n",
    "        df_all[\"following_count\"].fillna(0).to_numpy() > 0,\n",
    "        df_all[\"followers_count\"].fillna(0).to_numpy() / df_all[\"following_count\"].fillna(0).to_numpy(),\n",
    "        df_all[\"followers_count\"].fillna(0).to_numpy(),\n",
    "    ).reshape(-1, 1)\n",
    "\n",
    "    bio_len = desc.str.len().fillna(0).to_numpy().reshape(-1, 1)\n",
    "\n",
    "    # Scale each numerical feature individually\n",
    "    scaler_followers = StandardScaler()\n",
    "    followers_scaled = csr_matrix(scaler_followers.fit_transform(followers))\n",
    "    \n",
    "    scaler_following = StandardScaler()\n",
    "    following_scaled = csr_matrix(scaler_following.fit_transform(following))\n",
    "    \n",
    "    scaler_statuses = StandardScaler()\n",
    "    statuses_scaled = csr_matrix(scaler_statuses.fit_transform(statuses))\n",
    "    \n",
    "    scaler_ratio = StandardScaler()\n",
    "    ratio_scaled = csr_matrix(scaler_ratio.fit_transform(ratio))\n",
    "    \n",
    "    scaler_bio = StandardScaler()\n",
    "    bio_len_scaled = csr_matrix(scaler_bio.fit_transform(bio_len))\n",
    "\n",
    "    feature_sets = {\n",
    "        \"TFIDF_description\": X_desc,\n",
    "        \"TFIDF_username\": X_user,\n",
    "        \"TFIDF_full_name\": X_name,\n",
    "        \"TFIDF_description+username\": hstack([X_desc, X_user]),\n",
    "        \"TFIDF_description+full_name\": hstack([X_desc, X_name]),\n",
    "        \"TFIDF_username+full_name\": hstack([X_user, X_name]),\n",
    "        \"TFIDF_description+username+full_name\": hstack([X_desc, X_user, X_name]),\n",
    "        \"NUM_followers\": followers_scaled,\n",
    "        \"NUM_following\": following_scaled,\n",
    "        \"NUM_statuses\": statuses_scaled,\n",
    "        \"NUM_ratio\": ratio_scaled,\n",
    "        \"NUM_bio_len\": bio_len_scaled,\n",
    "    }\n",
    "\n",
    "    return feature_sets\n",
    "\n",
    "\n",
    "def make_algorithm_factories(random_state=42):\n",
    "    \"\"\"Return estimator factories keyed by algorithm name.\"\"\"\n",
    "\n",
    "    def lr_factory(is_binary, n_classes):\n",
    "        return LogisticRegression(max_iter=2000, random_state=random_state)\n",
    "\n",
    "    def dt_factory(is_binary, n_classes):\n",
    "        return DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "    def rf_factory(is_binary, n_classes):\n",
    "        return RandomForestClassifier(n_estimators=300, random_state=random_state, n_jobs=-1)\n",
    "\n",
    "    def svm_factory(is_binary, n_classes):\n",
    "        # probability=False is much faster; use decision_function for AUC.\n",
    "        return SVC(kernel=\"linear\", probability=False)\n",
    "\n",
    "    def xgb_factory(is_binary, n_classes):\n",
    "        common = dict(\n",
    "            n_estimators=400,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\",\n",
    "        )\n",
    "        if is_binary:\n",
    "            return XGBClassifier(\n",
    "                **common,\n",
    "                objective=\"binary:logistic\",\n",
    "                eval_metric=\"logloss\",\n",
    "            )\n",
    "        return XGBClassifier(\n",
    "            **common,\n",
    "            objective=\"multi:softprob\",\n",
    "            num_class=int(n_classes),\n",
    "            eval_metric=\"mlogloss\",\n",
    "        )\n",
    "\n",
    "    def ada_factory(is_binary, n_classes):\n",
    "        return AdaBoostClassifier(n_estimators=300, random_state=random_state)\n",
    "\n",
    "    return {\n",
    "        \"LogReg\": lr_factory,\n",
    "        \"DecisionTree\": dt_factory,\n",
    "        \"RandomForest\": rf_factory,\n",
    "        \"SVM\": svm_factory,\n",
    "        \"XGBoost\": xgb_factory,\n",
    "        \"AdaBoost\": ada_factory,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    *,\n",
    "    iteration,\n",
    "    target_column,\n",
    "    y_codes,\n",
    "    X,\n",
    "    mapping,\n",
    "    positive_class,\n",
    "    training_type,\n",
    "    K,\n",
    "    algorithm,\n",
    "    feature_set,\n",
    "    balanced,\n",
    "    multiclass,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"Run ONE experiment configuration and return ONE result row.\"\"\"\n",
    "\n",
    "    # Map codes -> labels for unknown handling\n",
    "    code_to_label = mapping\n",
    "    label_to_code = {v: k for k, v in code_to_label.items()}\n",
    "    unknown_code = label_to_code.get(\"unknown\", 2)\n",
    "\n",
    "    y_codes_work = np.asarray(y_codes)\n",
    "    X_work = X\n",
    "\n",
    "    # Binary vs multiclass logic\n",
    "    if multiclass:\n",
    "        is_binary = False\n",
    "        # Keep unknown\n",
    "        expected_codes = [0, 1, 2]\n",
    "    else:\n",
    "        is_binary = True\n",
    "        # Drop unknown\n",
    "        keep = y_codes_work != unknown_code\n",
    "        y_codes_work = y_codes_work[keep]\n",
    "        X_work = X_work[keep]\n",
    "        expected_codes = [0, 1]\n",
    "\n",
    "    # Class stats BEFORE balancing\n",
    "    counts_before = pd.Series(y_codes_work).value_counts().to_dict()\n",
    "    class_0 = int(counts_before.get(0, 0))\n",
    "    class_1 = int(counts_before.get(1, 0))\n",
    "    class_2 = int(counts_before.get(2, 0)) if multiclass else 0\n",
    "\n",
    "    present_counts = [c for c in [class_0, class_1, class_2] if c > 0]\n",
    "    min_class_size = int(min(present_counts)) if present_counts else 0\n",
    "\n",
    "    # Balance if needed (downsample)\n",
    "    if balanced:\n",
    "        idx_bal, _ = _balance_indices(y_codes_work, random_state=random_state)\n",
    "        y_codes_work = y_codes_work[idx_bal]\n",
    "        X_work = X_work[idx_bal]\n",
    "\n",
    "    # Recompute counts AFTER balancing (for sanity / sample size)\n",
    "    counts_after = pd.Series(y_codes_work).value_counts().to_dict()\n",
    "\n",
    "    # Prepare labels for sklearn\n",
    "    if is_binary:\n",
    "        pos_code = label_to_code[positive_class]\n",
    "        y_model = (y_codes_work == pos_code).astype(int)\n",
    "        n_classes = 2\n",
    "    else:\n",
    "        # Map 0/1/2 -> 0..C-1 for models like XGBoost\n",
    "        unique_codes = sorted(pd.Series(y_codes_work).dropna().unique().tolist())\n",
    "        code_to_num = {c: i for i, c in enumerate(unique_codes)}\n",
    "        y_model = np.array([code_to_num[c] for c in y_codes_work], dtype=int)\n",
    "        n_classes = len(unique_codes)\n",
    "\n",
    "    # CV strategy\n",
    "    if training_type == \"KFold\":\n",
    "        cv = StratifiedKFold(n_splits=K, shuffle=True, random_state=random_state)\n",
    "    elif training_type == \"LOOCV\":\n",
    "        cv = LeaveOneOut()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown training_type: {training_type}\")\n",
    "\n",
    "    # Build estimator\n",
    "    estimator = algorithm(is_binary, n_classes)\n",
    "\n",
    "    # Single-fit-per-split CV predictions (prevents double-fitting for binary)\n",
    "    y_pred, y_score = _manual_cv_predictions(estimator, X_work, y_model, cv=cv, is_binary=is_binary)\n",
    "\n",
    "    metrics = _evaluate_predictions(y_model, y_pred, y_score=y_score, is_binary=is_binary)\n",
    "\n",
    "    row = {\n",
    "        \"iteration\": int(iteration),\n",
    "        \"target_column\": str(target_column),\n",
    "        \"#classes\": int(3 if multiclass else 2),\n",
    "        \"#class_0\": int(class_0 if not balanced else counts_after.get(0, 0)),\n",
    "        \"#class_1\": int(class_1 if not balanced else counts_after.get(1, 0)),\n",
    "        \"#class_2\": int(class_2 if not balanced else counts_after.get(2, 0) if multiclass else 0),\n",
    "        \"min_class_size\": int(min_class_size),\n",
    "        \"training_type\": str(training_type),\n",
    "        \"K\": int(K if training_type == \"KFold\" else len(y_model)),\n",
    "        \"algorithm\": str(algorithm.__name__ if hasattr(algorithm, \"__name__\") else \"factory\"),\n",
    "        \"feature_set\": str(feature_set),\n",
    "        \"features_count\": int(X_work.shape[1]),\n",
    "        \"balanced\": bool(balanced),\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"F1\": metrics[\"F1\"],\n",
    "        \"AUC\": metrics[\"AUC\"],\n",
    "    }\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "# ============================\n",
    "# IO / Setup\n",
    "# ============================\n",
    "print(\"=\" * 90)\n",
    "print(\"STAGE 14: FULL ACTIVE LEARNING EXPERIMENT GRID\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "EXPERIMENTS_DIR = Path(\"POIs\") / \"Classification\" / \"Experiments\"\n",
    "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_FILE = EXPERIMENTS_DIR / \"experiments_results.csv\"\n",
    "\n",
    "print(f\"\\nğŸ“ Output directory: {EXPERIMENTS_DIR}\")\n",
    "print(f\"ğŸ“„ Results file: {OUTPUT_FILE}\")\n",
    "\n",
    "# Load data\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "TRANSLATED_FILE = CLASSIFICATION_DIR / \"iteration_1_locals_vs_diaspora_translated.csv\"\n",
    "\n",
    "df = pd.read_csv(TRANSLATED_FILE, encoding=\"utf-8\")\n",
    "print(f\"âœ… Loaded {len(df)} samples\")\n",
    "\n",
    "# Task config (labels are numeric in df)\n",
    "TASK_CONFIG = {\n",
    "    \"target_population\": {\n",
    "        \"mapping\": {0: \"non_target\", 1: \"target\", 2: \"unknown\"},\n",
    "        \"positive_class\": \"target\",\n",
    "        \"filter_condition\": None,\n",
    "    },\n",
    "    \"locals_vs_diaspora\": {\n",
    "        \"mapping\": {0: \"diaspora\", 1: \"local\", 2: \"unknown\"},\n",
    "        \"positive_class\": \"local\",\n",
    "        \"filter_condition\": \"target_population == 1\",\n",
    "    },\n",
    "    \"person_vs_organization\": {\n",
    "        \"mapping\": {0: \"organization\", 1: \"person\", 2: \"unknown\"},\n",
    "        \"positive_class\": \"person\",\n",
    "        \"filter_condition\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "TASKS = [\"target_population\", \"locals_vs_diaspora\", \"person_vs_organization\"]\n",
    "\n",
    "# Feature sets (ALL mandatory)\n",
    "print(\"\\nğŸ”§ Preparing feature sets...\")\n",
    "FEATURE_SETS = make_feature_sets(df)\n",
    "print(f\"âœ… Feature sets: {list(FEATURE_SETS.keys())}\")\n",
    "\n",
    "# Algorithms (ALL mandatory)\n",
    "ALGO_FACTORIES = make_algorithm_factories(random_state=42)\n",
    "print(f\"âœ… Algorithms: {list(ALGO_FACTORIES.keys())}\")\n",
    "\n",
    "TRAINING_TYPES = [(\"KFold\", 5), (\"LOOCV\", 5)]\n",
    "\n",
    "# ============================\n",
    "# Run grid\n",
    "# ============================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"RUNNING FULL GRID\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "iteration = 1\n",
    "new_rows = []\n",
    "\n",
    "total_configs = (\n",
    "    len(TASKS)\n",
    "    * len(FEATURE_SETS)\n",
    "    * len(ALGO_FACTORIES)\n",
    "    * len(TRAINING_TYPES)\n",
    "    * 4\n",
    ")\n",
    "print(f\"\\nPlanned experiment rows: {total_configs}\")\n",
    "\n",
    "for task_name in TASKS:\n",
    "    cfg = TASK_CONFIG[task_name]\n",
    "\n",
    "    for feature_set_name, X_full in FEATURE_SETS.items():\n",
    "        # Apply task filtering on THIS feature set\n",
    "        df_task, X_task = prepare_task_view(df, X_full, task_name, filter_condition=cfg[\"filter_condition\"])\n",
    "\n",
    "        # y codes aligned with X_task\n",
    "        y_codes = df_task[task_name].astype(int).to_numpy()\n",
    "\n",
    "        # Sanity: locals_vs_diaspora should be much smaller\n",
    "        if task_name == \"locals_vs_diaspora\":\n",
    "            print(f\"\\nğŸ” locals_vs_diaspora size check: {len(df_task)} samples (filtered)\")\n",
    "\n",
    "        for algo_name, algo_factory in ALGO_FACTORIES.items():\n",
    "            for training_type, K in TRAINING_TYPES:\n",
    "                # 4 variants per task\n",
    "                for multiclass in [True, False]:\n",
    "                    for balanced in [False, True]:\n",
    "                        row = run_experiment(\n",
    "                            iteration=iteration,\n",
    "                            target_column=task_name,\n",
    "                            y_codes=y_codes,\n",
    "                            X=X_task,\n",
    "                            mapping=cfg[\"mapping\"],\n",
    "                            positive_class=cfg[\"positive_class\"],\n",
    "                            training_type=training_type,\n",
    "                            K=K,\n",
    "                            algorithm=algo_factory,\n",
    "                            feature_set=feature_set_name,\n",
    "                            balanced=balanced,\n",
    "                            multiclass=multiclass,\n",
    "                            random_state=42,\n",
    "                        )\n",
    "\n",
    "                        # Store algorithm name (factory name is not meaningful)\n",
    "                        row[\"algorithm\"] = algo_name\n",
    "\n",
    "                        new_rows.append(row)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# Append (do not overwrite)\n",
    "write_header = not OUTPUT_FILE.exists()\n",
    "new_df.to_csv(OUTPUT_FILE, mode=\"a\", header=write_header, index=False)\n",
    "\n",
    "print(f\"âœ… Appended {len(new_df)} rows to {OUTPUT_FILE.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"NEW RESULTS (Preview)\")\n",
    "print(\"=\" * 90)\n",
    "print(new_df.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"âœ… STAGE 14 GRID COMPLETED\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ×ª×™×§×•×Ÿ: ×”×—×œ×¤×ª NUM_only ×‘-5 ×¤×™×¦'×¨×™× × ×•××¨×™×™× ×‘×•×“×“×™×\n",
    "\n",
    "**×©×œ×‘ 1:** ××—×™×§×ª ×©×•×¨×•×ª NUM_only ××§×•×‘×¥ ×”×ª×•×¦××•×ª  \n",
    "**×©×œ×‘ 2:** ×”×¨×¦×ª × ×™×¡×•×™×™× ×¢×‘×•×¨ 5 ×¤×™×¦'×¨×™× × ×•××¨×™×™× ×‘×•×“×“×™× ×•×”×•×¡×¤×ª× ×œ×§×•×‘×¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "×©×œ×‘ 1: ××—×™×§×ª ×©×•×¨×•×ª NUM_only ××§×•×‘×¥ experiments_results.csv\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "EXPERIMENTS_DIR = Path(\"POIs\") / \"Classification\" / \"Experiments\"\n",
    "OUTPUT_FILE = EXPERIMENTS_DIR / \"experiments_results.csv\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"×©×œ×‘ 1: ××—×™×§×ª ×©×•×¨×•×ª NUM_only\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ×§×¨×™××ª ×”×§×•×‘×¥ ×”×§×™×™×\n",
    "df_results = pd.read_csv(OUTPUT_FILE, encoding=\"utf-8\")\n",
    "print(f\"\\nğŸ“Š ×©×•×¨×•×ª ×œ×¤× ×™ ××—×™×§×”: {len(df_results)}\")\n",
    "\n",
    "# ×¡×¤×™×¨×ª ×©×•×¨×•×ª NUM_only\n",
    "num_only_count = (df_results['feature_set'] == 'NUM_only').sum()\n",
    "print(f\"ğŸ—‘ï¸  ×©×•×¨×•×ª NUM_only ×œ××—×™×§×”: {num_only_count}\")\n",
    "\n",
    "# ××—×™×§×ª ×”×©×•×¨×•×ª\n",
    "df_cleaned = df_results[df_results['feature_set'] != 'NUM_only'].copy()\n",
    "print(f\"âœ… ×©×•×¨×•×ª ××—×¨×™ ××—×™×§×”: {len(df_cleaned)}\")\n",
    "\n",
    "# ×©××™×¨×ª ×”×§×•×‘×¥ ×”×× ×•×§×”\n",
    "df_cleaned.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "print(f\"\\nğŸ’¾ ×”×§×•×‘×¥ × ×©××¨: {OUTPUT_FILE}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "×©×œ×‘ 2: ×”×¨×¦×ª × ×™×¡×•×™×™× ×¢×‘×•×¨ 5 ×¤×™×¦'×¨×™× × ×•××¨×™×™× ×‘×•×“×“×™×\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"×©×œ×‘ 2: ×”×¨×¦×ª × ×™×¡×•×™×™× ×¢×‘×•×¨ ×¤×™×¦'×¨×™× × ×•××¨×™×™× ×‘×•×“×“×™×\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ×˜×¢×™× ×ª × ×ª×•× ×™×\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "TRANSLATED_FILE = CLASSIFICATION_DIR / \"iteration_1_locals_vs_diaspora_translated.csv\"\n",
    "df = pd.read_csv(TRANSLATED_FILE, encoding=\"utf-8\")\n",
    "print(f\"\\nâœ… × ×˜×¢× ×• {len(df)} ×“×’×™××•×ª\")\n",
    "\n",
    "# ×”×¤×§×ª ×¤×™×¦'×¨×™× × ×•××¨×™×™×\n",
    "print(\"\\nğŸ”§ ×™×¦×™×¨×ª 5 ×¤×™×¦'×¨×™× × ×•××¨×™×™× ×‘×•×“×“×™×...\")\n",
    "\n",
    "followers = df[\"followers_count\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "following = df[\"following_count\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "statuses = df[\"statuses_count\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "\n",
    "ratio = np.where(\n",
    "    df[\"following_count\"].fillna(0).to_numpy() > 0,\n",
    "    df[\"followers_count\"].fillna(0).to_numpy() / df[\"following_count\"].fillna(0).to_numpy(),\n",
    "    df[\"followers_count\"].fillna(0).to_numpy(),\n",
    ").reshape(-1, 1)\n",
    "\n",
    "desc = df[\"description_en\"].fillna(\"\").astype(str)\n",
    "bio_len = desc.str.len().fillna(0).to_numpy().reshape(-1, 1)\n",
    "\n",
    "# × ×•×¨××œ×™×–×¦×™×” ×©×œ ×›×œ ×¤×™×¦'×¨ ×‘× ×¤×¨×“\n",
    "scaler_followers = StandardScaler()\n",
    "followers_scaled = csr_matrix(scaler_followers.fit_transform(followers))\n",
    "\n",
    "scaler_following = StandardScaler()\n",
    "following_scaled = csr_matrix(scaler_following.fit_transform(following))\n",
    "\n",
    "scaler_statuses = StandardScaler()\n",
    "statuses_scaled = csr_matrix(scaler_statuses.fit_transform(statuses))\n",
    "\n",
    "scaler_ratio = StandardScaler()\n",
    "ratio_scaled = csr_matrix(scaler_ratio.fit_transform(ratio))\n",
    "\n",
    "scaler_bio = StandardScaler()\n",
    "bio_len_scaled = csr_matrix(scaler_bio.fit_transform(bio_len))\n",
    "\n",
    "# ×™×¦×™×¨×ª ××™×œ×•×Ÿ ×©×œ feature sets ×—×“×©×™×\n",
    "NEW_FEATURE_SETS = {\n",
    "    \"NUM_followers\": followers_scaled,\n",
    "    \"NUM_following\": following_scaled,\n",
    "    \"NUM_statuses\": statuses_scaled,\n",
    "    \"NUM_ratio\": ratio_scaled,\n",
    "    \"NUM_bio_len\": bio_len_scaled,\n",
    "}\n",
    "\n",
    "print(f\"âœ… × ×•×¦×¨×•: {list(NEW_FEATURE_SETS.keys())}\")\n",
    "\n",
    "# ×—×™×©×•×‘ ××¡×¤×¨ × ×™×¡×•×™×™× ×¦×¤×•×™\n",
    "total_new = (\n",
    "    len(TASKS) * len(NEW_FEATURE_SETS) * len(ALGO_FACTORIES) * len(TRAINING_TYPES) * 4\n",
    ")\n",
    "print(f\"\\nğŸ“Š × ×™×¡×•×™×™× ×¦×¤×•×™×™×: {total_new}\")\n",
    "print(f\"   ({len(NEW_FEATURE_SETS)} ×¤×™×¦'×¨×™× Ã— {len(TASKS)} ××©×™××•×ª Ã— {len(ALGO_FACTORIES)} ××œ×’×•×¨×™×ª××™× Ã— {len(TRAINING_TYPES)} CV Ã— 4 ×•×¨×™×× ×˜×™×)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"××¨×™×¥ × ×™×¡×•×™×™×...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "iteration = 1\n",
    "new_rows = []\n",
    "\n",
    "for task_name in TASKS:\n",
    "    cfg = TASK_CONFIG[task_name]\n",
    "    \n",
    "    for feature_set_name, X_full in NEW_FEATURE_SETS.items():\n",
    "        # ×¡×™× ×•×Ÿ ×œ×¤×™ ××©×™××”\n",
    "        df_task, X_task = prepare_task_view(df, X_full, task_name, filter_condition=cfg[\"filter_condition\"])\n",
    "        y_codes = df_task[task_name].astype(int).to_numpy()\n",
    "        \n",
    "        print(f\"\\nğŸ“Œ {task_name} | {feature_set_name} | {len(df_task)} ×“×’×™××•×ª\")\n",
    "        \n",
    "        for algo_name, algo_factory in ALGO_FACTORIES.items():\n",
    "            for training_type, K in TRAINING_TYPES:\n",
    "                for multiclass in [True, False]:\n",
    "                    for balanced in [False, True]:\n",
    "                        row = run_experiment(\n",
    "                            iteration=iteration,\n",
    "                            target_column=task_name,\n",
    "                            y_codes=y_codes,\n",
    "                            X=X_task,\n",
    "                            mapping=cfg[\"mapping\"],\n",
    "                            positive_class=cfg[\"positive_class\"],\n",
    "                            training_type=training_type,\n",
    "                            K=K,\n",
    "                            algorithm=algo_factory,\n",
    "                            feature_set=feature_set_name,\n",
    "                            balanced=balanced,\n",
    "                            multiclass=multiclass,\n",
    "                            random_state=42,\n",
    "                        )\n",
    "                        row[\"algorithm\"] = algo_name\n",
    "                        new_rows.append(row)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"×©××™×¨×ª ×ª×•×¦××•×ª\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "# ×”×•×¡×¤×” ×œ×§×•×‘×¥ (append mode)\n",
    "EXPERIMENTS_DIR = Path(\"POIs\") / \"Classification\" / \"Experiments\"\n",
    "OUTPUT_FILE = EXPERIMENTS_DIR / \"experiments_results.csv\"\n",
    "\n",
    "# ×§×¨×™××ª ×”×§×•×‘×¥ ×”×§×™×™× ×›×“×™ ×œ×•×•×“× ×©×œ× ×›×•×ª×‘×™× header ××™×•×ª×¨\n",
    "existing_df = pd.read_csv(OUTPUT_FILE, encoding=\"utf-8\")\n",
    "print(f\"ğŸ“Š ×©×•×¨×•×ª ×§×™×™××•×ª ×‘×§×•×‘×¥: {len(existing_df)}\")\n",
    "\n",
    "# ×”×•×¡×¤×” ×‘×œ×™ header\n",
    "new_df.to_csv(OUTPUT_FILE, mode=\"a\", header=False, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ×•×™×“×•×\n",
    "final_df = pd.read_csv(OUTPUT_FILE, encoding=\"utf-8\")\n",
    "print(f\"âœ… × ×•×¡×¤×• {len(new_df)} ×©×•×¨×•×ª ×—×“×©×•×ª\")\n",
    "print(f\"ğŸ“Š ×¡×”\\\"×› ×©×•×¨×•×ª ×¢×›×©×™×•: {len(final_df)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"×ª×¦×•×’×” ××§×“×™××” ×©×œ ×©×•×¨×•×ª ×—×“×©×•×ª\")\n",
    "print(\"=\" * 80)\n",
    "print(new_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ×”×•×©×œ×! 5 ×¤×™×¦'×¨×™× × ×•××¨×™×™× ×‘×•×“×“×™× × ×•×¡×¤×• ×‘××§×•× NUM_only\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAGE 15: Active Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**×”×¨×¦×ª ××™×˜×¨×¦×™×” 2 ×¢×œ ×›×œ ×”××•×“×œ×™× ××©×œ×‘ 14**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STAGE 14 (NEW): FULL GRID ON MERGED ITERATION 2 LABELED DATA\n",
    "================================================================================\n",
    "This cell re-runs the Stage 14 experiment grid on the merged\n",
    "manual labels from iterations 1+2 (\"iteration_2_merged\"),\n",
    "**without changing any of the original Stage 14 code**.\n",
    "\n",
    "It uses the per-task merged CSV files in POIs/Classification/:\n",
    "- labeled_data_iteration_2_target_population.csv\n",
    "- labeled_data_iteration_2_locals_vs_diaspora.csv\n",
    "- labeled_data_iteration_2_person_vs_organization.csv\n",
    "\n",
    "Results are appended to:\n",
    "  POIs/Classification/Experiments/experiments_results.csv\n",
    "and a summary for this run is written to:\n",
    "  POIs/Classification/Experiments/experiments_summary_iteration_2_merged.csv\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneOut\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def _safe_get_score_binary_new(estimator, X):\n",
    "    \"\"\"Return continuous scores for ROC-AUC in binary classification (new cell).\"\"\"\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        proba = estimator.predict_proba(X)\n",
    "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "            return proba[:, 1]\n",
    "    if hasattr(estimator, \"decision_function\"):\n",
    "        return estimator.decision_function(X)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _manual_cv_predictions_new(estimator, X, y, cv, is_binary: bool):\n",
    "    \"\"\"Single-threaded manual CV loop (fit once per split) for this new cell.\"\"\"\n",
    "    y = np.asarray(y)\n",
    "    y_pred = np.empty(shape=(len(y),), dtype=int)\n",
    "    y_score = None\n",
    "    if is_binary:\n",
    "        y_score = np.full(shape=(len(y),), fill_value=np.nan, dtype=float)\n",
    "\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        model = clone(estimator)\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "\n",
    "        y_pred[test_idx] = model.predict(X[test_idx])\n",
    "\n",
    "        if is_binary and y_score is not None:\n",
    "            scores = _safe_get_score_binary_new(model, X[test_idx])\n",
    "            if scores is not None:\n",
    "                y_score[test_idx] = np.asarray(scores, dtype=float).ravel()\n",
    "\n",
    "    if is_binary and y_score is not None and np.isnan(y_score).all():\n",
    "        y_score = None\n",
    "\n",
    "    return y_pred, y_score\n",
    "\n",
    "\n",
    "def _evaluate_predictions_new(y_true, y_pred, y_score=None, is_binary=False):\n",
    "    \"\"\"Compute metrics on predictions for this new cell.\"\"\"\n",
    "    avg = \"binary\" if is_binary else \"weighted\"\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, average=avg, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, average=avg, zero_division=0)),\n",
    "        \"F1\": float(f1_score(y_true, y_pred, average=avg, zero_division=0)),\n",
    "        \"AUC\": np.nan,\n",
    "    }\n",
    "\n",
    "    if is_binary and y_score is not None:\n",
    "        try:\n",
    "            metrics[\"AUC\"] = float(roc_auc_score(y_true, y_score))\n",
    "        except Exception:\n",
    "            metrics[\"AUC\"] = np.nan\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def _balance_indices_new(y_codes, random_state=42):\n",
    "    \"\"\"Downsample all present classes to the smallest non-zero class (new cell).\"\"\"\n",
    "    counts = pd.Series(y_codes).value_counts(dropna=False)\n",
    "    counts_nonzero = counts[counts > 0]\n",
    "    min_class_size = int(counts_nonzero.min()) if len(counts_nonzero) else 0\n",
    "\n",
    "    balanced = []\n",
    "    for cls in sorted(counts_nonzero.index.tolist()):\n",
    "        cls_idx = np.where(y_codes == cls)[0]\n",
    "        sampled = resample(\n",
    "            cls_idx,\n",
    "            n_samples=min_class_size,\n",
    "            random_state=random_state,\n",
    "            replace=False,\n",
    ")\n",
    "        balanced.extend(sampled.tolist())\n",
    "\n",
    "    return sorted(balanced), min_class_size\n",
    "\n",
    "\n",
    "def make_feature_sets_new(df_all):\n",
    "    \"\"\"Prepare all Stage-14 feature sets for a given dataframe (with fallbacks).\"\"\"\n",
    "    # Text columns: prefer *_en if exist, else fall back\n",
    "    if \"description_en\" in df_all.columns:\n",
    "        desc = df_all[\"description_en\"].fillna(\"\").astype(str)\n",
    "    else:\n",
    "        desc = df_all[\"description\"].fillna(\"\").astype(str)\n",
    "\n",
    "    if \"display_name_en\" in df_all.columns:\n",
    "        full_name = df_all[\"display_name_en\"].fillna(\"\").astype(str)\n",
    "    else:\n",
    "        full_name = df_all[\"display_name\"].fillna(\"\").astype(str)\n",
    "\n",
    "    username = df_all[\"username\"].fillna(\"\").astype(str)\n",
    "\n",
    "    # TF-IDF vectorizers\n",
    "    tfidf_desc = TfidfVectorizer(\n",
    "        max_features=500,\n",
    "        min_df=2,\n",
    "        max_df=0.8,\n",
    "        ngram_range=(1, 2),\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    ")\n",
    "\n",
    "    tfidf_name = TfidfVectorizer(\n",
    "        max_features=200,\n",
    "        min_df=1,\n",
    "        ngram_range=(1, 2),\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    "        stop_words=\"english\",\n",
    ")\n",
    "\n",
    "    tfidf_user = TfidfVectorizer(\n",
    "        max_features=200,\n",
    "        min_df=1,\n",
    "        ngram_range=(1, 2),\n",
    "        strip_accents=\"unicode\",\n",
    "        lowercase=True,\n",
    ")\n",
    "\n",
    "    X_desc = tfidf_desc.fit_transform(desc)\n",
    "    X_user = tfidf_user.fit_transform(username)\n",
    "    X_name = tfidf_name.fit_transform(full_name)\n",
    "\n",
    "    # Numerical features\n",
    "    followers = df_all[\"followers_count\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "    following = df_all[\"following_count\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "    statuses = df_all[\"statuses_count\"].fillna(0).to_numpy().reshape(-1, 1)\n",
    "\n",
    "    ratio = np.where(\n",
    "        df_all[\"following_count\"].fillna(0).to_numpy() > 0,\n",
    "        df_all[\"followers_count\"].fillna(0).to_numpy() / df_all[\"following_count\"].fillna(0).to_numpy(),\n",
    "        df_all[\"followers_count\"].fillna(0).to_numpy(),\n",
    ").reshape(-1, 1)\n",
    "\n",
    "    bio_len = desc.str.len().fillna(0).to_numpy().reshape(-1, 1)\n",
    "\n",
    "    # Scale each numerical feature individually\n",
    "    scaler_followers = StandardScaler()\n",
    "    followers_scaled = csr_matrix(scaler_followers.fit_transform(followers))\n",
    "\n",
    "    scaler_following = StandardScaler()\n",
    "    following_scaled = csr_matrix(scaler_following.fit_transform(following))\n",
    "\n",
    "    scaler_statuses = StandardScaler()\n",
    "    statuses_scaled = csr_matrix(scaler_statuses.fit_transform(statuses))\n",
    "\n",
    "    scaler_ratio = StandardScaler()\n",
    "    ratio_scaled = csr_matrix(scaler_ratio.fit_transform(ratio))\n",
    "\n",
    "    scaler_bio = StandardScaler()\n",
    "    bio_len_scaled = csr_matrix(scaler_bio.fit_transform(bio_len))\n",
    "\n",
    "    feature_sets = {\n",
    "        \"TFIDF_description\": X_desc,\n",
    "        \"TFIDF_username\": X_user,\n",
    "        \"TFIDF_full_name\": X_name,\n",
    "        \"TFIDF_description+username\": hstack([X_desc, X_user]),\n",
    "        \"TFIDF_description+full_name\": hstack([X_desc, X_name]),\n",
    "        \"TFIDF_username+full_name\": hstack([X_user, X_name]),\n",
    "        \"TFIDF_description+username+full_name\": hstack([X_desc, X_user, X_name]),\n",
    "        \"NUM_followers\": followers_scaled,\n",
    "        \"NUM_following\": following_scaled,\n",
    "        \"NUM_statuses\": statuses_scaled,\n",
    "        \"NUM_ratio\": ratio_scaled,\n",
    "        \"NUM_bio_len\": bio_len_scaled,\n",
    "    }\n",
    "\n",
    "    return feature_sets\n",
    "\n",
    "\n",
    "def make_algorithm_factories_new(random_state=42):\n",
    "    \"\"\"Return estimator factories keyed by algorithm name (new cell).\"\"\"\n",
    "\n",
    "    def lr_factory(is_binary, n_classes):\n",
    "        return LogisticRegression(max_iter=2000, random_state=random_state)\n",
    "\n",
    "    def dt_factory(is_binary, n_classes):\n",
    "        return DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "    def rf_factory(is_binary, n_classes):\n",
    "        return RandomForestClassifier(n_estimators=300, random_state=random_state, n_jobs=-1)\n",
    "\n",
    "    def svm_factory(is_binary, n_classes):\n",
    "        # probability=False is much faster; use decision_function for AUC.\n",
    "        return SVC(kernel=\"linear\", probability=False)\n",
    "\n",
    "    def xgb_factory(is_binary, n_classes):\n",
    "        common = dict(\n",
    "            n_estimators=400,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\",\n",
    ")\n",
    "        if is_binary:\n",
    "            return XGBClassifier(\n",
    "                **common,\n",
    "                objective=\"binary:logistic\",\n",
    "                eval_metric=\"logloss\",\n",
    ")\n",
    "        return XGBClassifier(\n",
    "            **common,\n",
    "            objective=\"multi:softprob\",\n",
    "            num_class=int(n_classes),\n",
    "            eval_metric=\"mlogloss\",\n",
    ")\n",
    "\n",
    "    def ada_factory(is_binary, n_classes):\n",
    "        return AdaBoostClassifier(n_estimators=300, random_state=random_state)\n",
    "\n",
    "    return {\n",
    "        \"LogReg\": lr_factory,\n",
    "        \"DecisionTree\": dt_factory,\n",
    "        \"RandomForest\": rf_factory,\n",
    "        \"SVM\": svm_factory,\n",
    "        \"XGBoost\": xgb_factory,\n",
    "        \"AdaBoost\": ada_factory,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_experiment_new(\n",
    "    *,\n",
    "    iteration,\n",
    "    target_column,\n",
    "    y_codes,\n",
    "    X,\n",
    "    mapping,\n",
    "    positive_class,\n",
    "    training_type,\n",
    "    K,\n",
    "    algorithm,\n",
    "    feature_set,\n",
    "    balanced,\n",
    "    multiclass,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"Run ONE experiment configuration and return ONE result row (new cell).\"\"\"\n",
    "\n",
    "    code_to_label = mapping\n",
    "    label_to_code = {v: k for k, v in code_to_label.items()}\n",
    "    unknown_code = label_to_code.get(\"unknown\", 2)\n",
    "\n",
    "    y_codes_work = np.asarray(y_codes)\n",
    "    X_work = X\n",
    "\n",
    "    if multiclass:\n",
    "        is_binary = False\n",
    "    else:\n",
    "        is_binary = True\n",
    "        keep = y_codes_work != unknown_code\n",
    "        y_codes_work = y_codes_work[keep]\n",
    "        X_work = X_work[keep]\n",
    "\n",
    "    counts_before = pd.Series(y_codes_work).value_counts().to_dict()\n",
    "    class_0 = int(counts_before.get(0, 0))\n",
    "    class_1 = int(counts_before.get(1, 0))\n",
    "    class_2 = int(counts_before.get(2, 0)) if multiclass else 0\n",
    "\n",
    "    present_counts = [c for c in [class_0, class_1, class_2] if c > 0]\n",
    "    min_class_size = int(min(present_counts)) if present_counts else 0\n",
    "\n",
    "    if balanced:\n",
    "        idx_bal, _ = _balance_indices_new(y_codes_work, random_state=random_state)\n",
    "        y_codes_work = y_codes_work[idx_bal]\n",
    "        X_work = X_work[idx_bal]\n",
    "\n",
    "    counts_after = pd.Series(y_codes_work).value_counts().to_dict()\n",
    "\n",
    "    if is_binary:\n",
    "        pos_code = label_to_code[positive_class]\n",
    "        y_model = (y_codes_work == pos_code).astype(int)\n",
    "        n_classes = 2\n",
    "    else:\n",
    "        unique_codes = sorted(pd.Series(y_codes_work).dropna().unique().tolist())\n",
    "        code_to_num = {c: i for i, c in enumerate(unique_codes)}\n",
    "        y_model = np.array([code_to_num[c] for c in y_codes_work], dtype=int)\n",
    "        n_classes = len(unique_codes)\n",
    "\n",
    "    if training_type == \"KFold\":\n",
    "        cv = StratifiedKFold(n_splits=K, shuffle=True, random_state=random_state)\n",
    "    elif training_type == \"LOOCV\":\n",
    "        cv = LeaveOneOut()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown training_type: {training_type}\")\n",
    "\n",
    "    estimator = algorithm(is_binary, n_classes)\n",
    "\n",
    "    y_pred, y_score = _manual_cv_predictions_new(estimator, X_work, y_model, cv=cv, is_binary=is_binary)\n",
    "\n",
    "    metrics = _evaluate_predictions_new(y_model, y_pred, y_score=y_score, is_binary=is_binary)\n",
    "\n",
    "    row = {\n",
    "        \"iteration\": int(iteration),\n",
    "        \"target_column\": str(target_column),\n",
    "        \"#classes\": int(3 if multiclass else 2),\n",
    "        \"#class_0\": int(class_0 if not balanced else counts_after.get(0, 0)),\n",
    "        \"#class_1\": int(class_1 if not balanced else counts_after.get(1, 0)),\n",
    "        \"#class_2\": int(class_2 if not balanced else counts_after.get(2, 0) if multiclass else 0),\n",
    "        \"min_class_size\": int(min_class_size),\n",
    "        \"training_type\": str(training_type),\n",
    "        \"K\": int(K if training_type == \"KFold\" else len(y_model)),\n",
    "        \"algorithm\": str(algorithm.__name__ if hasattr(algorithm, \"__name__\") else \"factory\"),\n",
    "        \"feature_set\": str(feature_set),\n",
    "        \"features_count\": int(X_work.shape[1]),\n",
    "        \"balanced\": bool(balanced),\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"F1\": metrics[\"F1\"],\n",
    "        \"AUC\": metrics[\"AUC\"],\n",
    "    }\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "# ============================\n",
    "# IO / Setup (new cell)\n",
    "# ============================\n",
    "print(\"=\" * 90)\n",
    "print(\"STAGE 14 (NEW): FULL GRID ON MERGED ITERATION 2 LABELED DATA\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "EXPERIMENTS_DIR_NEW = Path(\"POIs\") / \"Classification\" / \"Experiments\"\n",
    "EXPERIMENTS_DIR_NEW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_FILE_NEW = EXPERIMENTS_DIR_NEW / \"experiments_results.csv\"\n",
    "\n",
    "print(f\"\\nğŸ“ Output directory: {EXPERIMENTS_DIR_NEW}\")\n",
    "print(f\"ğŸ“„ Results file: {OUTPUT_FILE_NEW}\")\n",
    "                                                        \n",
    "CLASSIFICATION_DIR_NEW = Path(\"POIs\") / \"Classification\"\n",
    "\n",
    "TP_FILE = CLASSIFICATION_DIR_NEW / \"labeled_data_iteration_2_target_population.csv\"\n",
    "LD_FILE = CLASSIFICATION_DIR_NEW / \"labeled_data_iteration_2_locals_vs_diaspora.csv\"\n",
    "PO_FILE = CLASSIFICATION_DIR_NEW / \"labeled_data_iteration_2_person_vs_organization.csv\"\n",
    "\n",
    "df_tp = pd.read_csv(TP_FILE, encoding=\"utf-8\")\n",
    "df_ld = pd.read_csv(LD_FILE, encoding=\"utf-8\")\n",
    "df_po = pd.read_csv(PO_FILE, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nâœ… Loaded merged iteration-2 labeled datasets:\")\n",
    "print(f\"   - target_population: {len(df_tp)} rows\")\n",
    "print(f\"   - locals_vs_diaspora: {len(df_ld)} rows\")\n",
    "print(f\"   - person_vs_organization: {len(df_po)} rows\")\n",
    "\n",
    "# For locals_vs_diaspora, attach target_population (if available)\n",
    "df_ld_merged = df_ld.merge(\n",
    "    df_tp[[\"username\", \"target_population\"]],\n",
    "    on=\"username\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Task configuration for this new run\n",
    "# ============================\n",
    "TASK_CONFIG_NEW = {\n",
    "    \"target_population\": {\n",
    "        \"df\": df_tp,\n",
    "        \"mapping\": {0: \"non_target\", 1: \"target\", 2: \"unknown\"},\n",
    "        \"positive_class\": \"target\",\n",
    "        \"filter_condition\": None,\n",
    "    },\n",
    "    \"locals_vs_diaspora\": {\n",
    "        \"df\": df_ld_merged,\n",
    "        \"mapping\": {0: \"diaspora\", 1: \"local\", 2: \"unknown\"},\n",
    "        \"positive_class\": \"local\",\n",
    "        # Use only users with target_population == 1 when available\n",
    "        \"filter_condition\": \"target_population == 1\",\n",
    "    },\n",
    "    \"person_vs_organization\": {\n",
    "        \"df\": df_po,\n",
    "        \"mapping\": {0: \"organization\", 1: \"person\", 2: \"unknown\"},\n",
    "        \"positive_class\": \"person\",\n",
    "        \"filter_condition\": None,\n",
    "    },\n",
    "}\n",
    "\n",
    "TASKS_NEW = [\"target_population\", \"locals_vs_diaspora\", \"person_vs_organization\"]\n",
    "\n",
    "ALGO_FACTORIES_NEW = make_algorithm_factories_new(random_state=42)\n",
    "TRAINING_TYPES_NEW = [(\"KFold\", 5), (\"LOOCV\", 5)]\n",
    "\n",
    "# ============================\n",
    "# Run grid for each task on its own merged dataset\n",
    "# ============================\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"RUNNING FULL GRID (iteration_2_merged, new cell)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "iteration_new = 2\n",
    "new_rows = []\n",
    "\n",
    "# Rough config count for logging only\n",
    "total_configs_est = (\n",
    "    len(TASKS_NEW)\n",
    "    * 12  # feature sets\n",
    "    * len(ALGO_FACTORIES_NEW)\n",
    "    * len(TRAINING_TYPES_NEW)\n",
    "    * 4\n",
    ")\n",
    "print(f\"\\nPlanned experiment rows (approx): {total_configs_est}\")\n",
    "\n",
    "for task_name in TASKS_NEW:\n",
    "    cfg = TASK_CONFIG_NEW[task_name]\n",
    "    df_all = cfg[\"df\"].copy()\n",
    "\n",
    "    # Optional filter (e.g., locals_vs_diaspora only where target_population == 1)\n",
    "    if cfg[\"filter_condition\"] is not None and \"target_population\" in df_all.columns:\n",
    "        before = len(df_all)\n",
    "        df_all = df_all.query(cfg[\"filter_condition\"]).copy()\n",
    "        print(f\"\\nğŸ” {task_name}: filtered from {before} to {len(df_all)} rows by '{cfg['filter_condition']}'\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ” {task_name}: using {len(df_all)} rows (no extra filter)\")\n",
    "\n",
    "    # Drop NaN labels in this task\n",
    "    if df_all[task_name].isna().any():\n",
    "        before_nan = len(df_all)\n",
    "        df_all = df_all[df_all[task_name].notna()].copy()\n",
    "        print(f\"   â†³ Dropped {before_nan - len(df_all)} rows with NaN in {task_name}\")\n",
    "\n",
    "    # Build feature sets for this task's dataframe\n",
    "    FEATURE_SETS_TASK = make_feature_sets_new(df_all)\n",
    "\n",
    "    for feature_set_name, X_full in FEATURE_SETS_TASK.items():\n",
    "        y_codes = df_all[task_name].astype(int).to_numpy()\n",
    "\n",
    "        for algo_name, algo_factory in ALGO_FACTORIES_NEW.items():\n",
    "            for training_type, K in TRAINING_TYPES_NEW:\n",
    "                for multiclass in [True, False]:\n",
    "                    for balanced in [False, True]:\n",
    "                        row = run_experiment_new(\n",
    "                            iteration=iteration_new,\n",
    "                            target_column=task_name,\n",
    "                            y_codes=y_codes,\n",
    "                            X=X_full,\n",
    "                            mapping=cfg[\"mapping\"],\n",
    "                            positive_class=cfg[\"positive_class\"],\n",
    "                            training_type=training_type,\n",
    "                            K=K,\n",
    "                            algorithm=algo_factory,\n",
    "                            feature_set=feature_set_name,\n",
    "                            balanced=balanced,\n",
    "                            multiclass=multiclass,\n",
    "                            random_state=42,\n",
    "                        )\n",
    "\n",
    "                        row[\"algorithm\"] = algo_name\n",
    "                        row[\"data_version\"] = \"iteration_2_merged\"\n",
    "\n",
    "                        new_rows.append(row)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"SAVING RESULTS (new cell)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "\n",
    "write_header = not OUTPUT_FILE_NEW.exists()\n",
    "new_df.to_csv(OUTPUT_FILE_NEW, mode=\"a\", header=write_header, index=False)\n",
    "\n",
    "print(f\"âœ… Appended {len(new_df)} rows to {OUTPUT_FILE_NEW.name}\")\n",
    "\n",
    "# Grouped summary only for this batch\n",
    "SUMMARY_FILE_NEW = EXPERIMENTS_DIR_NEW / \"experiments_summary_iteration_2_merged.csv\"\n",
    "\n",
    "group_cols = [\n",
    "    \"iteration\",\n",
    "    \"data_version\",\n",
    "    \"target_column\",\n",
    "    \"algorithm\",\n",
    "    \"feature_set\",\n",
    "    \"training_type\",\n",
    "    \"balanced\",\n",
    "    \"#classes\",\n",
    "]\n",
    "\n",
    "metrics_summary = (\n",
    "    new_df.groupby(group_cols)[[\"accuracy\", \"F1\", \"AUC\"]]\n",
    "    .agg([\"mean\", \"std\"])\n",
    ")\n",
    "\n",
    "metrics_summary.to_csv(SUMMARY_FILE_NEW)\n",
    "print(f\"âœ… Saved summary to {SUMMARY_FILE_NEW.name}\")\n",
    "print(\"Summary preview (first 20 groups):\")\n",
    "print(metrics_summary.head(20))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"NEW RESULTS (Preview from this cell)\")\n",
    "print(\"=\" * 90)\n",
    "print(new_df.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"âœ… STAGE 14 GRID COMPLETED (iteration_2_merged, new cell)\")\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STAGE 15 (ITERATION 2): PREDICTIONS FOR TARGET POPULATION AND PERSON VS ORGANIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 15 (ITERATION 2): PREDICTIONS FOR TARGET POPULATION AND PERSON VS ORGANIZATION\n",
    "# =====================================================================================\n",
    "# Generate predictions for 100 unlabeled users using best models from iteration 2\n",
    "# - Target Population: LogReg + TFIDF_description+username+full_name\n",
    "# - Person vs Organization: SVM + TFIDF_description+full_name\n",
    "# Exclude users from all manual labeling iterations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Paths\n",
    "BASE_PATH = Path('POIs/Classification')\n",
    "UNLABELED_FILE = BASE_PATH / 'unlabeled_users.csv'\n",
    "\n",
    "# Manual labeling files to exclude users from\n",
    "MANUAL_FILES = [\n",
    "    BASE_PATH / 'iteration_1_locals_vs_diaspora_translated.csv',\n",
    "    BASE_PATH / 'iteration_2_manual_labels_person_vs_organization.csv',\n",
    "    BASE_PATH / 'iteration_2_manual_labels_target_population.csv'\n",
    "]\n",
    "\n",
    "# Labeled data for training (iteration 2 merged)\n",
    "TP_LABELED_FILE = BASE_PATH / 'labeled_data_iteration_2_target_population.csv'\n",
    "PO_LABELED_FILE = BASE_PATH / 'labeled_data_iteration_2_person_vs_organization.csv'\n",
    "\n",
    "# Output files for predictions\n",
    "TP_OUTPUT_FILE = BASE_PATH / 'iteration_2_unlabeled_users_predictions_target_population.csv'\n",
    "PO_OUTPUT_FILE = BASE_PATH / 'iteration_2_unlabeled_users_predictions_person_vs_organization.csv'\n",
    "\n",
    "print('='*80)\n",
    "print('ITERATION 2: PREDICTIONS FOR UNLABELED USERS')\n",
    "print('='*80)\n",
    "\n",
    "# Load unlabeled users\n",
    "df_unlabeled = pd.read_csv(UNLABELED_FILE, encoding='utf-8')\n",
    "print(f'\\nTotal unlabeled users: {len(df_unlabeled)}')\n",
    "\n",
    "# Collect usernames from all manual labeling files\n",
    "manually_labeled_users = set()\n",
    "for manual_file in MANUAL_FILES:\n",
    "    if manual_file.exists():\n",
    "        df_manual = pd.read_csv(manual_file, encoding='utf-8')\n",
    "        manually_labeled_users.update(df_manual['username'].tolist())\n",
    "        print(f'Loaded {len(df_manual)} users from {manual_file.name}')\n",
    "\n",
    "print(f'\\nTotal unique manually labeled users to exclude: {len(manually_labeled_users)}')\n",
    "\n",
    "# Filter out manually labeled users\n",
    "df_unlabeled_filtered = df_unlabeled[~df_unlabeled['username'].isin(manually_labeled_users)].copy()\n",
    "print(f'Remaining unlabeled users: {len(df_unlabeled_filtered)}')\n",
    "\n",
    "# Prepare text columns\n",
    "df_unlabeled_filtered['description_clean'] = df_unlabeled_filtered['description'].fillna('').astype(str)\n",
    "df_unlabeled_filtered['username_clean'] = df_unlabeled_filtered['username'].fillna('').astype(str)\n",
    "df_unlabeled_filtered['full_name_clean'] = df_unlabeled_filtered['display_name'].fillna('').astype(str)\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('TASK 1: TARGET POPULATION')\n",
    "print('='*80)\n",
    "print('Model: LogReg + TFIDF_description+username+full_name')\n",
    "\n",
    "# Load training data for target population\n",
    "df_tp_train = pd.read_csv(TP_LABELED_FILE, encoding='utf-8')\n",
    "print(f'\\nTraining samples (before filtering): {len(df_tp_train)}')\n",
    "\n",
    "# Remove rows with NaN in target column and keep only binary classes (0 and 1)\n",
    "df_tp_train = df_tp_train[df_tp_train['target_population'].notna()].copy()\n",
    "df_tp_train = df_tp_train[df_tp_train['target_population'].isin([0, 1])].copy()\n",
    "print(f'Training samples (after filtering to binary): {len(df_tp_train)}')\n",
    "\n",
    "# Prepare training data\n",
    "df_tp_train['description_clean'] = df_tp_train['description'].fillna('').astype(str)\n",
    "df_tp_train['username_clean'] = df_tp_train['username'].fillna('').astype(str)\n",
    "df_tp_train['full_name_clean'] = df_tp_train['display_name'].fillna('').astype(str)\n",
    "\n",
    "# Build TFIDF features\n",
    "tfidf_desc_tp = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "tfidf_user_tp = TfidfVectorizer(max_features=50, ngram_range=(1, 1), analyzer='char_wb')\n",
    "tfidf_name_tp = TfidfVectorizer(max_features=50, ngram_range=(1, 2))\n",
    "\n",
    "X_train_desc = tfidf_desc_tp.fit_transform(df_tp_train['description_clean'])\n",
    "X_train_user = tfidf_user_tp.fit_transform(df_tp_train['username_clean'])\n",
    "X_train_name = tfidf_name_tp.fit_transform(df_tp_train['full_name_clean'])\n",
    "X_train = hstack([X_train_desc, X_train_user, X_train_name])\n",
    "\n",
    "y_train = df_tp_train['target_population'].astype(int).values\n",
    "\n",
    "# Train model\n",
    "clf_tp = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "clf_tp.fit(X_train, y_train)\n",
    "\n",
    "print(f'Model trained on {X_train.shape[0]} samples with {X_train.shape[1]} features')\n",
    "\n",
    "# Transform unlabeled data\n",
    "X_unlabeled_desc = tfidf_desc_tp.transform(df_unlabeled_filtered['description_clean'])\n",
    "X_unlabeled_user = tfidf_user_tp.transform(df_unlabeled_filtered['username_clean'])\n",
    "X_unlabeled_name = tfidf_name_tp.transform(df_unlabeled_filtered['full_name_clean'])\n",
    "X_unlabeled = hstack([X_unlabeled_desc, X_unlabeled_user, X_unlabeled_name])\n",
    "\n",
    "# Predict\n",
    "y_pred_tp = clf_tp.predict(X_unlabeled)\n",
    "y_proba_tp = clf_tp.predict_proba(X_unlabeled)\n",
    "\n",
    "# Calculate uncertainty scores (1 - max probability)\n",
    "uncertainty_tp = 1 - np.max(y_proba_tp, axis=1)\n",
    "\n",
    "# Create output dataframe\n",
    "df_tp_output = df_unlabeled_filtered.copy()\n",
    "df_tp_output['predicted_class'] = y_pred_tp\n",
    "df_tp_output['probability_class_0'] = y_proba_tp[:, 0]\n",
    "df_tp_output['probability_class_1'] = y_proba_tp[:, 1]\n",
    "df_tp_output['uncertainty_score'] = uncertainty_tp\n",
    "df_tp_output['confidence_level'] = 1 - uncertainty_tp\n",
    "\n",
    "# Sort by uncertainty (highest first)\n",
    "df_tp_output = df_tp_output.sort_values('uncertainty_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "df_tp_output.to_csv(TP_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "print(f'\\nPredictions saved to: {TP_OUTPUT_FILE}')\n",
    "print(f'Total predictions: {len(df_tp_output)}')\n",
    "print(f'Predicted as Iranian (class 1): {(y_pred_tp == 1).sum()}')\n",
    "print(f'Predicted as non-Iranian (class 0): {(y_pred_tp == 0).sum()}')\n",
    "print(f'\\nTop 10 most uncertain predictions:')\n",
    "print(df_tp_output[['username', 'display_name', 'predicted_class', 'uncertainty_score']].head(10).to_string(index=False))\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('TASK 2: PERSON VS ORGANIZATION')\n",
    "print('='*80)\n",
    "print('Model: SVM + TFIDF_description+full_name')\n",
    "\n",
    "# Load training data for person vs organization\n",
    "df_po_train = pd.read_csv(PO_LABELED_FILE, encoding='utf-8')\n",
    "print(f'\\nTraining samples (before filtering): {len(df_po_train)}')\n",
    "\n",
    "# Keep only binary classes (0 and 1)\n",
    "df_po_train = df_po_train[df_po_train['person_vs_organization'].isin([0, 1])].copy()\n",
    "print(f'Training samples (after filtering to binary): {len(df_po_train)}')\n",
    "\n",
    "# Prepare training data\n",
    "df_po_train['description_clean'] = df_po_train['description'].fillna('').astype(str)\n",
    "df_po_train['full_name_clean'] = df_po_train['display_name'].fillna('').astype(str)\n",
    "\n",
    "# Build TFIDF features\n",
    "tfidf_desc_po = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "tfidf_name_po = TfidfVectorizer(max_features=50, ngram_range=(1, 2))\n",
    "\n",
    "X_train_desc_po = tfidf_desc_po.fit_transform(df_po_train['description_clean'])\n",
    "X_train_name_po = tfidf_name_po.fit_transform(df_po_train['full_name_clean'])\n",
    "X_train_po = hstack([X_train_desc_po, X_train_name_po])\n",
    "\n",
    "y_train_po = df_po_train['person_vs_organization'].astype(int).values\n",
    "\n",
    "# Train model\n",
    "clf_po = SVC(kernel='rbf', probability=True, random_state=42, class_weight='balanced')\n",
    "clf_po.fit(X_train_po, y_train_po)\n",
    "\n",
    "print(f'Model trained on {X_train_po.shape[0]} samples with {X_train_po.shape[1]} features')\n",
    "\n",
    "# Transform unlabeled data\n",
    "X_unlabeled_desc_po = tfidf_desc_po.transform(df_unlabeled_filtered['description_clean'])\n",
    "X_unlabeled_name_po = tfidf_name_po.transform(df_unlabeled_filtered['full_name_clean'])\n",
    "X_unlabeled_po = hstack([X_unlabeled_desc_po, X_unlabeled_name_po])\n",
    "\n",
    "# Predict\n",
    "y_pred_po = clf_po.predict(X_unlabeled_po)\n",
    "y_proba_po = clf_po.predict_proba(X_unlabeled_po)\n",
    "\n",
    "# Calculate uncertainty scores\n",
    "uncertainty_po = 1 - np.max(y_proba_po, axis=1)\n",
    "\n",
    "# Create output dataframe\n",
    "df_po_output = df_unlabeled_filtered.copy()\n",
    "df_po_output['predicted_class'] = y_pred_po\n",
    "df_po_output['probability_class_0'] = y_proba_po[:, 0]\n",
    "df_po_output['probability_class_1'] = y_proba_po[:, 1]\n",
    "df_po_output['uncertainty_score'] = uncertainty_po\n",
    "df_po_output['confidence_level'] = 1 - uncertainty_po\n",
    "\n",
    "# Sort by uncertainty (highest first)\n",
    "df_po_output = df_po_output.sort_values('uncertainty_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "df_po_output.to_csv(PO_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "print(f'\\nPredictions saved to: {PO_OUTPUT_FILE}')\n",
    "print(f'Total predictions: {len(df_po_output)}')\n",
    "print(f'Predicted as Person (class 1): {(y_pred_po == 1).sum()}')\n",
    "print(f'Predicted as Organization (class 0): {(y_pred_po == 0).sum()}')\n",
    "print(f'\\nTop 10 most uncertain predictions:')\n",
    "print(df_po_output[['username', 'display_name', 'predicted_class', 'uncertainty_score']].head(10).to_string(index=False))\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('SUMMARY')\n",
    "print('='*80)\n",
    "print(f'\\nTarget Population predictions: {TP_OUTPUT_FILE.name}')\n",
    "print(f'Person vs Organization predictions: {PO_OUTPUT_FILE.name}')\n",
    "print('\\nBoth files contain:')\n",
    "print('  - All original user data')\n",
    "print('  - predicted_class')\n",
    "print('  - probability_class_0, probability_class_1')\n",
    "print('  - confidence_level (1 - uncertainty_score)')\n",
    "print('\\nNext step: Review top uncertain cases for potential iteration 3 labeling')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STAGE 15 (ITERATION 3): CREATE MANUAL LABELING FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 15 (ITERATION 3): CREATE MANUAL LABELING FILES\n",
    "# =====================================================\n",
    "# Extract top 100 most uncertain predictions for manual labeling in iteration 3\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = Path('POIs/Classification')\n",
    "\n",
    "# Input: Iteration 2 prediction files\n",
    "TP_PREDICTIONS_FILE = BASE_PATH / 'iteration_2_unlabeled_users_predictions_target_population.csv'\n",
    "PO_PREDICTIONS_FILE = BASE_PATH / 'iteration_2_unlabeled_users_predictions_person_vs_organization.csv'\n",
    "\n",
    "# Output: Iteration 3 manual labeling files\n",
    "TP_MANUAL_FILE = BASE_PATH / 'iteration_3_manual_labels_target_population.csv'\n",
    "PO_MANUAL_FILE = BASE_PATH / 'iteration_3_manual_labels_person_vs_organization.csv'\n",
    "\n",
    "print('='*80)\n",
    "print('ITERATION 3: CREATE MANUAL LABELING FILES')\n",
    "print('='*80)\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1: TARGET POPULATION\n",
    "# ============================================================================\n",
    "print('\\n' + '='*80)\n",
    "print('TASK 1: TARGET POPULATION')\n",
    "print('='*80)\n",
    "\n",
    "df_tp_pred = pd.read_csv(TP_PREDICTIONS_FILE, encoding='utf-8')\n",
    "print(f'Total predictions available: {len(df_tp_pred)}')\n",
    "\n",
    "# Already sorted by uncertainty_score (descending), take top 100\n",
    "df_tp_top100 = df_tp_pred.head(100).copy()\n",
    "\n",
    "# Keep only essential columns for manual labeling\n",
    "essential_cols = [\n",
    "    'username', 'display_name', 'description', \n",
    "    'followers_count', 'following_count', 'statuses_count',\n",
    "    'verified', 'created_at', 'location', 'url',\n",
    "    'predicted_class', 'uncertainty_score', 'confidence_level'\n",
    "]\n",
    "\n",
    "# Include optional columns if they exist\n",
    "if 'profile_image_url' in df_tp_top100.columns:\n",
    "    essential_cols.insert(-3, 'profile_image_url')\n",
    "if 'profile_banner_url' in df_tp_top100.columns:\n",
    "    essential_cols.insert(-3, 'profile_banner_url')\n",
    "if 'description_en' in df_tp_top100.columns:\n",
    "    essential_cols.insert(-3, 'description_en')\n",
    "if 'display_name_en' in df_tp_top100.columns:\n",
    "    essential_cols.insert(-3, 'display_name_en')\n",
    "\n",
    "df_tp_manual = df_tp_top100[essential_cols].copy()\n",
    "\n",
    "# Add empty manual labeling column at the end\n",
    "df_tp_manual['manual_target_population'] = ''\n",
    "\n",
    "# Reorder columns to put manual label column early for easier labeling\n",
    "cols_reordered = ['username', 'display_name', 'manual_target_population', 'predicted_class', \n",
    "                  'uncertainty_score', 'confidence_level'] + \\\n",
    "                 [c for c in df_tp_manual.columns if c not in \n",
    "                  ['username', 'display_name', 'manual_target_population', 'predicted_class', \n",
    "                   'uncertainty_score', 'confidence_level']]\n",
    "\n",
    "df_tp_manual = df_tp_manual[cols_reordered]\n",
    "\n",
    "# Save\n",
    "df_tp_manual.to_csv(TP_MANUAL_FILE, index=False, encoding='utf-8')\n",
    "\n",
    "print(f'\\nCreated: {TP_MANUAL_FILE.name}')\n",
    "print(f'Records: {len(df_tp_manual)}')\n",
    "print(f'Uncertainty range: {df_tp_manual[\"uncertainty_score\"].min():.4f} - {df_tp_manual[\"uncertainty_score\"].max():.4f}')\n",
    "print(f'\\nPredicted class distribution:')\n",
    "print(df_tp_manual['predicted_class'].value_counts().to_dict())\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 2: PERSON VS ORGANIZATION\n",
    "# ============================================================================\n",
    "print('\\n' + '='*80)\n",
    "print('TASK 2: PERSON VS ORGANIZATION')\n",
    "print('='*80)\n",
    "\n",
    "df_po_pred = pd.read_csv(PO_PREDICTIONS_FILE, encoding='utf-8')\n",
    "print(f'Total predictions available: {len(df_po_pred)}')\n",
    "\n",
    "# Already sorted by uncertainty_score (descending), take top 100\n",
    "df_po_top100 = df_po_pred.head(100).copy()\n",
    "\n",
    "# Keep only essential columns for manual labeling\n",
    "essential_cols_po = [\n",
    "    'username', 'display_name', 'description', \n",
    "    'followers_count', 'following_count', 'statuses_count',\n",
    "    'verified', 'created_at', 'location', 'url',\n",
    "    'predicted_class', 'uncertainty_score', 'confidence_level'\n",
    "]\n",
    "\n",
    "# Include optional columns if they exist\n",
    "if 'profile_image_url' in df_po_top100.columns:\n",
    "    essential_cols_po.insert(-3, 'profile_image_url')\n",
    "if 'profile_banner_url' in df_po_top100.columns:\n",
    "    essential_cols_po.insert(-3, 'profile_banner_url')\n",
    "if 'description_en' in df_po_top100.columns:\n",
    "    essential_cols_po.insert(-3, 'description_en')\n",
    "if 'display_name_en' in df_po_top100.columns:\n",
    "    essential_cols_po.insert(-3, 'display_name_en')\n",
    "\n",
    "df_po_manual = df_po_top100[essential_cols_po].copy()\n",
    "\n",
    "# Add empty manual labeling column at the end\n",
    "df_po_manual['manual_person_vs_organization'] = ''\n",
    "\n",
    "# Reorder columns to put manual label column early for easier labeling\n",
    "cols_reordered_po = ['username', 'display_name', 'manual_person_vs_organization', 'predicted_class', \n",
    "                     'uncertainty_score', 'confidence_level'] + \\\n",
    "                    [c for c in df_po_manual.columns if c not in \n",
    "                     ['username', 'display_name', 'manual_person_vs_organization', 'predicted_class', \n",
    "                      'uncertainty_score', 'confidence_level']]\n",
    "\n",
    "df_po_manual = df_po_manual[cols_reordered_po]\n",
    "\n",
    "# Save\n",
    "df_po_manual.to_csv(PO_MANUAL_FILE, index=False, encoding='utf-8')\n",
    "\n",
    "print(f'\\nCreated: {PO_MANUAL_FILE.name}')\n",
    "print(f'Records: {len(df_po_manual)}')\n",
    "print(f'Uncertainty range: {df_po_manual[\"uncertainty_score\"].min():.4f} - {df_po_manual[\"uncertainty_score\"].max():.4f}')\n",
    "print(f'\\nPredicted class distribution:')\n",
    "print(df_po_manual['predicted_class'].value_counts().to_dict())\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "print('\\n' + '='*80)\n",
    "print('SUMMARY')\n",
    "print('='*80)\n",
    "print(f'\\nCreated 2 manual labeling files for iteration 3:')\n",
    "print(f'  1. {TP_MANUAL_FILE.name} (100 records)')\n",
    "print(f'  2. {PO_MANUAL_FILE.name} (100 records)')\n",
    "print(f'\\nEach file contains:')\n",
    "print(f'  - Top 100 most uncertain predictions')\n",
    "print(f'  - Empty manual label column for your input')\n",
    "print(f'  - Model prediction for reference (predicted_class)')\n",
    "print(f'  - Uncertainty and confidence scores')\n",
    "print(f'  - All user profile information')\n",
    "print(f'\\nNext steps:')\n",
    "print(f'  1. Open each CSV file')\n",
    "print(f'  2. Fill in the manual label column (manual_target_population or manual_person_vs_organization)')\n",
    "print(f'  3. Save the files')\n",
    "print(f'  4. Run iteration 3 merge and training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 15 (ITERATION 2 - REVISED): PERSON VS ORGANIZATION WITH RANDOM FOREST + KEYWORD FEATURE\n",
    "# ================================================================================================\n",
    "# Regenerate predictions using RandomForest with enhanced features:\n",
    "# 1. TF-IDF on description (Persian/multilingual)\n",
    "# 2. TF-IDF on description_en (English translation)\n",
    "# 3. TF-IDF on display_name\n",
    "# 4. NEW: Organizational keyword count feature (Persian + English)\n",
    "#\n",
    "# Previous SVM was overconfident (73% predictions with >80% confidence)\n",
    "# RandomForest provides better probability calibration\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import re\n",
    "\n",
    "# ============================================================================\n",
    "# ORGANIZATIONAL KEYWORD FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "def count_organizational_keywords(text):\n",
    "    \"\"\"\n",
    "    Count organizational keywords in both Persian and English.\n",
    "    This is a FEATURE, not a rule-based classifier.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw description text (can be Persian, English, or mixed)\n",
    "    \n",
    "    Returns:\n",
    "        Integer count of matched organizational keywords\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return 0\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    # English organizational keywords\n",
    "    english_keywords = [\n",
    "        'news', 'official', 'agency', 'foundation', 'organization',\n",
    "        'media', 'channel', 'tv', 'radio', 'press', 'institute',\n",
    "        'association', 'ngo', 'company', 'corp', 'ltd', 'network',\n",
    "        'bureau', 'center', 'centre', 'service', 'group', 'team',\n",
    "        'department', 'ministry', 'office', 'club', 'federation'\n",
    "    ]\n",
    "    \n",
    "    # Persian organizational keywords\n",
    "    persian_keywords = [\n",
    "        'Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ', 'Ø±Ø³Ø§Ù†Ù‡', 'Ø³Ø§Ø²Ù…Ø§Ù†', 'Ø¨Ù†ÛŒØ§Ø¯', 'Ø´Ø¨Ú©Ù‡',\n",
    "        'Ú©Ø§Ù†Ø§Ù„', 'ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ†', 'Ø±Ø§Ø¯ÛŒÙˆ', 'Ù…ÙˆØ³Ø³Ù‡', 'Ø§Ù†Ø¬Ù…Ù†',\n",
    "        'Ø´Ø±Ú©Øª', 'Ø¯ÙØªØ±', 'Ø§Ø¯Ø§Ø±Ù‡', 'ÙˆØ²Ø§Ø±Øª', 'Ù…Ø±Ú©Ø²',\n",
    "        'Ø®Ø¨Ø±', 'Ø¨Ø§Ø´Ú¯Ø§Ù‡', 'ÙØ¯Ø±Ø§Ø³ÛŒÙˆÙ†', 'Ø§ØªØ­Ø§Ø¯ÛŒÙ‡', 'Ú¯Ø±ÙˆÙ‡'\n",
    "    ]\n",
    "    \n",
    "    # Count matches in both languages\n",
    "    count = 0\n",
    "    \n",
    "    # English keywords (word boundary matching)\n",
    "    for keyword in english_keywords:\n",
    "        # Use word boundaries to avoid partial matches\n",
    "        pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
    "        count += len(re.findall(pattern, text_lower))\n",
    "    \n",
    "    # Persian keywords (direct substring matching, as Persian doesn't have clear word boundaries)\n",
    "    for keyword in persian_keywords:\n",
    "        count += text_lower.count(keyword)\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Paths\n",
    "BASE_PATH = Path('POIs/Classification')\n",
    "UNLABELED_FILE = BASE_PATH / 'unlabeled_users.csv'\n",
    "\n",
    "# Manual labeling files to exclude users from\n",
    "MANUAL_FILES = [\n",
    "    BASE_PATH / 'iteration_1_locals_vs_diaspora_translated.csv',\n",
    "    BASE_PATH / 'iteration_2_manual_labels_person_vs_organization.csv',\n",
    "    BASE_PATH / 'iteration_2_manual_labels_target_population.csv'\n",
    "]\n",
    "\n",
    "# Labeled data for training\n",
    "PO_LABELED_FILE = BASE_PATH / 'labeled_data_iteration_2_person_vs_organization.csv'\n",
    "\n",
    "# Output file\n",
    "PO_OUTPUT_FILE = BASE_PATH / 'iteration_2_unlabeled_users_predictions_person_vs_organization.csv'\n",
    "\n",
    "print('='*80)\n",
    "print('PERSON VS ORGANIZATION - ENHANCED WITH ORGANIZATIONAL KEYWORD FEATURE')\n",
    "print('='*80)\n",
    "print('Model: RandomForest')\n",
    "print('Features: TF-IDF(description) + TF-IDF(display_name)')\n",
    "print('          + Organizational Keyword Count (Persian + English)')\n",
    "\n",
    "# Load unlabeled users\n",
    "df_unlabeled = pd.read_csv(UNLABELED_FILE, encoding='utf-8')\n",
    "\n",
    "# Collect usernames from all manual labeling files\n",
    "manually_labeled_users = set()\n",
    "for manual_file in MANUAL_FILES:\n",
    "    if manual_file.exists():\n",
    "        df_manual = pd.read_csv(manual_file, encoding='utf-8')\n",
    "        manually_labeled_users.update(df_manual['username'].tolist())\n",
    "\n",
    "# Filter out manually labeled users\n",
    "df_unlabeled_filtered = df_unlabeled[~df_unlabeled['username'].isin(manually_labeled_users)].copy()\n",
    "print(f'\\nUnlabeled users (after excluding manual labels): {len(df_unlabeled_filtered)}')\n",
    "\n",
    "# Prepare text columns\n",
    "df_unlabeled_filtered['description_clean'] = df_unlabeled_filtered['description'].fillna('').astype(str)\n",
    "df_unlabeled_filtered['full_name_clean'] = df_unlabeled_filtered['display_name'].fillna('').astype(str)\n",
    "\n",
    "# Load training data\n",
    "df_po_train = pd.read_csv(PO_LABELED_FILE, encoding='utf-8')\n",
    "print(f'Training samples (before filtering): {len(df_po_train)}')\n",
    "\n",
    "# Keep only binary classes (0 and 1)\n",
    "df_po_train = df_po_train[df_po_train['person_vs_organization'].isin([0, 1])].copy()\n",
    "print(f'Training samples (binary only): {len(df_po_train)}')\n",
    "\n",
    "# Prepare training data text columns\n",
    "df_po_train['description_clean'] = df_po_train['description'].fillna('').astype(str)\n",
    "df_po_train['full_name_clean'] = df_po_train['display_name'].fillna('').astype(str)\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD FEATURE MATRIX: TF-IDF + KEYWORD FEATURE\n",
    "# ============================================================================\n",
    "\n",
    "print('\\nBuilding enhanced feature matrix...')\n",
    "\n",
    "# 1. TF-IDF on description (Persian/multilingual)\n",
    "tfidf_desc = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
    "X_train_desc = tfidf_desc.fit_transform(df_po_train['description_clean'])\n",
    "\n",
    "# 2. TF-IDF on display name\n",
    "tfidf_name = TfidfVectorizer(max_features=50, ngram_range=(1, 2))\n",
    "X_train_name = tfidf_name.fit_transform(df_po_train['full_name_clean'])\n",
    "\n",
    "# 3. Organizational keyword count feature (NEW!)\n",
    "train_keyword_counts = df_po_train['description_clean'].apply(count_organizational_keywords).values\n",
    "train_keyword_feature = csr_matrix(train_keyword_counts.reshape(-1, 1))\n",
    "\n",
    "# Combine all features\n",
    "X_train = hstack([X_train_desc, X_train_name, train_keyword_feature])\n",
    "\n",
    "y_train = df_po_train['person_vs_organization'].astype(int).values\n",
    "\n",
    "print(f'  TF-IDF(description): {X_train_desc.shape[1]} features')\n",
    "print(f'  TF-IDF(display_name): {X_train_name.shape[1]} features')\n",
    "print(f'  Keyword count: 1 feature')\n",
    "print(f'  Total: {X_train.shape[1]} features')\n",
    "\n",
    "# Analyze keyword feature distribution\n",
    "keyword_stats = pd.DataFrame({\n",
    "    'keyword_count': train_keyword_counts,\n",
    "    'label': y_train\n",
    "})\n",
    "print(f'\\nKeyword Feature Statistics (Training):')\n",
    "print(f'  Organizations (0): mean={keyword_stats[keyword_stats[\"label\"]==0][\"keyword_count\"].mean():.2f}, '\n",
    "      f'median={keyword_stats[keyword_stats[\"label\"]==0][\"keyword_count\"].median():.0f}')\n",
    "print(f'  Persons (1): mean={keyword_stats[keyword_stats[\"label\"]==1][\"keyword_count\"].mean():.2f}, '\n",
    "      f'median={keyword_stats[keyword_stats[\"label\"]==1][\"keyword_count\"].median():.0f}')\n",
    "\n",
    "# Train RandomForest model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(f'\\nRandomForest trained on {X_train.shape[0]} samples with {X_train.shape[1]} features')\n",
    "\n",
    "# ============================================================================\n",
    "# TRANSFORM UNLABELED DATA WITH SAME FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "X_unlabeled_desc = tfidf_desc.transform(df_unlabeled_filtered['description_clean'])\n",
    "X_unlabeled_name = tfidf_name.transform(df_unlabeled_filtered['full_name_clean'])\n",
    "\n",
    "unlabeled_keyword_counts = df_unlabeled_filtered['description_clean'].apply(count_organizational_keywords).values\n",
    "unlabeled_keyword_feature = csr_matrix(unlabeled_keyword_counts.reshape(-1, 1))\n",
    "\n",
    "X_unlabeled = hstack([X_unlabeled_desc, X_unlabeled_name, unlabeled_keyword_feature])\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X_unlabeled)\n",
    "y_proba = clf.predict_proba(X_unlabeled)\n",
    "\n",
    "# Calculate uncertainty and confidence\n",
    "uncertainty = 1 - np.max(y_proba, axis=1)\n",
    "confidence = 1 - uncertainty\n",
    "\n",
    "# Create output dataframe\n",
    "df_output = df_unlabeled_filtered.copy()\n",
    "df_output['predicted_class'] = y_pred\n",
    "df_output['probability_class_0'] = y_proba[:, 0]\n",
    "df_output['probability_class_1'] = y_proba[:, 1]\n",
    "df_output['uncertainty_score'] = uncertainty\n",
    "df_output['confidence_level'] = confidence\n",
    "df_output['org_keyword_count'] = unlabeled_keyword_counts  # Add keyword count for analysis\n",
    "\n",
    "# Sort by uncertainty (highest first)\n",
    "df_output = df_output.sort_values('uncertainty_score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "df_output.to_csv(PO_OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "\n",
    "print(f'\\nPredictions saved to: {PO_OUTPUT_FILE.name}')\n",
    "print(f'Total predictions: {len(df_output)}')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('PREDICTIONS ANALYSIS')\n",
    "print('='*80)\n",
    "\n",
    "print(f'\\nClass Distribution:')\n",
    "print(f'  Person (class 1): {(y_pred == 1).sum()} ({100*(y_pred == 1).sum()/len(y_pred):.1f}%)')\n",
    "print(f'  Organization (class 0): {(y_pred == 0).sum()} ({100*(y_pred == 0).sum()/len(y_pred):.1f}%)')\n",
    "\n",
    "print(f'\\nKeyword Feature Statistics (Predictions):')\n",
    "print(f'  Organizations (0): mean={unlabeled_keyword_counts[y_pred==0].mean():.2f}, '\n",
    "      f'median={np.median(unlabeled_keyword_counts[y_pred==0]):.0f}')\n",
    "print(f'  Persons (1): mean={unlabeled_keyword_counts[y_pred==1].mean():.2f}, '\n",
    "      f'median={np.median(unlabeled_keyword_counts[y_pred==1]):.0f}')\n",
    "print(f'  Users with keywords>0: {(unlabeled_keyword_counts > 0).sum()} ({100*(unlabeled_keyword_counts > 0).sum()/len(unlabeled_keyword_counts):.1f}%)')\n",
    "\n",
    "print(f'\\nConfidence Distribution:')\n",
    "print(f'  Min: {confidence.min():.4f}')\n",
    "print(f'  Max: {confidence.max():.4f}')\n",
    "print(f'  Mean: {confidence.mean():.4f}')\n",
    "print(f'  Median: {np.median(confidence):.4f}')\n",
    "\n",
    "print(f'\\nUncertainty Distribution:')\n",
    "print(f'  Min: {uncertainty.min():.4f}')\n",
    "print(f'  Max: {uncertainty.max():.4f}')\n",
    "print(f'  Mean: {uncertainty.mean():.4f}')\n",
    "print(f'  Median: {np.median(uncertainty):.4f}')\n",
    "\n",
    "print(f'\\nConfidence Histogram:')\n",
    "bins = [(0, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]\n",
    "for low, high in bins:\n",
    "    count = ((confidence >= low) & (confidence < high)).sum()\n",
    "    pct = 100 * count / len(confidence)\n",
    "    print(f'  {low:.1f}-{high:.1f}: {count} ({pct:.1f}%)')\n",
    "\n",
    "print(f'\\nHigh Confidence Predictions (>0.8):')\n",
    "high_conf = df_output[df_output['confidence_level'] > 0.8]\n",
    "print(f'  Total: {len(high_conf)} ({100*len(high_conf)/len(df_output):.1f}%)')\n",
    "print(f'  Person: {(high_conf[\"predicted_class\"] == 1).sum()}')\n",
    "print(f'  Organization: {(high_conf[\"predicted_class\"] == 0).sum()}')\n",
    "\n",
    "print(f'\\nTop 10 most uncertain predictions:')\n",
    "print(df_output[['username', 'display_name', 'predicted_class', 'uncertainty_score', 'confidence_level']].head(10).to_string(index=False))\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('COMPARISON WITH PREVIOUS VERSION')\n",
    "print('='*80)\n",
    "print('Improvements with keyword feature:')\n",
    "print('  - Added organizational keyword count (Persian + English)')\n",
    "print('  - Total features increased from 150 to 151')\n",
    "print('  - Keyword feature detects org-specific words in descriptions')\n",
    "print('  - This is evidence-based feature engineering, not hard-coded rules')\n",
    "print('\\nExpected benefits:')\n",
    "print('  - Fewer overconfident mistakes on organizations')\n",
    "print('  - Better probability calibration')\n",
    "print('  - More informative uncertainty scores for active learning')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reate merged labeled file for ITERATION 3 (person_vs_organization)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 15: Create merged labeled file for ITERATION 3 (person_vs_organization)\n",
    "\n",
    "# This merges:\n",
    "\n",
    "#   - labeled_data_iteration_2_person_vs_organization.csv\n",
    "\n",
    "#   - iteration_3_manual_labels_person_vs_organization.csv\n",
    "\n",
    "# Into:\n",
    "\n",
    "#   - labeled_data_iteration_3_person_vs_organization.csv\n",
    "\n",
    "#\n",
    "\n",
    "# The merged file is deduplicated by username (keeping the latest iteration_source),\n",
    "\n",
    "# and ensures the label column is manual_person_vs_organization.\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"STAGE 15: MERGE LABELED DATA FOR ITERATION 3 - PERSON VS ORGANIZATION\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "# Base classification directory (reuse if already defined)\n",
    "\n",
    "try:\n",
    "\n",
    "    CLASSIFICATION_DIR\n",
    "\n",
    "except NameError:\n",
    "\n",
    "    CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "\n",
    "\n",
    "\n",
    "iter2_file = CLASSIFICATION_DIR / \"labeled_data_iteration_2_person_vs_organization.csv\"\n",
    "\n",
    "iter3_file = CLASSIFICATION_DIR / \"iteration_3_manual_labels_person_vs_organization.csv\"\n",
    "\n",
    "out_file = CLASSIFICATION_DIR / \"labeled_data_iteration_3_person_vs_organization.csv\"\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Iteration 2 labeled file: {iter2_file}\")\n",
    "\n",
    "print(f\"Iteration 3 manual labels: {iter3_file}\")\n",
    "\n",
    "print(f\"Output (iteration 3 merged): {out_file}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Load iteration 2 labeled data\n",
    "\n",
    "df_iter2 = pd.read_csv(iter2_file)\n",
    "\n",
    "print(f\"Loaded iteration 2 labeled data: {len(df_iter2)} rows\")\n",
    "\n",
    "\n",
    "\n",
    "# Ensure manual_person_vs_organization exists for iteration 2\n",
    "\n",
    "if \"manual_person_vs_organization\" not in df_iter2.columns:\n",
    "\n",
    "    if \"person_vs_organization\" in df_iter2.columns:\n",
    "\n",
    "        df_iter2[\"manual_person_vs_organization\"] = df_iter2[\"person_vs_organization\"]\n",
    "\n",
    "        print(\"Added manual_person_vs_organization column to iteration 2 from person_vs_organization\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise ValueError(\"Iteration 2 file must contain either 'manual_person_vs_organization' or 'person_vs_organization' column.\")\n",
    "\n",
    "\n",
    "\n",
    "df_iter2[\"iteration_source\"] = 2\n",
    "\n",
    "\n",
    "\n",
    "# Load iteration 3 manual labels\n",
    "\n",
    "df_iter3 = pd.read_csv(iter3_file)\n",
    "\n",
    "print(f\"Loaded iteration 3 manual labels: {len(df_iter3)} rows\")\n",
    "\n",
    "\n",
    "\n",
    "if \"manual_person_vs_organization\" not in df_iter3.columns:\n",
    "\n",
    "    raise ValueError(\"Iteration 3 file must contain 'manual_person_vs_organization' column.\")\n",
    "\n",
    "\n",
    "\n",
    "df_iter3[\"iteration_source\"] = 3\n",
    "\n",
    "\n",
    "\n",
    "# Align columns (union of both files)\n",
    "\n",
    "all_columns = sorted(set(df_iter2.columns) | set(df_iter3.columns))\n",
    "\n",
    "df_iter2_aligned = df_iter2.reindex(columns=all_columns)\n",
    "\n",
    "df_iter3_aligned = df_iter3.reindex(columns=all_columns)\n",
    "\n",
    "\n",
    "\n",
    "df_merged = pd.concat([df_iter2_aligned, df_iter3_aligned], ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal rows before de-duplication: {len(df_merged)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Deduplicate by username, keeping the latest iteration_source (3 over 2)\n",
    "\n",
    "if \"username\" not in df_merged.columns:\n",
    "\n",
    "    raise ValueError(\"Merged data must contain a 'username' column for de-duplication.\")\n",
    "\n",
    "\n",
    "\n",
    "df_merged = (\n",
    "\n",
    "    df_merged\n",
    "\n",
    "    .sort_values(by=[\"username\", \"iteration_source\"])\n",
    "\n",
    "    .drop_duplicates(subset=[\"username\"], keep=\"last\")\n",
    "\n",
    "    .reset_index(drop=True)\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Total rows after de-duplication by username: {len(df_merged)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Report iteration_source distribution\n",
    "\n",
    "print(\"\\nIteration source distribution (after merge):\")\n",
    "\n",
    "print(df_merged[\"iteration_source\"].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "\n",
    "# Report label distribution\n",
    "\n",
    "if \"manual_person_vs_organization\" in df_merged.columns:\n",
    "\n",
    "    print(\"\\nLabel distribution (manual_person_vs_organization):\")\n",
    "\n",
    "    print(df_merged[\"manual_person_vs_organization\"].value_counts(dropna=False))\n",
    "\n",
    "    if df_merged[\"manual_person_vs_organization\"].isna().any():\n",
    "\n",
    "        print(\"WARNING: There are rows with NaN in manual_person_vs_organization.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    raise ValueError(\"Merged data must contain 'manual_person_vs_organization' column.\")\n",
    "\n",
    "\n",
    "\n",
    "# Save merged iteration 3 labeled data\n",
    "\n",
    "df_merged.to_csv(out_file, index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nMerged labeled file for iteration 3 saved successfully.\")\n",
    "\n",
    "print(f\"Output path: {out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create merged labeled file for ITERATION 3 (target_population)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 15B: Create merged labeled file for ITERATION 3 (target_population)\n",
    "\n",
    "# This merges:\n",
    "#   - labeled_data_iteration_2_target_population.csv\n",
    "#   - iteration_3_manual_labels_target_population.csv\n",
    "# Into:\n",
    "#   - labeled_data_iteration_3_target_population.csv\n",
    "#\n",
    "# The merged file is deduplicated by username (keeping the latest\n",
    "# iteration_source), and ensures the label column is manual_target_population.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 15B: MERGE LABELED DATA FOR ITERATION 3 - TARGET POPULATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Base classification directory (reuse if already defined)\n",
    "try:\n",
    "    CLASSIFICATION_DIR\n",
    "except NameError:\n",
    "    CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "\n",
    "iter2_tp_file = CLASSIFICATION_DIR / \"labeled_data_iteration_2_target_population.csv\"\n",
    "iter3_tp_file = CLASSIFICATION_DIR / \"iteration_3_manual_labels_target_population.csv\"\n",
    "out_tp_file = CLASSIFICATION_DIR / \"labeled_data_iteration_3_target_population.csv\"\n",
    "\n",
    "print(f\"Iteration 2 labeled file (TP): {iter2_tp_file}\")\n",
    "print(f\"Iteration 3 manual labels (TP): {iter3_tp_file}\")\n",
    "print(f\"Output (iteration 3 merged TP): {out_tp_file}\\n\")\n",
    "\n",
    "# Load iteration 2 labeled data (target_population)\n",
    "df_tp_iter2 = pd.read_csv(iter2_tp_file, encoding=\"utf-8\")\n",
    "print(f\"Loaded iteration 2 TP labeled data: {len(df_tp_iter2)} rows\")\n",
    "\n",
    "# Ensure manual_target_population exists for iteration 2\n",
    "if \"manual_target_population\" not in df_tp_iter2.columns:\n",
    "    if \"target_population\" in df_tp_iter2.columns:\n",
    "        df_tp_iter2[\"manual_target_population\"] = df_tp_iter2[\"target_population\"]\n",
    "        print(\"Added manual_target_population column to iteration 2 from target_population\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Iteration 2 TP file must contain either 'manual_target_population' or 'target_population' column.\"\n",
    "        )\n",
    "\n",
    "# Mark iteration source\n",
    "df_tp_iter2[\"iteration_source\"] = 2\n",
    "\n",
    "# Load iteration 3 manual labels (target_population)\n",
    "df_tp_iter3 = pd.read_csv(iter3_tp_file, encoding=\"utf-8\")\n",
    "print(f\"Loaded iteration 3 TP manual labels: {len(df_tp_iter3)} rows\")\n",
    "\n",
    "if \"manual_target_population\" not in df_tp_iter3.columns:\n",
    "    raise ValueError(\"Iteration 3 TP file must contain 'manual_target_population' column.\")\n",
    "\n",
    "# Mark iteration source\n",
    "df_tp_iter3[\"iteration_source\"] = 3\n",
    "\n",
    "# Align columns (union of both files)\n",
    "all_tp_columns = sorted(set(df_tp_iter2.columns) | set(df_tp_iter3.columns))\n",
    "\n",
    "df_tp_iter2_aligned = df_tp_iter2.reindex(columns=all_tp_columns)\n",
    "df_tp_iter3_aligned = df_tp_iter3.reindex(columns=all_tp_columns)\n",
    "\n",
    "# Concatenate and de-duplicate by username (keep latest iteration_source)\n",
    "df_tp_merged = pd.concat([df_tp_iter2_aligned, df_tp_iter3_aligned], ignore_index=True)\n",
    "print(f\"\\nTotal TP rows before de-duplication: {len(df_tp_merged)}\")\n",
    "\n",
    "if \"username\" not in df_tp_merged.columns:\n",
    "    raise ValueError(\"Merged TP data must contain a 'username' column for de-duplication.\")\n",
    "\n",
    "df_tp_merged = (\n",
    "    df_tp_merged\n",
    "    .sort_values(by=[\"username\", \"iteration_source\"])\n",
    "    .drop_duplicates(subset=[\"username\"], keep=\"last\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Total TP rows after de-duplication by username: {len(df_tp_merged)}\")\n",
    "\n",
    "# Report iteration_source distribution\n",
    "print(\"\\nIteration source distribution for TP (after merge):\")\n",
    "print(df_tp_merged[\"iteration_source\"].value_counts(dropna=False))\n",
    "\n",
    "# Report label distribution\n",
    "if \"manual_target_population\" in df_tp_merged.columns:\n",
    "    print(\"\\nLabel distribution (manual_target_population):\")\n",
    "    print(df_tp_merged[\"manual_target_population\"].value_counts(dropna=False))\n",
    "    if df_tp_merged[\"manual_target_population\"].isna().any():\n",
    "        print(\"WARNING: There are rows with NaN in manual_target_population.\")\n",
    "else:\n",
    "    raise ValueError(\"Merged TP data must contain 'manual_target_population' column.\")\n",
    "\n",
    "# Save merged iteration 3 TP labeled data\n",
    "df_tp_merged.to_csv(out_tp_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nMerged TP labeled file for iteration 3 saved successfully.\")\n",
    "print(f\"Output path: {out_tp_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running all classifiers from step 14 on the person vs organization column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 16 (ITERATION 3): FULL GRID FOR PERSON VS ORGANIZATION (WITH PERSON/ORG FEATURES)\n",
    "# This cell runs the Stage 14-style experiment grid ONLY for the\n",
    "# person_vs_organization task, using the merged labeled data from\n",
    "# iteration 2 + iteration 3:\n",
    "#   POIs/Classification/labeled_data_iteration_3_person_vs_organization.csv\n",
    "#\n",
    "# It uses:\n",
    "#   - TF-IDF text features (username, display_name, description)\n",
    "#   - Numeric profile features (followers, following, statuses, verified)\n",
    "#   - PERSON_KEYWORDS / ORG_KEYWORDS indicator features\n",
    "#\n",
    "# IMPORTANT:\n",
    "#   * It DOES NOT modify or delete any existing rows in\n",
    "#       POIs/Classification/Experiments/experiments_results.csv\n",
    "#   * It only APPENDS new rows for iteration=3.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold, LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"STAGE 16 (ITERATION 3): PERSON VS ORGANIZATION GRID WITH PERSON/ORG FEATURES\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Ensure PERSON_KEYWORDS / ORG_KEYWORDS are available (fallback if not executed)\n",
    "try:\n",
    "    PERSON_KEYWORDS\n",
    "    ORG_KEYWORDS\n",
    "\n",
    "except NameError:\n",
    "    PERSON_KEYWORDS = [\n",
    "        \"doctor\", \"dr\", \"md\", \"prof\", \"professor\", \"researcher\", \"activist\", \"journalist\",\n",
    "        \"writer\", \"author\", \"artist\", \"singer\", \"player\", \"athlete\", \"lawyer\", \"engineer\",\n",
    "    ]\n",
    "\n",
    "    ORG_KEYWORDS = [\n",
    "        \"news\", \"agency\", \"organization\", \"foundation\", \"media\", \"company\", \"corp\", \"ltd\",\n",
    "        \"network\", \"center\", \"centre\", \"ministry\", \"office\", \"club\", \"association\", \"ngo\",\n",
    "        \"university\", \"hospital\", \"clinic\",\n",
    "    ]\n",
    "\n",
    "    print(\"[INFO] PERSON_KEYWORDS / ORG_KEYWORDS were not defined in this kernel; using fallback lists.\")\n",
    "\n",
    "# Helper functions (self-contained copy from Stage 14 NEW)\n",
    "\n",
    "def _safe_get_score_binary_new(estimator, X):\n",
    "    if hasattr(estimator, \"predict_proba\"):\n",
    "        proba = estimator.predict_proba(X)\n",
    "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
    "            return proba[:, 1]\n",
    "    if hasattr(estimator, \"decision_function\"):\n",
    "        return estimator.decision_function(X)\n",
    "    return None\n",
    "\n",
    "def _manual_cv_predictions_new(estimator, X, y, cv, is_binary: bool):\n",
    "    y = np.asarray(y)\n",
    "    y_pred = np.empty(shape=(len(y),), dtype=int)\n",
    "    y_score = None\n",
    "    if is_binary:\n",
    "        y_score = np.full(shape=(len(y),), fill_value=np.nan, dtype=float)\n",
    "    for train_idx, test_idx in cv.split(X, y):\n",
    "        model = clone(estimator)\n",
    "        model.fit(X[train_idx], y[train_idx])\n",
    "        y_pred[test_idx] = model.predict(X[test_idx])\n",
    "        if is_binary and y_score is not None:\n",
    "            scores = _safe_get_score_binary_new(model, X[test_idx])\n",
    "            if scores is not None:\n",
    "                y_score[test_idx] = np.asarray(scores, dtype=float).ravel()\n",
    "    if is_binary and y_score is not None and np.isnan(y_score).all():\n",
    "        y_score = None\n",
    "    return y_pred, y_score\n",
    "\n",
    "def _evaluate_predictions_new(y_true, y_pred, y_score=None, is_binary=False):\n",
    "    avg = \"binary\" if is_binary else \"weighted\"\n",
    "    metrics = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, average=avg, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, average=avg, zero_division=0)),\n",
    "        \"F1\": float(f1_score(y_true, y_pred, average=avg, zero_division=0)),\n",
    "        \"AUC\": np.nan,\n",
    "    }\n",
    "\n",
    "    if is_binary and y_score is not None:\n",
    "        try:\n",
    "            metrics[\"AUC\"] = float(roc_auc_score(y_true, y_score))\n",
    "        except Exception:\n",
    "            metrics[\"AUC\"] = np.nan\n",
    "    return metrics\n",
    "\n",
    "def _balance_indices_new(y_codes, random_state=42):\n",
    "    counts = pd.Series(y_codes).value_counts(dropna=False)\n",
    "    counts_nonzero = counts[counts > 0]\n",
    "    min_class_size = int(counts_nonzero.min()) if len(counts_nonzero) else 0\n",
    "    balanced = []\n",
    "    for cls in sorted(counts_nonzero.index.tolist()):\n",
    "        cls_idx = np.where(y_codes == cls)[0]\n",
    "        sampled = resample(cls_idx, n_samples=min_class_size, random_state=random_state, replace=False)\n",
    "        balanced.extend(sampled.tolist())\n",
    "    return sorted(balanced), min_class_size\n",
    "\n",
    "def make_algorithm_factories_new(random_state=42):\n",
    "    def lr_factory(is_binary, n_classes):\n",
    "        return LogisticRegression(max_iter=2000, random_state=random_state)\n",
    "    def dt_factory(is_binary, n_classes):\n",
    "        return DecisionTreeClassifier(random_state=random_state)\n",
    "    def rf_factory(is_binary, n_classes):\n",
    "        return RandomForestClassifier(n_estimators=300, random_state=random_state, n_jobs=-1)\n",
    "    def svm_factory(is_binary, n_classes):\n",
    "        return SVC(kernel=\"linear\", probability=False)\n",
    "    def xgb_factory(is_binary, n_classes):\n",
    "        common = dict(\n",
    "            n_estimators=400,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\",\n",
    "        )\n",
    "        if is_binary:\n",
    "            return XGBClassifier(**common, objective=\"binary:logistic\", eval_metric=\"logloss\")\n",
    "        return XGBClassifier(**common, objective=\"multi:softprob\", num_class=int(n_classes), eval_metric=\"mlogloss\")\n",
    "    def ada_factory(is_binary, n_classes):\n",
    "        return AdaBoostClassifier(n_estimators=300, random_state=random_state)\n",
    "\n",
    "    return {\n",
    "        \"LogReg\": lr_factory,\n",
    "        \"DecisionTree\": dt_factory,\n",
    "        \"RandomForest\": rf_factory,\n",
    "        \"SVM\": svm_factory,\n",
    "        \"XGBoost\": xgb_factory,\n",
    "        \"AdaBoost\": ada_factory,\n",
    "    }\n",
    "\n",
    "def run_experiment_new(\n",
    "    *, iteration, target_column, y_codes, X, mapping, positive_class,\n",
    "    training_type, K, algorithm, feature_set, balanced, multiclass, random_state=42,\n",
    "):\n",
    "\n",
    "    code_to_label = mapping\n",
    "    label_to_code = {v: k for k, v in code_to_label.items()}\n",
    "    unknown_code = label_to_code.get(\"unknown\", 2)\n",
    "    y_codes_work = np.asarray(y_codes)\n",
    "    X_work = X\n",
    "\n",
    "    if multiclass:\n",
    "        is_binary = False\n",
    "\n",
    "    else:\n",
    "        is_binary = True\n",
    "        keep = y_codes_work != unknown_code\n",
    "        y_codes_work = y_codes_work[keep]\n",
    "        X_work = X_work[keep]\n",
    "    counts_before = pd.Series(y_codes_work).value_counts().to_dict()\n",
    "\n",
    "    class_0 = int(counts_before.get(0, 0))\n",
    "    class_1 = int(counts_before.get(1, 0))\n",
    "    class_2 = int(counts_before.get(2, 0)) if multiclass else 0\n",
    "    present_counts = [c for c in [class_0, class_1, class_2] if c > 0]\n",
    "    min_class_size = int(min(present_counts)) if present_counts else 0\n",
    "    if balanced:\n",
    "        idx_bal, _ = _balance_indices_new(y_codes_work, random_state=random_state)\n",
    "        y_codes_work = y_codes_work[idx_bal]\n",
    "        X_work = X_work[idx_bal]\n",
    "    counts_after = pd.Series(y_codes_work).value_counts().to_dict()\n",
    "    if is_binary:\n",
    "        pos_code = label_to_code[positive_class]\n",
    "        y_model = (y_codes_work == pos_code).astype(int)\n",
    "        n_classes = 2\n",
    "    else:\n",
    "        unique_codes = sorted(pd.Series(y_codes_work).dropna().unique().tolist())\n",
    "        code_to_num = {c: i for i, c in enumerate(unique_codes)}\n",
    "        y_model = np.array([code_to_num[c] for c in y_codes_work], dtype=int)\n",
    "        n_classes = len(unique_codes)\n",
    "    if training_type == \"KFold\":\n",
    "        cv = StratifiedKFold(n_splits=K, shuffle=True, random_state=random_state)\n",
    "    elif training_type == \"LOOCV\":\n",
    "        cv = LeaveOneOut()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown training_type: {training_type}\")\n",
    "    estimator = algorithm(is_binary, n_classes)\n",
    "    y_pred, y_score = _manual_cv_predictions_new(estimator, X_work, y_model, cv=cv, is_binary=is_binary)\n",
    "    metrics = _evaluate_predictions_new(y_model, y_pred, y_score=y_score, is_binary=is_binary)\n",
    "\n",
    "    row = {\n",
    "        \"iteration\": int(iteration),\n",
    "        \"target_column\": str(target_column),\n",
    "        \"#classes\": int(3 if multiclass else 2),\n",
    "        \"#class_0\": int(class_0 if not balanced else counts_after.get(0, 0)),\n",
    "        \"#class_1\": int(class_1 if not balanced else counts_after.get(1, 0)),\n",
    "        \"#class_2\": int(class_2 if not balanced else counts_after.get(2, 0) if multiclass else 0),\n",
    "        \"min_class_size\": int(min_class_size),\n",
    "        \"training_type\": str(training_type),\n",
    "        \"K\": int(K if training_type == \"KFold\" else len(y_model)),\n",
    "        \"algorithm\": str(algorithm.__name__ if hasattr(algorithm, \"__name__\") else \"factory\"),\n",
    "        \"feature_set\": str(feature_set),\n",
    "        \"features_count\": int(X_work.shape[1]),\n",
    "        \"balanced\": bool(balanced),\n",
    "        \"accuracy\": metrics[\"accuracy\"],\n",
    "        \"precision\": metrics[\"precision\"],\n",
    "        \"recall\": metrics[\"recall\"],\n",
    "        \"F1\": metrics[\"F1\"],\n",
    "        \"AUC\": metrics[\"AUC\"],\n",
    "    }\n",
    "    return row\n",
    "# Paths and basic config\n",
    "\n",
    "CLASSIFICATION_DIR_3 = Path(\"POIs\") / \"Classification\"\n",
    "EXPERIMENTS_DIR_3 = CLASSIFICATION_DIR_3 / \"Experiments\"\n",
    "EXPERIMENTS_DIR_3.mkdir(parents=True, exist_ok=True)\n",
    "LABELED_PO_ITER3 = CLASSIFICATION_DIR_3 / \"labeled_data_iteration_3_person_vs_organization.csv\"\n",
    "RESULTS_FILE_3 = EXPERIMENTS_DIR_3 / \"experiments_results.csv\"\n",
    "ITERATION_3 = 3\n",
    "TARGET_NAME_3 = \"person_vs_organization\"\n",
    "print(f\"Labeled data (iteration 3 merged): {LABELED_PO_ITER3}\")\n",
    "print(f\"Results file (append only): {RESULTS_FILE_3}\\n\")\n",
    "\n",
    "# Load merged iteration-3 labeled data\n",
    "\n",
    "df_po_iter3 = pd.read_csv(LABELED_PO_ITER3, encoding=\"utf-8\")\n",
    "print(f\"Loaded merged iteration-3 person_vs_organization data: {len(df_po_iter3)} rows\")\n",
    "if \"manual_person_vs_organization\" not in df_po_iter3.columns:\n",
    "    raise ValueError(\"Expected 'manual_person_vs_organization' column in iteration 3 labeled file.\")\n",
    "y_full = df_po_iter3[\"manual_person_vs_organization\"].astype(int).to_numpy()\n",
    "mapping_po = {0: \"organization\", 1: \"person\", 2: \"unknown\"}\n",
    "positive_class_po = \"person\"\n",
    "\n",
    "# Build text features (TF-IDF)\n",
    "\n",
    "desc_series = df_po_iter3.get(\"description_en\", df_po_iter3.get(\"description\", \"\")).fillna(\"\").astype(str)\n",
    "name_series = df_po_iter3.get(\"display_name_en\", df_po_iter3.get(\"display_name\", \"\")).fillna(\"\").astype(str)\n",
    "user_series = df_po_iter3[\"username\"].fillna(\"\").astype(str)\n",
    "tfidf_params_desc = dict(max_features=500, min_df=2, max_df=0.8, ngram_range=(1, 2), strip_accents=\"unicode\", lowercase=True)\n",
    "vec_desc_3 = TfidfVectorizer(stop_words=\"english\", **tfidf_params_desc)\n",
    "X_desc_3 = vec_desc_3.fit_transform(desc_series)\n",
    "vec_name_3 = TfidfVectorizer(max_features=200, min_df=1, ngram_range=(1, 2), strip_accents=\"unicode\", lowercase=True, stop_words=\"english\")\n",
    "X_name_3 = vec_name_3.fit_transform(name_series)\n",
    "vec_user_3 = TfidfVectorizer(max_features=200, min_df=1, ngram_range=(1, 2), strip_accents=\"unicode\", lowercase=True)\n",
    "X_user_3 = vec_user_3.fit_transform(user_series)\n",
    "\n",
    "print(\"Text feature shapes:\")\n",
    "print(f\"  description: {X_desc_3.shape}\")\n",
    "print(f\"  full_name:   {X_name_3.shape}\")\n",
    "print(f\"  username:    {X_user_3.shape}\")\n",
    "\n",
    "# Numeric + PERSON/ORG keyword indicator features\n",
    "\n",
    "numeric_cols_3 = [\"followers_count\", \"following_count\", \"statuses_count\", \"verified\"]\n",
    "for col in numeric_cols_3:\n",
    "\n",
    "    if col not in df_po_iter3.columns:\n",
    "        df_po_iter3[col] = 0\n",
    "\n",
    "X_numeric_base_3 = df_po_iter3[numeric_cols_3].fillna(0).to_numpy(dtype=float)\n",
    "\n",
    "if \"verified\" in df_po_iter3.columns:\n",
    "    X_numeric_base_3[:, numeric_cols_3.index(\"verified\")] = df_po_iter3[\"verified\"].fillna(0).astype(int).to_numpy()\n",
    "\n",
    "combined_text_3 = (\n",
    "    df_po_iter3[\"username\"].fillna(\"\").astype(str)\n",
    "    + \" \"\n",
    "    + df_po_iter3.get(\"display_name_en\", df_po_iter3.get(\"display_name\", \"\")).fillna(\"\").astype(str)\n",
    "    + \" \"\n",
    "    + df_po_iter3.get(\"description_en\", df_po_iter3.get(\"description\", \"\")).fillna(\"\").astype(str)\n",
    "\n",
    ")\n",
    "combined_lower_3 = combined_text_3.str.lower().fillna(\"\")\n",
    "\n",
    "def _keyword_flag(series, keywords):\n",
    "    flags = []\n",
    "    for text in series:\n",
    "        t = str(text)\n",
    "        flags.append(1 if any(kw.lower() in t for kw in keywords) else 0)\n",
    "    return np.array(flags, dtype=int)\n",
    "\n",
    "\n",
    "\n",
    "person_kw_flag_3 = _keyword_flag(combined_lower_3, PERSON_KEYWORDS)\n",
    "\n",
    "org_kw_flag_3 = _keyword_flag(combined_lower_3, ORG_KEYWORDS)\n",
    "X_kw_3 = np.vstack([person_kw_flag_3, org_kw_flag_3]).T\n",
    "X_numeric_all_3 = np.hstack([X_numeric_base_3, X_kw_3])\n",
    "scaler_3 = StandardScaler()\n",
    "X_numeric_scaled_3 = scaler_3.fit_transform(X_numeric_all_3)\n",
    "X_num_sparse_3 = csr_matrix(X_numeric_scaled_3)\n",
    "print(\"Numeric + keyword feature shape:\")\n",
    "print(f\"  base numeric: {X_numeric_base_3.shape}\")\n",
    "print(f\"  keyword flags: {X_kw_3.shape}\")\n",
    "print(f\"  all numeric+kw (scaled): {X_num_sparse_3.shape}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Define feature sets (text-only and text+numeric+keyword) â€“ no numeric-only\n",
    "\n",
    "feature_sets_3 = {\n",
    "    \"TFIDF_description\": X_desc_3,\n",
    "    \"TFIDF_username\": X_user_3,\n",
    "    \"TFIDF_full_name\": X_name_3,\n",
    "    \"TFIDF_description+username\": hstack([X_desc_3, X_user_3]),\n",
    "    \"TFIDF_description+full_name\": hstack([X_desc_3, X_name_3]),\n",
    "    \"TFIDF_username+full_name\": hstack([X_user_3, X_name_3]),\n",
    "    \"TFIDF_description+username+full_name\": hstack([X_desc_3, X_user_3, X_name_3]),\n",
    "    \"TFIDF_description + NUM\": hstack([X_desc_3, X_num_sparse_3]),\n",
    "    \"TFIDF_username + NUM\": hstack([X_user_3, X_num_sparse_3]),\n",
    "    \"TFIDF_full_name + NUM\": hstack([X_name_3, X_num_sparse_3]),\n",
    "    \"TFIDF_description+username + NUM\": hstack([X_desc_3, X_user_3, X_num_sparse_3]),\n",
    "    \"TFIDF_description+full_name + NUM\": hstack([X_desc_3, X_name_3, X_num_sparse_3]),\n",
    "    \"TFIDF_username+full_name + NUM\": hstack([X_user_3, X_name_3, X_num_sparse_3]),\n",
    "    \"TFIDF_description+username+full_name + NUM\": hstack([X_desc_3, X_user_3, X_name_3, X_num_sparse_3]),\n",
    "\n",
    "}\n",
    "\n",
    "print(\"Defined feature sets (iteration 3, person_vs_organization):\")\n",
    "for name, Xmat in feature_sets_3.items():\n",
    "    print(f\"  {name}: shape={Xmat.shape}\")\n",
    "\n",
    "# Run grid and append to experiments_results.csv\n",
    "TRAINING_TYPES_3 = [(\"KFold\", 5), (\"LOOCV\", 5)]\n",
    "ALGO_FACTORIES_NEW = make_algorithm_factories_new(random_state=42)\n",
    "results_3 = []\n",
    "total_configs_est_3 = len(feature_sets_3) * len(ALGO_FACTORIES_NEW) * len(TRAINING_TYPES_3) * 2 * 2\n",
    "print(f\"\\nApproximate number of experiment rows to create: {total_configs_est_3}\")\n",
    "for feature_name, X_mat_3 in feature_sets_3.items():\n",
    "    for algo_name, algo_factory in ALGO_FACTORIES_NEW.items():\n",
    "        for training_type, K_val in TRAINING_TYPES_3:\n",
    "            for multiclass_flag in [True, False]:\n",
    "                for balanced_flag in [False, True]:\n",
    "                    row = run_experiment_new(\n",
    "                        iteration=ITERATION_3,\n",
    "                        target_column=TARGET_NAME_3,\n",
    "                        y_codes=y_full,\n",
    "                        X=X_mat_3,\n",
    "                        mapping=mapping_po,\n",
    "                        positive_class=positive_class_po,\n",
    "                        training_type=training_type,\n",
    "                        K=K_val,\n",
    "                        algorithm=algo_factory,\n",
    "                        feature_set=feature_name,\n",
    "                        balanced=balanced_flag,\n",
    "                        multiclass=multiclass_flag,\n",
    "                        random_state=42,\n",
    "                    )\n",
    "                    row[\"algorithm\"] = algo_name\n",
    "                    results_3.append(row)\n",
    "\n",
    "print(f\"\\nFinished running grid. New rows generated: {len(results_3)}\")\n",
    "df_results_3 = pd.DataFrame(results_3)\n",
    "if RESULTS_FILE_3.exists():\n",
    "    df_results_3.to_csv(RESULTS_FILE_3, mode=\"a\", header=False, index=False)\n",
    "    print(f\"âœ… Appended {len(df_results_3)} rows to existing {RESULTS_FILE_3.name}\")\n",
    "else:\n",
    "    df_results_3.to_csv(RESULTS_FILE_3, mode=\"w\", header=True, index=False)\n",
    "    print(f\"âœ… Created {RESULTS_FILE_3.name} with {len(df_results_3)} rows\")\n",
    "\n",
    "print(\"\\nPreview of iteration-3 person_vs_organization experiments (first 20 rows):\")\n",
    "print(df_results_3.head(20).to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"âœ… STAGE 16 COMPLETED: ITERATION 3 GRID FOR PERSON VS ORGANIZATION\")\n",
    "print(\"=\" * 90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STAGE 15 â€“ PREDICTION ON TRANSLATED UNLABELED PERSON VS ORGANIZATION (AFTER ITERATION 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 15 â€“ PREDICTION ON TRANSLATED UNLABELED (AFTER ITERATION 3)\n",
    "# Same model/logic as Stage 15, but using:\n",
    "#   POIs/Classification/unlabeled_users_after_iteration_3_person_vs_organization.csv\n",
    "# which now has description_en and display_name_en.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"STAGE 15B â€“ PREDICTION (ENGLISH TRANSLATED UNLABELED AFTER ITERATION 3)\")\n",
    "print(\"RandomForest + TFIDF(description_en/full_name_en) + numeric features\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "CLASSIFICATION_DIR_PO_2 = Path(\"POIs\") / \"Classification\"\n",
    "LABELED_FILE_PO_3_2 = CLASSIFICATION_DIR_PO_2 / \"labeled_data_iteration_3_person_vs_organization.csv\"\n",
    "UNLABELED_FILE_TRANSLATED = CLASSIFICATION_DIR_PO_2 / \"unlabeled_users_after_iteration_3_person_vs_organization.csv\"\n",
    "OUTPUT_PREDICTIONS_EN = CLASSIFICATION_DIR_PO_2 / \"iteration_3_unlabeled_users_predictions_person_vs_organization_english.csv\"\n",
    "OUTPUT_TOP100_EN = CLASSIFICATION_DIR_PO_2 / \"iteration_4_manual_labeling_candidates_person_vs_organization_english.csv\"\n",
    "\n",
    "print(f\"Labeled file:          {LABELED_FILE_PO_3_2}\")\n",
    "print(f\"Unlabeled (translted):{UNLABELED_FILE_TRANSLATED}\")\n",
    "print(f\"Output predictions:    {OUTPUT_PREDICTIONS_EN}\")\n",
    "print(f\"Output top-100:        {OUTPUT_TOP100_EN}\\n\")\n",
    "\n",
    "# Load labeled data and keep labels {0,1}\n",
    "\n",
    "df_labeled_po_2 = pd.read_csv(LABELED_FILE_PO_3_2, encoding=\"utf-8\")\n",
    "label_col_po_2 = \"manual_person_vs_organization\" if \"manual_person_vs_organization\" in df_labeled_po_2.columns else \"person_vs_organization\"\n",
    "\n",
    "if label_col_po_2 not in df_labeled_po_2.columns:\n",
    "    raise ValueError(\"No person_vs_organization label column found in labeled data.\")\n",
    "\n",
    "df_train_po_2 = df_labeled_po_2[df_labeled_po_2[label_col_po_2].isin([0, 1])].copy()\n",
    "df_train_po_2[label_col_po_2] = df_train_po_2[label_col_po_2].astype(int)\n",
    "y_train_po_2 = df_train_po_2[label_col_po_2].to_numpy()\n",
    "\n",
    "print(f\"Training rows used (labels 0/1 only): {len(df_train_po_2)}\\n\")\n",
    "\n",
    "if len(df_train_po_2) == 0:\n",
    "    raise ValueError(\"No training data with labels 0/1 available for person_vs_organization.\")\n",
    "\n",
    "# Load translated unlabeled pool and (optionally) exclude any labeled usernames\n",
    "df_unlabeled_tr_2 = pd.read_csv(UNLABELED_FILE_TRANSLATED, encoding=\"utf-8\")\n",
    "\n",
    "if \"username\" not in df_train_po_2.columns or \"username\" not in df_unlabeled_tr_2.columns:\n",
    "    raise ValueError(\"Both labeled and unlabeled data must contain a 'username' column.\")\n",
    "\n",
    "labeled_usernames_all_2 = set(df_train_po_2[\"username\"].astype(str))\n",
    "df_unlabeled_po_2 = df_unlabeled_tr_2[~df_unlabeled_tr_2[\"username\"].astype(str).isin(labeled_usernames_all_2)].copy()\n",
    "\n",
    "print(f\"Unlabeled rows in file:          {len(df_unlabeled_tr_2)}\")\n",
    "print(f\"After excluding labeled (if any): {len(df_unlabeled_po_2)}\\n\")\n",
    "\n",
    "if len(df_unlabeled_po_2) == 0:\n",
    "    raise ValueError(\"No remaining unlabeled users after excluding labeled usernames.\")\n",
    "\n",
    "def _get_text_series(df, primary, fallback):\n",
    "    if primary in df.columns:\n",
    "\n",
    "        return df[primary].fillna(\"\").astype(str)\n",
    "    if fallback in df.columns:\n",
    "\n",
    "        return df[fallback].fillna(\"\").astype(str)\n",
    "    return pd.Series([\"\"] * len(df), index=df.index)\n",
    "\n",
    "desc_train_2 = _get_text_series(df_train_po_2, \"description_en\", \"description\")\n",
    "name_train_2 = _get_text_series(df_train_po_2, \"display_name_en\", \"display_name\")\n",
    "desc_unlabeled_2 = _get_text_series(df_unlabeled_po_2, \"description_en\", \"description\")\n",
    "name_unlabeled_2 = _get_text_series(df_unlabeled_po_2, \"display_name_en\", \"display_name\")\n",
    "\n",
    "tfidf_params_2 = dict(max_features=500, ngram_range=(1, 2), lowercase=True, strip_accents=\"unicode\", stop_words=\"english\")\n",
    "\n",
    "tfidf_desc_po_2 = TfidfVectorizer(**tfidf_params_2)\n",
    "X_train_desc_po_2 = tfidf_desc_po_2.fit_transform(desc_train_2)\n",
    "X_unlabeled_desc_po_2 = tfidf_desc_po_2.transform(desc_unlabeled_2)\n",
    "\n",
    "tfidf_name_po_2 = TfidfVectorizer(**tfidf_params_2)\n",
    "X_train_name_po_2 = tfidf_name_po_2.fit_transform(name_train_2)\n",
    "X_unlabeled_name_po_2 = tfidf_name_po_2.transform(name_unlabeled_2)\n",
    "\n",
    "print(\"Text feature shapes (training):\")\n",
    "print(f\"  description_en: {X_train_desc_po_2.shape}\")\n",
    "print(f\"  full_name_en:   {X_train_name_po_2.shape}\")\n",
    "print(\"Text feature shapes (unlabeled):\")\n",
    "print(f\"  description_en: {X_unlabeled_desc_po_2.shape}\")\n",
    "print(f\"  full_name_en:   {X_unlabeled_name_po_2.shape}\\n\")\n",
    "\n",
    "# Numeric features + ratio\n",
    "num_cols_2 = [\"followers_count\", \"following_count\", \"statuses_count\"]\n",
    "\n",
    "for col in num_cols_2:\n",
    "\n",
    "    if col not in df_train_po_2.columns:\n",
    "        df_train_po_2[col] = 0\n",
    "\n",
    "    if col not in df_unlabeled_po_2.columns:\n",
    "        df_unlabeled_po_2[col] = 0\n",
    "\n",
    "def _compute_ratio(df):\n",
    "    followers = df[\"followers_count\"].fillna(0).astype(float)\n",
    "    following = df[\"following_count\"].fillna(0).astype(float)\n",
    "    following_safe = following.where(following != 0, 1.0)\n",
    "    return (followers / following_safe).astype(float)\n",
    "\n",
    "df_train_po_2[\"ratio\"] = _compute_ratio(df_train_po_2)\n",
    "df_unlabeled_po_2[\"ratio\"] = _compute_ratio(df_unlabeled_po_2)\n",
    "\n",
    "num_cols_full_2 = [\"followers_count\", \"following_count\", \"statuses_count\", \"ratio\"]\n",
    "X_num_train_2 = df_train_po_2[num_cols_full_2].fillna(0).to_numpy(dtype=float)\n",
    "X_num_unlabeled_2 = df_unlabeled_po_2[num_cols_full_2].fillna(0).to_numpy(dtype=float)\n",
    "\n",
    "scaler_po_2 = StandardScaler()\n",
    "\n",
    "X_num_train_scaled_2 = scaler_po_2.fit_transform(X_num_train_2)\n",
    "X_num_unlabeled_scaled_2 = scaler_po_2.transform(X_num_unlabeled_2)\n",
    "\n",
    "X_num_train_sparse_2 = csr_matrix(X_num_train_scaled_2)\n",
    "X_num_unlabeled_sparse_2 = csr_matrix(X_num_unlabeled_scaled_2)\n",
    "\n",
    "print(\"Numeric feature shapes (training / unlabeled):\")\n",
    "print(f\"  numeric: {X_num_train_sparse_2.shape} / {X_num_unlabeled_sparse_2.shape}\\n\")\n",
    "\n",
    "X_train_po_2 = hstack([X_train_desc_po_2, X_train_name_po_2, X_num_train_sparse_2])\n",
    "X_unlabeled_po_2 = hstack([X_unlabeled_desc_po_2, X_unlabeled_name_po_2, X_num_unlabeled_sparse_2])\n",
    "\n",
    "print(\"Final feature shapes:\")\n",
    "print(f\"  X_train_po_2:    {X_train_po_2.shape}\")\n",
    "print(f\"  X_unlabeled_po_2:{X_unlabeled_po_2.shape}\\n\")\n",
    "\n",
    "rf_po_2 = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "rf_po_2.fit(X_train_po_2, y_train_po_2)\n",
    "\n",
    "print(\"Model trained (Stage 15B): RandomForestClassifier\")\n",
    "print(f\"# training samples: {len(df_train_po_2)}\\n\")\n",
    "\n",
    "proba_po_2 = rf_po_2.predict_proba(X_unlabeled_po_2)\n",
    "class_to_index_po_2 = {int(cls): idx for idx, cls in enumerate(rf_po_2.classes_)}\n",
    "\n",
    "if not set(class_to_index_po_2.keys()).issuperset({0, 1}):\n",
    "    raise ValueError(f\"RandomForest classes are {rf_po_2.classes_}, expected [0, 1].\")\n",
    "\n",
    "prob_0_2 = proba_po_2[:, class_to_index_po_2[0]]\n",
    "prob_1_2 = proba_po_2[:, class_to_index_po_2[1]]\n",
    "predicted_class_2 = (prob_1_2 >= prob_0_2).astype(int)\n",
    "confidence_level_2 = np.maximum(prob_0_2, prob_1_2)\n",
    "uncertainty_score_2 = 1.0 - confidence_level_2\n",
    "\n",
    "df_pred_po_2 = df_unlabeled_po_2.copy()\n",
    "df_pred_po_2[\"prob_0_organization\"] = prob_0_2\n",
    "df_pred_po_2[\"prob_1_person\"] = prob_1_2\n",
    "df_pred_po_2[\"predicted_class_person_vs_organization\"] = predicted_class_2\n",
    "df_pred_po_2[\"confidence_level_person_vs_organization\"] = confidence_level_2\n",
    "df_pred_po_2[\"uncertainty_score_person_vs_organization\"] = uncertainty_score_2\n",
    "\n",
    "df_pred_po_2.to_csv(OUTPUT_PREDICTIONS_EN, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"# unlabeled samples predicted: {len(df_pred_po_2)}\")\n",
    "print(f\"Predictions saved to: {OUTPUT_PREDICTIONS_EN}\\n\")\n",
    "\n",
    "class_counts_po_2 = pd.Series(predicted_class_2).value_counts().sort_index()\n",
    "print(\"Predicted class distribution (0=organization, 1=person):\")\n",
    "for cls, cnt in class_counts_po_2.items():\n",
    "    print(f\"  class {cls}: {cnt}\")\n",
    "print()\n",
    "\n",
    "df_top100_2 = df_pred_po_2.sort_values(\"uncertainty_score_person_vs_organization\", ascending=False).head(100).copy()\n",
    "df_top100_2.to_csv(OUTPUT_TOP100_EN, index=False, encoding=\"utf-8\")\n",
    "\n",
    "if len(df_top100_2) > 0:\n",
    "    unc_min_2 = float(df_top100_2[\"uncertainty_score_person_vs_organization\"].min())\n",
    "    unc_max_2 = float(df_top100_2[\"uncertainty_score_person_vs_organization\"].max())\n",
    "    print(f\"Top 100 uncertain users saved to: {OUTPUT_TOP100_EN}\")\n",
    "    print(f\"Uncertainty range among top 100: min={unc_min_2:.4f}, max={unc_max_2:.4f}\")\n",
    "else:\n",
    "    print(\"Fewer than 1 user available for uncertainty ranking; top-100 file may be small or empty.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"âœ… STAGE 15B â€“ PREDICTION COMPLETED (TRANSLATED UNLABELED, ITERATION 3)\")\n",
    "print(\"=\" * 90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE 16C: Run Iteration 3 Experiments for Target Population (merging all 3 iterations, filtering NaN labels, running full experiment grid with 672 configurations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STAGE 16C: TARGET_POPULATION Iteration 3 - Complete Experiment Grid\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) LOAD MERGED ITERATION 3 DATA\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n[1/4] Loading merged iteration 3 labeled data...\")\n",
    "\n",
    "# Load the merged iteration 3 file (already contains all iterations merged)\n",
    "LABELED_TP_ITER3 = CLASSIFICATION_DIR_3 / \"labeled_data_iteration_3_target_population.csv\"\n",
    "df_tp_merged = pd.read_csv(LABELED_TP_ITER3, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"   Loaded {len(df_tp_merged)} rows from: {LABELED_TP_ITER3}\")\n",
    "\n",
    "# Rename column if needed\n",
    "if \"manual_target_population\" not in df_tp_merged.columns:\n",
    "    if \"target_population\" in df_tp_merged.columns:\n",
    "        df_tp_merged = df_tp_merged.rename(columns={\"target_population\": \"manual_target_population\"})\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) FILTER NaN LABELS\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n[2/4] Filtering NaN labels...\")\n",
    "before_filter = len(df_tp_merged)\n",
    "df_tp_merged = df_tp_merged[df_tp_merged[\"manual_target_population\"].notna()].copy()\n",
    "after_filter = len(df_tp_merged)\n",
    "print(f\"   Removed {before_filter - after_filter} rows with NaN labels\")\n",
    "print(f\"   Remaining rows: {after_filter}\")\n",
    "\n",
    "# Convert to int safely\n",
    "df_tp_merged[\"manual_target_population\"] = df_tp_merged[\"manual_target_population\"].astype(int)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) PREPARE FEATURES\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n[3/4] Preparing features for target_population...\")\n",
    "\n",
    "# Map labels\n",
    "mapping_tp = {0: 'non_target', 1: 'target', 2: 'unknown'}\n",
    "positive_class_tp = 'target'\n",
    "\n",
    "# Get label vector\n",
    "y_tp_full = df_tp_merged[\"manual_target_population\"].astype(int).to_numpy()\n",
    "print(f\"   Label distribution: {pd.Series(y_tp_full).value_counts().to_dict()}\")\n",
    "\n",
    "# Text features (data already in df_tp_merged from CSV files)\n",
    "desc_series_tp = df_tp_merged[\"description\"].fillna(\"\").astype(str)\n",
    "name_series_tp = df_tp_merged[\"display_name\"].fillna(\"\").astype(str)\n",
    "user_series_tp = df_tp_merged[\"username\"].fillna(\"\").astype(str)\n",
    "combined_text_tp = (name_series_tp + \" \" + desc_series_tp + \" \" + user_series_tp).str.lower()\n",
    "\n",
    "# TF-IDF\n",
    "vec_desc_tp = TfidfVectorizer(max_features=500, ngram_range=(1,2), min_df=1)\n",
    "vec_name_tp = TfidfVectorizer(max_features=200, ngram_range=(1,2), min_df=1)\n",
    "vec_user_tp = TfidfVectorizer(max_features=200, ngram_range=(1,2), min_df=1)\n",
    "\n",
    "X_desc_tp = vec_desc_tp.fit_transform(desc_series_tp)\n",
    "X_name_tp = vec_name_tp.fit_transform(name_series_tp)\n",
    "X_user_tp = vec_user_tp.fit_transform(user_series_tp)\n",
    "X_mat_tp = hstack([X_desc_tp, X_name_tp, X_user_tp])\n",
    "\n",
    "print(f\"   Text features shape: {X_mat_tp.shape}\")\n",
    "\n",
    "# Numeric features\n",
    "followers = df_tp_merged[\"followers_count\"].fillna(0).to_numpy()\n",
    "following = df_tp_merged[\"following_count\"].fillna(0).to_numpy()\n",
    "statuses = df_tp_merged[\"statuses_count\"].fillna(0).to_numpy()\n",
    "verified = df_tp_merged[\"verified\"].fillna(0).astype(int).to_numpy()\n",
    "\n",
    "# Followers/following ratio\n",
    "ratio = np.divide(followers, following, out=np.zeros_like(followers, dtype=float), where=following > 0)\n",
    "\n",
    "# Bio length\n",
    "bio_lengths = desc_series_tp.str.len().fillna(0).to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Combine numeric\n",
    "X_numeric_all_tp = np.column_stack([followers, following, statuses, verified, ratio, bio_lengths])\n",
    "scaler_tp = StandardScaler()\n",
    "X_numeric_scaled_tp = scaler_tp.fit_transform(X_numeric_all_tp)\n",
    "X_num_sparse_tp = csr_matrix(X_numeric_scaled_tp)\n",
    "\n",
    "print(f\"   Numeric features shape: {X_numeric_scaled_tp.shape}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) DEFINE FEATURE SETS (14 sets: 7 text-only + 7 text+numeric)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n[4/4] Defining 14 feature sets...\")\n",
    "\n",
    "feature_sets_tp = {\n",
    "    # Text-only\n",
    "    \"desc_only\": X_desc_tp,\n",
    "    \"name_only\": X_name_tp,\n",
    "    \"user_only\": X_user_tp,\n",
    "    \"desc+name\": hstack([X_desc_tp, X_name_tp]),\n",
    "    \"desc+user\": hstack([X_desc_tp, X_user_tp]),\n",
    "    \"name+user\": hstack([X_name_tp, X_user_tp]),\n",
    "    \"all_text\": X_mat_tp,\n",
    "    \n",
    "    # Text + numeric\n",
    "    \"desc+num\": hstack([X_desc_tp, X_num_sparse_tp]),\n",
    "    \"name+num\": hstack([X_name_tp, X_num_sparse_tp]),\n",
    "    \"user+num\": hstack([X_user_tp, X_num_sparse_tp]),\n",
    "    \"desc+name+num\": hstack([X_desc_tp, X_name_tp, X_num_sparse_tp]),\n",
    "    \"desc+user+num\": hstack([X_desc_tp, X_user_tp, X_num_sparse_tp]),\n",
    "    \"name+user+num\": hstack([X_name_tp, X_user_tp, X_num_sparse_tp]),\n",
    "    \"all_text+num\": hstack([X_mat_tp, X_num_sparse_tp]),\n",
    "}\n",
    "\n",
    "for fs_name, fs_mat in feature_sets_tp.items():\n",
    "    print(f\"   {fs_name}: {fs_mat.shape}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) RUN FULL EXPERIMENT GRID (672 experiments)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n[5/5] Running 672 experiments (6 algorithms Ã— 14 feature sets Ã— 2 training types Ã— 2 class modes Ã— 2 balance modes)...\")\n",
    "print(\"   This will take some time. Progress updates every 50 experiments.\\n\")\n",
    "\n",
    "results_tp = []\n",
    "experiment_counter = 0\n",
    "total_experiments = len(feature_sets_tp) * len(ALGO_FACTORIES_NEW) * len(TRAINING_TYPES_3) * 2 * 2\n",
    "\n",
    "for feature_name, Xmat_tp in feature_sets_tp.items():\n",
    "    for algo_name in ALGO_FACTORIES_NEW.keys():\n",
    "        for training_type, K_val in TRAINING_TYPES_3:\n",
    "            for multiclass_flag in [True, False]:\n",
    "                for balanced_flag in [True, False]:\n",
    "                    experiment_counter += 1\n",
    "                    \n",
    "                    # Progress update\n",
    "                    if experiment_counter % 50 == 0:\n",
    "                        print(f\"   Progress: {experiment_counter}/{total_experiments} experiments completed...\")\n",
    "                    \n",
    "                    # Run experiment\n",
    "                    row = run_experiment_new(\n",
    "                        iteration=ITERATION_3,\n",
    "                        target_column=TARGET_NAME_3_TP,\n",
    "                        y_codes=y_tp_full,\n",
    "                        X=Xmat_tp,\n",
    "                        mapping=mapping_tp,\n",
    "                        positive_class=positive_class_tp,\n",
    "                        training_type=training_type,\n",
    "                        K=K_val,\n",
    "                        algorithm=ALGO_FACTORIES_NEW[algo_name],\n",
    "                        feature_set=feature_name,\n",
    "                        balanced=balanced_flag,\n",
    "                        multiclass=multiclass_flag\n",
    "                    )\n",
    "                    \n",
    "                    results_tp.append(row)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6) SAVE RESULTS\n",
    "# -------------------------------------------------------------------\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPLETED! Total experiments run: {len(results_tp)}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results_tp = pd.DataFrame(results_tp)\n",
    "\n",
    "# Reorder columns to match experiments_results.csv\n",
    "col_order = [\"iteration\", \"target_column\", \"#classes\", \"#class_0\", \"#class_1\", \"#class_2\", \"min_class_size\",\n",
    "             \"training_type\", \"K\", \"algorithm\", \"feature_set\", \"features_count\", \"balanced\",\n",
    "             \"accuracy\", \"precision\", \"recall\", \"F1\", \"AUC\"]\n",
    "df_results_tp = df_results_tp[col_order]\n",
    "\n",
    "# Append to main results file\n",
    "if RESULTS_FILE_3.exists():\n",
    "    df_results_tp.to_csv(RESULTS_FILE_3, mode='a', header=False, index=False)\n",
    "    print(f\"\\nâœ“ Appended {len(df_results_tp)} rows to: {RESULTS_FILE_3}\")\n",
    "else:\n",
    "    df_results_tp.to_csv(RESULTS_FILE_3, index=False)\n",
    "    print(f\"\\nâœ“ Created new file with {len(df_results_tp)} rows: {RESULTS_FILE_3}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label_val, label_name in mapping_tp.items():\n",
    "    count = (y_tp_full == label_val).sum()\n",
    "    print(f\"   {label_name} ({label_val}): {count} samples\")\n",
    "\n",
    "print(f\"\\nFeature matrix shapes:\")\n",
    "print(f\"   Text-only: {X_mat_tp.shape}\")\n",
    "print(f\"   Numeric: {X_numeric_scaled_tp.shape}\")\n",
    "print(f\"   Combined: {hstack([X_mat_tp, X_num_sparse_tp]).shape}\")\n",
    "\n",
    "print(f\"\\nResults breakdown:\")\n",
    "print(f\"   Total experiments: {len(df_results_tp)}\")\n",
    "print(f\"   Feature sets: {len(feature_sets_tp)}\")\n",
    "print(f\"   Algorithms: {len(ALGO_FACTORIES_NEW)}\")\n",
    "print(f\"   Training types: {len(TRAINING_TYPES_3)}\")\n",
    "print(f\"   Iterations merged: 3 (iteration 1 + 2 + 3)\")\n",
    "print(f\"   Total labeled samples: {len(y_tp_full)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DONE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**translate all the unlabled users descriptions an usernames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 16D: TRANSLATE UNLABELED USERS (AFTER ITERATION 3) TO ENGLISH\n",
      "================================================================================\n",
      "Loading unlabeled users from: POIs\\Classification\\unlabeled_users.csv\n",
      "Loaded 846 unlabeled users.\n",
      "Translating descriptions and display names to English...\n",
      "Translation completed in 941.76 seconds.\n",
      "Translated unlabeled users saved to: POIs\\Classification\\unlabeled_users_after_iteration_3_target_population.csv\n"
     ]
    }
   ],
   "source": [
    "# translate all the unlabeled users descriptions and usernames after iteration 3 to English\n",
    "# Uses deep-translator's GoogleTranslator\n",
    "\n",
    "import time\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 16D: TRANSLATE UNLABELED USERS (AFTER ITERATION 3) TO ENGLISH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "CLASSIFICATION_DIR_TP = Path(\"POIs\") / \"Classification\"\n",
    "UNLABELED_TP_FILE = CLASSIFICATION_DIR_TP / \"unlabeled_users.csv\"\n",
    "OUTPUT_TRANSLATED_TP_FILE = CLASSIFICATION_DIR_TP / \"unlabeled_users_after_iteration_3_target_population.csv\"\n",
    "\n",
    "print(f\"Loading unlabeled users from: {UNLABELED_TP_FILE}\")\n",
    "df_unlabeled_tp = pd.read_csv(UNLABELED_TP_FILE, encoding=\"utf-8\")\n",
    "print(f\"Loaded {len(df_unlabeled_tp)} unlabeled users.\")\n",
    "\n",
    "def translate_text(text, max_retries: int = 3):\n",
    "    \"\"\"Translate text to English using deep-translator (GoogleTranslator).\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text).strip()\n",
    "    if text == \"\":\n",
    "        return \"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            translator = GoogleTranslator(source=\"auto\", target=\"en\")\n",
    "            text_to_translate = text[:5000] if len(text) > 5000 else text\n",
    "            translated = translator.translate(text_to_translate)\n",
    "            if translated and str(translated).strip() != \"\":\n",
    "                return str(translated).strip()\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(0.5 * (attempt + 1))\n",
    "                continue\n",
    "            print(f\"   âš ï¸ Translation failed after {max_retries} attempts: {str(e)[:50]}...\")\n",
    "            return text\n",
    "    return text\n",
    "\n",
    "print(\"Translating descriptions and display names to English...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_unlabeled_tp[\"description_en\"] = df_unlabeled_tp[\"description\"].apply(translate_text)\n",
    "df_unlabeled_tp[\"display_name_en\"] = df_unlabeled_tp[\"display_name\"].apply(translate_text)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Translation completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "df_unlabeled_tp.to_csv(OUTPUT_TRANSLATED_TP_FILE, index=False, encoding=\"utf-8\")\n",
    "print(f\"Translated unlabeled users saved to: {OUTPUT_TRANSLATED_TP_FILE}\")\n",
    "# End of Stage translate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**chhose the best classifier to predict target_population iteration 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show only the iteration-3 target_population experiment results : show the highest accuracy ones\n",
    "#and the AUC ROC ones and print the first 20 rows\n",
    "\n",
    "TARGET_NAME_3_TP = \"target_population\"  \n",
    "import pandas as pd\n",
    "print(\"\\nLoading iteration-3 target_population experiment results...\")\n",
    "\n",
    "df_results_tp = pd.read_csv(RESULTS_FILE_3, encoding=\"utf-8\", engine='python', on_bad_lines='skip')\n",
    "\n",
    "# use the Python engine and skip malformed lines to avoid ParserError\n",
    "df_tp_iter3 = df_results_tp[df_results_tp[\"iteration\"] == 3]\n",
    "# Filter for target_population experiments\n",
    "df_tp_iter3 = df_tp_iter3[df_tp_iter3[\"target_column\"] == TARGET_NAME_3_TP]\n",
    "print(f\"Loaded {len(df_tp_iter3)} iteration-3 target_population experiment results.\\n\")\n",
    "df_tp_iter3_sorted_acc = df_tp_iter3.sort_values(by=\"accuracy\", ascending=False).head(20)\n",
    "df_tp_iter3_sorted_auc = df_tp_iter3.sort_values(by=\"AUC\", ascending=False).head(20)\n",
    "print(\"Top 20 experiments by Accuracy:\")\n",
    "print(df_tp_iter3_sorted_acc.to_string(index=False))\n",
    "print(\"\\nTop 20 experiments by AUC:\")\n",
    "print(df_tp_iter3_sorted_auc.to_string(index=False))\n",
    "\n",
    "#make a csv file for the top 20 accuracy and AUC ones\n",
    "TOP20_RESULTS_FILE_TP = EXPERIMENTS_DIR_3 / \"top20_iteration3_target_population_experiments.csv\"\n",
    "df_top20_tp = pd.concat([df_tp_iter3_sorted_acc, df_tp_iter3_sorted_auc]).drop_duplicates().reset_index(drop=True)\n",
    "df_top20_tp.to_csv(TOP20_RESULTS_FILE_TP, index=False, encoding=\"utf-8\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Predictions for Target Population (Iteration 3) - Using Logistic Regression with desc_only features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 15C: GENERATE PREDICTIONS - TARGET_POPULATION (ITERATION 3)\n",
      "================================================================================\n",
      "\n",
      "Input files:\n",
      "  Labeled training data: POIs\\Classification\\labeled_data_iteration_3_target_population.csv\n",
      "  Unlabeled data: POIs\\Classification\\unlabeled_users_after_iteration_3_target_population.csv\n",
      "  Output file: POIs\\Classification\\iteration_3_unlabeled_users_predictions_target_population.csv\n",
      "\n",
      "[1/6] Loading labeled training data (iteration 3)...\n",
      "  Loaded 300 labeled records\n",
      "\n",
      "[2/6] Filtering training data (removing 'unknown' and NaN labels)...\n",
      "  Label distribution before filtering:\n",
      "    {0.0: 131, 1.0: 105, 2.0: 62, nan: 2}\n",
      "\n",
      "  Filtered to 236 training samples\n",
      "  Label distribution after filtering:\n",
      "    {0.0: 131, 1.0: 105}\n",
      "\n",
      "[3/6] Preparing text features (desc_only)...\n",
      "  Training feature matrix shape: (236, 500)\n",
      "  Training labels shape: (236,)\n",
      "  Class distribution: [131 105]\n",
      "\n",
      "[4/6] Training Logistic Regression model...\n",
      "  Model trained successfully\n",
      "  Classes: [0 1]\n",
      "\n",
      "[5/6] Loading unlabeled data and generating predictions...\n",
      "  Loaded 846 unlabeled records\n",
      "  âœ“ Validation passed: probabilities sum to 1\n",
      "\n",
      "[6/6] Appending predictions to unlabeled data...\n",
      "  âœ“ Validation passed: 846 rows in output\n",
      "\n",
      "================================================================================\n",
      "SUCCESS! Predictions saved to:\n",
      "  POIs\\Classification\\iteration_3_unlabeled_users_predictions_target_population.csv\n",
      "\n",
      "Prediction summary:\n",
      "  Total predictions: 846\n",
      "  Predicted 'target': 159\n",
      "  Predicted 'non_target': 687\n",
      "  Mean confidence: 0.5884\n",
      "  Mean uncertainty: 0.4116\n",
      "\n",
      "Top 5 most uncertain predictions:\n",
      "          username predicted_class  uncertainty_score  confidence_level\n",
      "170   90ndstoppage               1           0.499000          0.501000\n",
      "583      al_thawaq               0           0.497704          0.502296\n",
      "637  alexeygorboot               1           0.497048          0.502952\n",
      "643     alexlaughs               0           0.495601          0.504399\n",
      "257  abdolhamidnet               1           0.494911          0.505089\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# STAGE 15C: GENERATE PREDICTIONS FOR TARGET_POPULATION (ITERATION 3)\n",
    "# \n",
    "# This stage generates predictions on unlabeled users using:\n",
    "# - Classifier: Logistic Regression\n",
    "# - Features: desc_only (TF-IDF from description column only)\n",
    "# - Model selected based on iteration-3 experimental results\n",
    "#\n",
    "# Input: unlabeled_users_after_iteration_3_target_population.csv\n",
    "# Output: iteration_3_unlabeled_users_predictions_target_population.csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 15C: GENERATE PREDICTIONS - TARGET_POPULATION (ITERATION 3)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# PATHS\n",
    "# ============================================================================\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "LABELED_FILE = CLASSIFICATION_DIR / \"labeled_data_iteration_3_target_population.csv\"\n",
    "UNLABELED_FILE = CLASSIFICATION_DIR / \"unlabeled_users_after_iteration_3_target_population.csv\"\n",
    "OUTPUT_FILE = CLASSIFICATION_DIR / \"iteration_3_unlabeled_users_predictions_target_population.csv\"\n",
    "\n",
    "print(f\"\\nInput files:\")\n",
    "print(f\"  Labeled training data: {LABELED_FILE}\")\n",
    "print(f\"  Unlabeled data: {UNLABELED_FILE}\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD LABELED TRAINING DATA\n",
    "# ============================================================================\n",
    "print(\"[1/6] Loading labeled training data (iteration 3)...\")\n",
    "df_labeled = pd.read_csv(LABELED_FILE, encoding=\"utf-8\")\n",
    "print(f\"  Loaded {len(df_labeled)} labeled records\")\n",
    "\n",
    "# Ensure we have the right column name\n",
    "if \"manual_target_population\" not in df_labeled.columns:\n",
    "    if \"target_population\" in df_labeled.columns:\n",
    "        df_labeled = df_labeled.rename(columns={\"target_population\": \"manual_target_population\"})\n",
    "    else:\n",
    "        raise ValueError(\"Labeled file must contain 'target_population' or 'manual_target_population' column\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: FILTER OUT \"UNKNOWN\" LABELS AND NaN VALUES\n",
    "# ============================================================================\n",
    "print(\"\\n[2/6] Filtering training data (removing 'unknown' and NaN labels)...\")\n",
    "print(f\"  Label distribution before filtering:\")\n",
    "print(f\"    {df_labeled['manual_target_population'].value_counts(dropna=False).to_dict()}\")\n",
    "\n",
    "# Map labels: 0=non_target, 1=target, 2=unknown\n",
    "# Filter out both unknown (2) AND NaN values\n",
    "df_train = df_labeled[\n",
    "    (df_labeled[\"manual_target_population\"].notna()) &  # Remove NaN\n",
    "    (df_labeled[\"manual_target_population\"] != 2)        # Remove unknown\n",
    "].copy()\n",
    "\n",
    "print(f\"\\n  Filtered to {len(df_train)} training samples\")\n",
    "print(f\"  Label distribution after filtering:\")\n",
    "print(f\"    {df_train['manual_target_population'].value_counts().to_dict()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: PREPARE TEXT FEATURES (desc_only)\n",
    "# ============================================================================\n",
    "print(\"\\n[3/6] Preparing text features (desc_only)...\")\n",
    "\n",
    "# Extract descriptions and fill missing values\n",
    "desc_train = df_train[\"description\"].fillna(\"\").astype(str)\n",
    "\n",
    "# TF-IDF Vectorizer (matching desc_only experiment configuration)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=\"english\",\n",
    "    max_features=500,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(desc_train)\n",
    "y_train = df_train[\"manual_target_population\"].astype(int).to_numpy()\n",
    "\n",
    "print(f\"  Training feature matrix shape: {X_train.shape}\")\n",
    "print(f\"  Training labels shape: {y_train.shape}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_train)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: TRAIN LOGISTIC REGRESSION MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n[4/6] Training Logistic Regression model...\")\n",
    "\n",
    "model = LogisticRegression(\n",
    "    solver=\"liblinear\",\n",
    "    max_iter=5000,\n",
    "    class_weight=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"  Model trained successfully\")\n",
    "print(f\"  Classes: {model.classes_}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: LOAD UNLABELED DATA AND GENERATE PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"\\n[5/6] Loading unlabeled data and generating predictions...\")\n",
    "\n",
    "df_unlabeled = pd.read_csv(UNLABELED_FILE, encoding=\"utf-8\")\n",
    "print(f\"  Loaded {len(df_unlabeled)} unlabeled records\")\n",
    "\n",
    "# Prepare text features for unlabeled data\n",
    "desc_unlabeled = df_unlabeled[\"description\"].fillna(\"\").astype(str)\n",
    "X_unlabeled = vectorizer.transform(desc_unlabeled)\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_unlabeled)\n",
    "y_proba = model.predict_proba(X_unlabeled)\n",
    "\n",
    "# Extract probabilities\n",
    "prob_0 = y_proba[:, 0]  # P(non_target)\n",
    "prob_1 = y_proba[:, 1]  # P(target)\n",
    "\n",
    "# Derive prediction columns\n",
    "predicted_class = np.where(prob_1 >= prob_0, \"1\", \"0\")\n",
    "confidence_level = np.maximum(prob_0, prob_1)\n",
    "uncertainty_score = 1 - confidence_level\n",
    "prob_2 = np.full(len(df_unlabeled), np.nan)  # Binary task, no prob_2\n",
    "\n",
    "# Validation: Check probabilities sum to ~1\n",
    "prob_sums = prob_0 + prob_1\n",
    "assert np.allclose(prob_sums, 1.0, atol=1e-6), \"Probabilities do not sum to 1\"\n",
    "print(f\"  âœ“ Validation passed: probabilities sum to 1\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: APPEND PREDICTIONS AND SAVE\n",
    "# ============================================================================\n",
    "print(\"\\n[6/6] Appending predictions to unlabeled data...\")\n",
    "\n",
    "# Add prediction columns to the end\n",
    "df_unlabeled[\"predicted_class\"] = predicted_class\n",
    "df_unlabeled[\"confidence_level\"] = confidence_level\n",
    "df_unlabeled[\"prob_0\"] = prob_0\n",
    "df_unlabeled[\"prob_1\"] = prob_1\n",
    "df_unlabeled[\"prob_2\"] = prob_2\n",
    "df_unlabeled[\"uncertainty_score\"] = uncertainty_score\n",
    "\n",
    "# Sort by uncertainty_score descending\n",
    "df_unlabeled = df_unlabeled.sort_values(by=\"uncertainty_score\", ascending=False)\n",
    "\n",
    "# Validation: Check row count\n",
    "assert len(df_unlabeled) == len(df_unlabeled), \"Output row count mismatch\"\n",
    "print(f\"  âœ“ Validation passed: {len(df_unlabeled)} rows in output\")\n",
    "\n",
    "# Save to CSV\n",
    "CLASSIFICATION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "df_unlabeled.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"SUCCESS! Predictions saved to:\")\n",
    "print(f\"  {OUTPUT_FILE}\")\n",
    "print(f\"\\nPrediction summary:\")\n",
    "print(f\"  Total predictions: {len(df_unlabeled)}\")\n",
    "print(f\"  Predicted 'target': {(predicted_class == '1').sum()}\")\n",
    "print(f\"  Predicted 'non_target': {(predicted_class == '0').sum()}\")\n",
    "print(f\"  Mean confidence: {confidence_level.mean():.4f}\")\n",
    "print(f\"  Mean uncertainty: {uncertainty_score.mean():.4f}\")\n",
    "print(f\"\\nTop 5 most uncertain predictions:\")\n",
    "print(df_unlabeled[[\"username\", \"predicted_class\", \"uncertainty_score\", \"confidence_level\"]].head())\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE 16E: Select Top 100 Most Uncertain Predictions for Manual Labeling (Iteration 4 Candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 16E: SELECT TOP 100 UNCERTAIN PREDICTIONS FOR ITERATION 4\n",
      "================================================================================\n",
      "\n",
      "Input file: POIs\\Classification\\iteration_3_unlabeled_users_predictions_target_population.csv\n",
      "Output file: POIs\\Classification\\iteration_4_manual_labels_target_population.csv\n",
      "\n",
      "[1/3] Loading predictions from iteration 3...\n",
      "  Loaded 846 predictions\n",
      "  Uncertainty score range: 0.2272 - 0.4990\n",
      "\n",
      "[2/3] Selecting top 100 most uncertain predictions...\n",
      "  Selected 100 candidates\n",
      "  Uncertainty score range in selection: 0.4334 - 0.4990\n",
      "\n",
      "  Predicted class distribution in top 100:\n",
      "    {1: 53, 0: 47}\n",
      "\n",
      "[3/3] Preparing manual labeling file...\n",
      "\n",
      "================================================================================\n",
      "SUCCESS! Manual labeling file created:\n",
      "  POIs\\Classification\\iteration_4_manual_labels_target_population.csv\n",
      "\n",
      "File details:\n",
      "  Total candidates: 100\n",
      "  Columns included: 18\n",
      "\n",
      "Columns in file:\n",
      "  - username\n",
      "  - display_name\n",
      "  - manual_target_population\n",
      "  - predicted_class\n",
      "  - uncertainty_score\n",
      "  - confidence_level\n",
      "  - description\n",
      "  - description_en\n",
      "  - display_name_en\n",
      "  - followers_count\n",
      "  - following_count\n",
      "  - statuses_count\n",
      "  - verified\n",
      "  - created_at\n",
      "  - location\n",
      "  - url\n",
      "  - profile_image_url\n",
      "  - profile_banner_url\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ NEXT STEPS:\n",
      "  1. Open: POIs\\Classification\\iteration_4_manual_labels_target_population.csv\n",
      "  2. Manually label the 'manual_target_population' column\n",
      "  3. Use values: 0 (non_target), 1 (target), 2 (unknown)\n",
      "  4. Save the file when done\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# STAGE 16E: CREATE ITERATION 4 MANUAL LABELING FILE\n",
    "# \n",
    "# This stage selects the top 100 most uncertain predictions from iteration 3\n",
    "# and creates a file for manual labeling in iteration 4.\n",
    "#\n",
    "# Input: iteration_3_unlabeled_users_predictions_target_population.csv\n",
    "# Output: iteration_4_manual_labels_target_population.csv\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 16E: SELECT TOP 100 UNCERTAIN PREDICTIONS FOR ITERATION 4\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# PATHS\n",
    "# ============================================================================\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "PREDICTIONS_FILE = CLASSIFICATION_DIR / \"iteration_3_unlabeled_users_predictions_target_population.csv\"\n",
    "OUTPUT_FILE = CLASSIFICATION_DIR / \"iteration_4_manual_labels_target_population.csv\"\n",
    "\n",
    "print(f\"\\nInput file: {PREDICTIONS_FILE}\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD PREDICTIONS\n",
    "# ============================================================================\n",
    "print(\"[1/3] Loading predictions from iteration 3...\")\n",
    "df_predictions = pd.read_csv(PREDICTIONS_FILE, encoding=\"utf-8\")\n",
    "print(f\"  Loaded {len(df_predictions)} predictions\")\n",
    "print(f\"  Uncertainty score range: {df_predictions['uncertainty_score'].min():.4f} - {df_predictions['uncertainty_score'].max():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT TOP 100 MOST UNCERTAIN\n",
    "# ============================================================================\n",
    "print(\"\\n[2/3] Selecting top 100 most uncertain predictions...\")\n",
    "\n",
    "# The file is already sorted by uncertainty_score descending, but let's ensure it\n",
    "df_top100 = df_predictions.nlargest(100, 'uncertainty_score').copy()\n",
    "\n",
    "print(f\"  Selected {len(df_top100)} candidates\")\n",
    "print(f\"  Uncertainty score range in selection: {df_top100['uncertainty_score'].min():.4f} - {df_top100['uncertainty_score'].max():.4f}\")\n",
    "print(f\"\\n  Predicted class distribution in top 100:\")\n",
    "print(f\"    {df_top100['predicted_class'].value_counts().to_dict()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE MANUAL LABELING FILE\n",
    "# ============================================================================\n",
    "print(\"\\n[3/3] Preparing manual labeling file...\")\n",
    "\n",
    "# Add a column for manual labels (empty, to be filled by annotators)\n",
    "df_top100[\"manual_target_population\"] = \"\"\n",
    "\n",
    "# Select columns for manual labeling\n",
    "# Include key information for annotators to make decisions\n",
    "columns_to_keep = [\n",
    "    \"username\",\n",
    "    \"display_name\",\n",
    "    \"manual_target_population\",  # Empty column for manual annotation\n",
    "    \"predicted_class\",\n",
    "    \"uncertainty_score\",\n",
    "    \"confidence_level\",\n",
    "    \"description\",\n",
    "    \"description_en\",\n",
    "    \"display_name_en\",\n",
    "    \"followers_count\",\n",
    "    \"following_count\",\n",
    "    \"statuses_count\",\n",
    "    \"verified\",\n",
    "    \"created_at\",\n",
    "    \"location\",\n",
    "    \"url\",\n",
    "    \"profile_image_url\",\n",
    "    \"profile_banner_url\"\n",
    "]\n",
    "\n",
    "# Keep only columns that exist in the dataframe\n",
    "columns_available = [col for col in columns_to_keep if col in df_top100.columns]\n",
    "df_manual = df_top100[columns_available].copy()\n",
    "\n",
    "# Save to CSV\n",
    "df_manual.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"SUCCESS! Manual labeling file created:\")\n",
    "print(f\"  {OUTPUT_FILE}\")\n",
    "print(f\"\\nFile details:\")\n",
    "print(f\"  Total candidates: {len(df_manual)}\")\n",
    "print(f\"  Columns included: {len(columns_available)}\")\n",
    "print(f\"\\nColumns in file:\")\n",
    "for col in columns_available:\n",
    "    print(f\"  - {col}\")\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"ğŸ“‹ NEXT STEPS:\")\n",
    "print(f\"  1. Open: {OUTPUT_FILE}\")\n",
    "print(f\"  2. Manually label the 'manual_target_population' column\")\n",
    "print(f\"  3. Use values: 0 (non_target), 1 (target), 2 (unknown)\")\n",
    "print(f\"  4. Save the file when done\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 16F: ADD MANUAL LABEL COLUMN TO ITERATION 4 PERSON_VS_ORGANIZATION CANDIDATES\n",
      "================================================================================\n",
      "\n",
      "Loading candidates from: POIs\\Classification\\iteration_4_manual_labeling_candidates_person_vs_organization_english.csv\n",
      "Loaded 100 candidates.\n",
      "Saved updated file with manual label column to: POIs\\Classification\\iteration_4_manual_labels_person_vs_organization_english.csv\n"
     ]
    }
   ],
   "source": [
    "#add menual column empty for iteration_4_manual_labeling_candidates_person_vs_organization_english\n",
    "# add empty column for manual labels to the existing file\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 16F: ADD MANUAL LABEL COLUMN TO ITERATION 4 PERSON_VS_ORGANIZATION CANDIDATES\")\n",
    "print(\"=\" * 80)\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "INPUT_FILE = CLASSIFICATION_DIR / \"iteration_4_manual_labeling_candidates_person_vs_organization_english.csv\"\n",
    "OUTPUT_FILE = CLASSIFICATION_DIR / \"iteration_4_manual_labels_person_vs_organization_english.csv\"\n",
    "print(f\"\\nLoading candidates from: {INPUT_FILE}\")\n",
    "df_candidates = pd.read_csv(INPUT_FILE, encoding=\"utf-8\")\n",
    "print(f\"Loaded {len(df_candidates)} candidates.\")\n",
    "df_candidates[\"manual_person_vs_organization\"] = \"\"\n",
    "df_candidates.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8\")\n",
    "print(f\"Saved updated file with manual label column to: {OUTPUT_FILE}\")\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 16G: CREATE UNLABELED USERS FILES AFTER ITERATION 4\n",
      "================================================================================\n",
      "\n",
      "Processing Person vs Organization...\n",
      "Loading unlabeled users from: POIs\\Classification\\unlabeled_users_after_iteration_3_person_vs_organization.csv\n",
      "Loaded 648 candidate users.\n",
      "Loading manually labeled users from: POIs\\Classification\\iteration_4_manual_labels_person_vs_organization_english.csv\n",
      "Loaded 100 manually labeled users.\n",
      "After excluding manually labeled users: 548 unlabeled users remain.\n",
      "âœ“ Unlabeled users saved to: POIs\\Classification\\unlabeled_users_after_iteration_4_person_vs_organization.csv\n",
      "\n",
      "Processing Target Population...\n",
      "Loading unlabeled users from: POIs\\Classification\\unlabeled_users_after_iteration_3_target_population.csv\n",
      "Loaded 846 candidate users.\n",
      "Loading manually labeled users from: POIs\\Classification\\iteration_4_manual_labels_target_population.csv\n",
      "Loaded 100 manually labeled users.\n",
      "After excluding manually labeled users: 746 unlabeled users remain.\n",
      "âœ“ Unlabeled users saved to: POIs\\Classification\\unlabeled_users_after_iteration_4_target_population.csv\n",
      "\n",
      "================================================================================\n",
      "PROCESSING COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create unlabeled users files after iteration 4 by excluding manually labeled users\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 16G: CREATE UNLABELED USERS FILES AFTER ITERATION 4\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "CLASSIFICATION_DIR = Path(\"POIs\") / \"Classification\"\n",
    "\n",
    "# Process Person vs Organization\n",
    "print(f\"\\nProcessing Person vs Organization...\")\n",
    "INPUT_FILE_PO = CLASSIFICATION_DIR / \"unlabeled_users_after_iteration_3_person_vs_organization.csv\"\n",
    "MANUAL_LABELS_PO = CLASSIFICATION_DIR / \"iteration_4_manual_labels_person_vs_organization_english.csv\"\n",
    "OUTPUT_FILE_PO = CLASSIFICATION_DIR / \"unlabeled_users_after_iteration_4_person_vs_organization.csv\"\n",
    "\n",
    "print(f\"Loading unlabeled users from: {INPUT_FILE_PO}\")\n",
    "df_candidates_po = pd.read_csv(INPUT_FILE_PO, encoding=\"utf-8\")\n",
    "print(f\"Loaded {len(df_candidates_po)} candidate users.\")\n",
    "\n",
    "print(f\"Loading manually labeled users from: {MANUAL_LABELS_PO}\")\n",
    "df_labeled_po = pd.read_csv(MANUAL_LABELS_PO, encoding=\"utf-8\")\n",
    "print(f\"Loaded {len(df_labeled_po)} manually labeled users.\")\n",
    "\n",
    "# Exclude manually labeled users by username\n",
    "df_unlabeled_po = df_candidates_po[~df_candidates_po[\"username\"].isin(df_labeled_po[\"username\"])]\n",
    "print(f\"After excluding manually labeled users: {len(df_unlabeled_po)} unlabeled users remain.\")\n",
    "\n",
    "df_unlabeled_po.to_csv(OUTPUT_FILE_PO, index=False, encoding=\"utf-8\")\n",
    "print(f\"âœ“ Unlabeled users saved to: {OUTPUT_FILE_PO}\")\n",
    "\n",
    "# Process Target Population\n",
    "print(f\"\\nProcessing Target Population...\")\n",
    "INPUT_FILE_TP = CLASSIFICATION_DIR / \"unlabeled_users_after_iteration_3_target_population.csv\"\n",
    "MANUAL_LABELS_TP = CLASSIFICATION_DIR / \"iteration_4_manual_labels_target_population.csv\"\n",
    "OUTPUT_FILE_TP = CLASSIFICATION_DIR / \"unlabeled_users_after_iteration_4_target_population.csv\"\n",
    "\n",
    "print(f\"Loading unlabeled users from: {INPUT_FILE_TP}\")\n",
    "df_candidates_tp = pd.read_csv(INPUT_FILE_TP, encoding=\"utf-8\")\n",
    "print(f\"Loaded {len(df_candidates_tp)} candidate users.\")\n",
    "\n",
    "print(f\"Loading manually labeled users from: {MANUAL_LABELS_TP}\")\n",
    "df_labeled_tp = pd.read_csv(MANUAL_LABELS_TP, encoding=\"utf-8\")\n",
    "print(f\"Loaded {len(df_labeled_tp)} manually labeled users.\")\n",
    "\n",
    "# Exclude manually labeled users by username\n",
    "df_unlabeled_tp = df_candidates_tp[~df_candidates_tp[\"username\"].isin(df_labeled_tp[\"username\"])]\n",
    "print(f\"After excluding manually labeled users: {len(df_unlabeled_tp)} unlabeled users remain.\")\n",
    "\n",
    "df_unlabeled_tp.to_csv(OUTPUT_FILE_TP, index=False, encoding=\"utf-8\")\n",
    "print(f\"âœ“ Unlabeled users saved to: {OUTPUT_FILE_TP}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (IRAN Project)",
   "language": "python",
   "name": "iran"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
